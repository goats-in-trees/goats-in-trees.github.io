
var documents = [{
    "id": 0,
    "url": "https://goatsintrees.net/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://goatsintrees.net/about",
    "title": "Goats in Trees, articles about Technology",
    "body": "Goats in Trees offers articles about Cloud technologies, DevOps, AWS, GCP, and other topics that interest its author Dave Sugden. "
    }, {
    "id": 2,
    "url": "https://goatsintrees.net/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://goatsintrees.net/contact",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 4,
    "url": "https://goatsintrees.net/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 5,
    "url": "https://goatsintrees.net/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 6,
    "url": "https://goatsintrees.net/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "https://goatsintrees.net/page3/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "https://goatsintrees.net/get-started-with-ci-cd-using-buddy/",
    "title": "Get started with CI/CD using Buddy",
    "body": "2020/11/02 - Welcome to the second part of a series of articles comparing CI/CD platforms. To help evaluate, compare and contrast the tools currently dominating the market, the goal will be to automate the deployment of a Flask application onto AWS Elastic Beanstalk. A new deployment will need to occur after every push to main branch, and during the series this same requirement will be implemented across several CI/CD tools. Our first article looked at GitHub Actions, which you can read below. Get Started With CI/CD Using GitHub Actions Next up, part 2 will focus on Buddy — to be found online at https://buddy. works. I’ll document the steps taken along the way to achieve the goal with Buddy, and there’s a write up of the positives and negatives in a conclusion at the end of this article. Our application: The Flask application Our Flask application is a simple “Hello World” example, and for the purposes of demonstrating running unit tests within the pipeline, we have included a test case as well. AWS Elastic Beanstalk Our Flask application is going to be deployed to AWS Elastic Beanstalk which is a service that automates the deployment and scaling of a web application. It takes in our source code and takes care of all the infrastructure configuration. Introduction to Buddy: The Buddy homepage boasts some very impressive stats: “87% faster CI/CD adoption time by teams” and “12 seconds of average deployment time”. It’s not hard to see why — there is a low barrier to adoption and pretty much anyone could get started with Buddy within minutes. What I like straight off the bat is the fact the initial steer for users is to create the pipeline via the GUI, which will be a huge draw for many — with the fallback of configuring as code for your DevOps folks (we do love our yaml). Once you’re into the nuts and bolts, here are a few core concepts of Buddy. Pipeline: The core concept of the Buddy platform is the pipeline that allows you to build, test and deploy your application on a single push to a chosen branch. Pipelines can also be triggered manually or on a timer, and they can be chained (pipeline A &gt; pipeline B &gt; pipeline C). Action: A pipeline is made up of a series of steps and on Buddy these are called actions. An action will run in a Docker container, and each action consists of a small series of shell commands — such as building and running unit tests for an application, uploading to a server, sending notifications, and so on. You have access to configuring environment variables for each action, and can configure and integrate with associated services such as a database. Environment variables can also be configured at the workspace or project level, not just at the pipeline/action level, so your options are powerful there. Execution: An execution is a single run of a pipeline on the Buddy platform. Every run is saved and you can review the execution history to see who or what triggered the pipeline, when was it and for which git revision. Integrations: Integrations are third-party services that you can integrate with your Buddy projects, like repository and website hosting services, testing tools and notification apps. Examples include GitHub, AWS, Heroku, Slack, GitLab, Cloudflare, Firebase, and much more. Although not technically called an “integration” within the Buddy ecosystem, there are many built-in actions that build out common use-cases for integrating with third party platforms. And while I was writing this article, they added a few more. Steps: These are the steps we will follow to create our CI/CD pipeline.    Create an empty repository in GitHub.     Sign up for an account on Buddy.     Create a project and set up an initial pipeline. Our pipeline will run after every push event on the main branch.     Extend our pipeline to deploy to Elastic Beanstalk.     Try out the yaml helper and configuration.  Step 1. Create GitHub repository and initial workflow: Create your GitHub repository and upload your initial application code. My demonstration code is a simple “Hello World” Flask application, a test case, and a requirements. txt for defining the dependencies. If you are following along your initial structure should contain three files:    application. py     test_application. py     requirements. txt  Alternative. I went with GitHub, but you don’t have to. You can create your repository directly within the Buddy platform; use Bitbucket or GitLab; or bring your own Git hosting. Step 2. Sign up for a Buddy account: Head over to https://buddy. works and register. I used my GitHub account to register and gave Buddy access to my source code repositories. You will automatically be put onto a 14-day free trial of their Pro Plan ($75 per month), but there is also a Free Plan which is sufficient for developers, freelancers, and more than enough to get a good look at the platform. Step 3. Set up our first pipeline: Under the Free Plan you’ll be able to create 5 projects. Once registered and signed in, you’ll find yourself on the workspace dashboard where you can “Create a new project”. Let’s do that now.  Select your repository and Buddy will auto-detect the language as Python and give you the option to set up your pipeline. Keep going through the GUI — it’s the default option and we’ll look at the yaml option later.  We want to trigger our pipeline after every push to the main branch, so select the “on push” option and ensure that the correct branch is selected — Buddy will pre-populate and it was all correct for me without needing any changes.  After creating your pipeline, the next step is to add an action. You’ll see that Buddy has auto-selected Python and is making a suggestion to use it. Let’s follow that and select Python.  Buddy will pre-populate the out-of-the-box Python action and we’re going to do a few small tweaks.    Firstly, update the version of Python. The default on Buddy is 3. 5, but we’re going to bump this to 3. 6. 12. You can pick 3. 7. x and 3. 8. x as well.     Next let’s change the Action name from “Execute: nosetests” to “Execute: test_application. py”. You see this under the Action tab.     Finally, update the script commands per that shown below.  pip install -r requirements. txtpython test_application. pyAt this stage you can save your action. Before you do, check out the other tabs while you are here. For example, you will see that Buddy will automatically create a pip cache to save your dependencies — this is a good feature, one we typically would want anyway, and so by being enabled by default this saves users from having to spend the time to configure it. On saving, you will see your pipeline has the single primary action we just created.  Once saved, we now want to manually “Run pipeline” which you will find in the top right of the UI screen. In this section, you’ll also notice “Badge” (which you can configure in your README markdown file if you have one) and “Slack handle” options for notifications. Run the pipeline and you should see it run quickly and successfully.  Click through for the logs of the execution. Well done, at this stage you have successfully linked your source code repository, downloaded (and cached) dependencies, and tested your application works correctly. Step 4. Deploy Flask application to Elastic Beanstalk: Now that we have a successful “build &amp; test”, we want to deploy the application for testing purposes. We’re going to do this by adding a new action to our existing pipeline which will run after the first action is successful. Back in the pipeline click the cog underneath our first Action to append a new one. You will notice how you can add pre- and post- actions and chain a workflow together — very nice visualization.  You could scroll down here, but let’s use the “Filter actions…” box and search for “AWS”. Here you will find many targets, one of which is Elastic Beanstalk. First time through you’ll be asked to create a new “AWS Integration”, so paste in your Access Key ID and Secret Access Key and save. (remember, AWS security best practice would be to create a dedicated User with programmatic access and having permissions just to deploy to AWS Elastic Beanstalk). Now we want to configure our options for our AWS Elastic Beanstalk application. Keep the defaults and under Application drop down select “Create an application” — the Buddy platform will direct you to AWS to complete this part — or select an existing Application if you have one. Here’s my configuration, ready to be saved.  And my complete pipeline now looks like this.  Run the pipeline again, and within a minute you should see your Flask application up and running in AWS Elastic Beanstalk.  Well done, you’ve successfully run a build, test, deploy to Elastic Beanstalk, all thanks to the Buddy platform — it’s probably taken longer to read than do the work; I would estimate getting to this stage for most users would be 2–3 minutes maximum. Now let’s test the trigger on code push. I’ve made a small edit to my example application to change the message to “Hello World!”. Within seconds of push, the pipeline was running in Buddy, and less than a minute later it had finished. Refresh our browser and we can see the new message is being displayed.  That’s the end of the demonstration. You should have successfully built and deployed your application to Elastic Beanstalk and been through the cycle of a code change and seen that change reflected in AWS almost immediately. Did we see YAML?: Yes, there’s a YAML option as well as the UI-generated process. You can build your pipeline from scratch, or in our case, since we already have our pipeline created we can use the option to “Export the current pipelines to YAML”.  If you choose to configure your pipeline using YAML, you have to save this file as buddy. yml in the root of the source code repository. Let’s do that now. You’ll also need to toggle from “GUI” to “YML” via the “YAML configuration” tab. By default Buddy is all GUI-driven, but you switch to a “pipelines as code” model very easily.  Conclusion: Before this week, I hadn’t come across Buddy but it’s very much got the feel of a big-player. The ease of use, the feature set, the speed of deployment, the number of integrations — I could go on — it’s got a lot going for it. Back to their website claims — “87% faster CI/CD adoption time by teams” and “12 seconds of average deployment time” — to be honest, I believe both to be true. Having used many different platforms over the years, Buddy has definitely been one of the quickest to get started with and I’d expect engineers of all disciplines and experience to be up and running within minutes. Article first written November 2020 and things change over time. At the time of writing, here were my findings… + Positives:    Free tier to get started. Not overly generous, but sufficient. You get 500MB of storage and 120 executions per month. Noticeable that Buddy don’t feature “execution minutes” in their pricing model.     A huge list of ready-made integrations with GitHub, AWS, Heroku, Slack, GitLab, Cloudflare, Firebase, and many more.     Provides git repository hosting for a fully integrated service. Although not tied — so hosting at GitHub works just as well.     All the core facets of a pipeline are catered for including my basic “must have” options to trigger on code updates, manual “push button” steps, and a scheduler option to run pipelines on a timer.     The extensive documentation is awesome — both layman and developer focussed and very well presented. I was amazed at how much content there was and found step-by-step examples for pretty much every scenario I could think of.     To be honest — the documentation is so good that it makes articles such as this largely redundant — on top of which, the platform is so intuitive the documentation might not even be that necessary in most cases.  Neutral:  There’s a feature around monitoring pipelines which is more than intriguing, if not a little puzzling. At first this felt out-of-place under the banner of a CI/CD, and I was concerned if this could dilute the platform — since Buddy see it as a means to “monitor your websites and services for downtimes and get notified whenever taking action is required”. On the one hand, there’s power in all-in-one platforms but on the other hand, I also like platforms that do just what they say on the tin where their focus is to do one thing and do it excellently. I also like picking the best tool for the job, so my natural inclination is to avoid the multi-purpose options. In Buddy defence here, their web page title is “The DevOps Automation Platform” so this feature clearly fits into that domain — yet the site shouts “CI/CD” everywhere else — maybe Buddy would do well spinning this off and maturing their service observability, monitoring, and alerting separately. With that written, I then thought about it over the last week and I began to see how you might be able to chain pipelines and use the concept of service monitoring into a workflow — perhaps as a canary or to verify a deployment to trigger a switchover of traffic. Good feature, but makes my neutral list for now while I sit on the fence. - Negatives:    Once you burned your Free Plan limit of 120 executions in a month there is a big step in terms of pricing — “free” jumps to $75 per month and then $200 per month. Definitely a lot of value in the platform to justify, just feels for me like an intermediary price point is required.     In my particular use-case, directing me to AWS to manually set up my Elastic Beanstalk Application and Environment felt a touch clunky — given the high expectations of “low touch” set by this stage — so I’m sure a seamless fully-integrated EB experience won’t be too long away.     I got an error using the yaml that was auto-generated via the yaml helper. Although it worked first time through, I was hoping Buddy would pick it up automatically on a new project pointing at a repository already containing a buddy. yml file. I got the error below with the same repository used throughout the demonstration. At the time of writing, I have pinged the Buddy team to ask why this might be the case.   A note from the author: Thank you for reading this article — I hope you found it useful and please read the remainder of the series as I compare other CI/CD tools. All source code for this demonstration can be downloaded at GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 9,
    "url": "https://goatsintrees.net/get-started-with-ci-cd-using-github-actions/",
    "title": "Get Started With CI/CD Using GitHub Actions",
    "body": "2020/10/25 - Welcome to the first part of a series of articles comparing CI/CD platforms. To help evaluate, compare and contrast the tools currently dominating the market, the goal will be to automate the deployment of a Flask application onto AWS Elastic Beanstalk. A new deployment will need to occur after every push to main branch, and during the series this same requirement will be implemented across a multitude of CI/CD tools. First up, part 1 will focus on GitHub Actions, where the support for CI/CD was first announced in August 2019. I’ll document the steps taken along the way to achieve the goal with GitHub Actions, and there’s a write up of the positives and negatives in a conclusion at the end of this article. Our application: The Flask application Our Flask application is a simple “Hello World” example, and for the purposes of demonstrating running unit tests within the pipeline, we have included a test case as well. AWS Elastic Beanstalk Our Flask application is going to be deployed to AWS Elastic Beanstalk which is a service that automates the deployment and scaling of a web application. It takes in our source code and takes care of all the infrastructure configuration. There’s a handy CLI for Elastic Beanstalk, and ultimately we’re going to automate the following commands into our pipeline. However, these are just for reference and you do not need to run these now. $ eb init -p python-3. 6 hello-world --region us-east-1$ eb create hello-world-envIntroduction to GitHub Actions:  An event will trigger a workflow.  A workflow contains one or more jobs.  A job is executed on a runner and will contain one or more steps.  A step will contain a single action or a series of inline commands.  The run can generate artifacts and use a cache. Runner: A job runner is any machine with the GitHub Actions Runner Application installed — these machines can be GitHub-hosted or you can run your own. GitHub-hosted runners are virtual machines — Linux (Ubuntu), Windows, and macOS — and are fully managed by GitHub and come with a lot of installed software. For example, the image for Ubuntu comes armed with multiple libraries, including the AWS CLI and Google SDK. As part of the free-tier, GitHub provides 2,000 minutes per month of its hosted runners. The runner is defined in the workflow yaml with the runs-on property. Action: An action is the smallest unit of work in the GitHub Actions ecosystem. These are the individual tasks that can be combined as steps to create a job. You can create your own actions, use community-shared actions from the Marketplace, and customize public actions. Actions can also be packaged up as docker containers that are executed on the runner machine — these shared actions in the Marketplace may provide additional software or scripts in a container that encapsulate more complex or repeat scenarios. Step: A step is an individual task that can run a series of commands or a single action. By default all steps will run directly on the job runner virtual machine unless the step refers to an action that is configured to run in a container. Job: A job is a group of one of more steps, and all steps within the same job will share the filesystem. Jobs can run at the same time in parallel (default) or run sequentially — to define that run jobs are to sequentially, specify the dependencies in the job specification. For example, a workflow can have two sequential jobs that build and deploy code, where the deploy job is dependent on the status of the build job. If the build job fails, the deploy job will not run. Workflow: A workflow is the overall automated process that is made up of one or more jobs. You define a workflow using YAML format files and these are saved alongside your application code in the . github/workflows directory. Dependency Caching: Caching third party dependencies is a common technique when building out a CI/CD pipeline as it can save minutes from repeatedly downloading the same dependencies. It’s useful to save off the contents of node_modules for npm; the . m2/repository for Apache Maven; and in our case the pip cache from ~/. cache/pip. GitHub Actions provides a simple solution that allows us to save off the path and all its contents to the cache at the end of a successful completion of the job. The cache will be restored to the path at the start of the next job execution. Pay attention however, because if the job fails the cache will not be saved. Steps: These are the steps we will follow to create our CI/CD pipeline.    Create an empty repository in GitHub and set up an initial workflow and run our first “build” job. This workflow will run after every push event on the main branch.     Add our Python code for a “Hello World” application, and update the steps in our “build” job definition to test the Python code after every change.     Create a “deploy-to-test” job definition which is configured to run after the “build” job has successfully completed. We will install the Elastic Beanstalk CLI, configure our AWS credentials using GitHub Secrets, and create the application and environment.     With an existing environment, the “deploy-to-test” job will fail on the second run — so we need to fix that issue so that the workflow will run after every push event. We’ll make a change to the application to test its repeatability.  Step 1. Create GitHub repository and initial workflow: Create your GitHub repository and upload your initial application code. My demonstration code is a simple “Hello World” Flask application, a test case, and a requirements. txt for defining the dependencies. If you are following along your initial structure should contain three files:    application. py     test_application. py     requirements. txt  With your code now in place, head over to the GitHub console, go into “Actions”, and create a “New Workflow”. GitHub will detect you have a Python application and will give you some starter pipelines. There’s many to choose from — some from GitHub, some from third parties — for the purpose of this demo, select “set up a workflow yourself” so that we can start with a blank canvas and learn as we go.  The default name is main. yaml — paste the following to get started. name: CIon: push:  branches: [main]jobs: build:  runs-on: ubuntu-latest  steps:   # Checks-out your repository under $GITHUB_WORKSPACE   - uses: actions/checkout@v2   # Set up Python 3. 6 environment   - name: Set up Python 3. 6    uses: actions/setup-python@v1    with:     python-version:  3. 6    # Install dependencies   - name: Install dependencies    run: |     python -m pip install --upgrade pip     pip install -r requirements. txt   # Run our unit tests   - name: Run unit tests    run: |     python test_application. pyOur initial workflow defines 1 job (called “build”) consisting of 4 steps:  Checkout the code Set up Python 3. 6 environment Install dependencies Run &amp; test the application codeWhen you save it will create a file at the path . github/workflows/main. yaml and from here on you make, add, commit, and push updates just as you would with any other file in a git repository.  Your workflow should run immediately. Check back into “Actions” to see the output. At this stage, you should have run your first GitHub Actions workflow.  Well done! Step 2. Add a cache to speed up runtime. : To enable a cache for the pip dependencies, we can use the built-in cache Action to save off the pip cache located at ~/. cache/pip. We set the cache path based on the location of the pip cache, and we use our requirements. txt file as a hash to detect when we need to rebuild the cache. Insert the following into your main. yaml immediately after the setup-python step. Save and commit. - name: Get pip cache dir id: pip-cache run: |  echo  ::set-output name=dir::$(pip cache dir) - name: Cache pip uses: actions/cache@v1 with:  path: ${{ steps. pip-cache. outputs. dir }}  key: ${{ runner. os }}-pip-${{ hashFiles('**/requirements. txt') }}  restore-keys: |   ${{ runner. os }}-pip-Run it once and you will see that there is no cache hit. Re-run your workflow and you will see a hit. This will save time for larger applications with lots of dependencies.  Step completed. Well done! Step 3a. Create our initial “deploy-to-test” job: Now that we have a successful “build”, we want to deploy the application for testing purposes. We could continue in a sequential manner within the “build” job, but let’s demonstrate how we would create a multi-job workflow instead. By default, jobs run in parallel. To run jobs sequentially, as we require, we have to specify we have a dependency between “deploy-to-test” upon “build”. To achieve this we state our “deploy-to-test” job needs “build” to be successful before the job will run — so if “build” fails, GitHub Actions will not invoke the “deploy-to-test” job. deploy-to-test: # Only run this job if  build  has ended successfully needs:  - buildWe’ll run on Ubuntu as we did before, and we’ll need to ensure that we checkout the git repository and set up Python 3. 6 again — it’s not shared. Although, AWS CLI is installed we default, the Elastic Beanstalk CLI is not so we need to install it using the pip install awsebcli command. # Elastic Beanstalk CLI version- name: Get EB CLI version run: |  python -m pip install --upgrade pip setuptools wheel  pip install awsebcli --upgrade  eb --versionHere’s the full definition to append to your main. yaml file. deploy-to-test:  # Only run this job if  build  has ended successfully  needs:   - build     runs-on: ubuntu-latest  steps:   # Checks-out your repository under $GITHUB_WORKSPACE   - uses: actions/checkout@v2   # Set up Python 3. 6 environment   - name: Set up Python 3. 6    uses: actions/setup-python@v1    with:     python-version:  3. 6    # Set up cache for pip   - name: Get pip cache dir    id: pip-cache    run: |     echo  ::set-output name=dir::$(pip cache dir)    - name: Cache pip    uses: actions/cache@v1    with:     path: ${{ steps. pip-cache. outputs. dir }}     key: ${{ runner. os }}-pip-${{ hashFiles('**/requirements. txt') }}     restore-keys: |      ${{ runner. os }}-pip-   # Elastic Beanstalk CLI version   - name: Get EB CLI version    run: |     python -m pip install --upgrade pip     pip install awsebcli --upgrade     eb --versionAt this stage, you should now have the Elastic Beanstalk CLI deployed and available to your “deploy-to-test” job. In the UI you will see the additional job listed in the left hand column.  Check the output to confirm the Elastic Beanstalk CLI version was displayed.  Well done, another milestone achieved. Step 3b. Configure our AWS credentials using GitHub Secrets: Configure two secrets in GitHub to contain your User access key and client secret. Secrets are made available to your job as ${{ secrets. XXXX }}.  AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEYTo set these up, head into the GitHub console and Secrets is under Settings.  (remember, AWS security best practice would be to create a dedicate User with programmatic access and having permissions just to deploy to AWS Elastic Beanstalk). In a new step, we can add the Action configure-aws-credentials which is a docker container provided via the AWS-team managed project aws-actions. - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with:  aws-access-key-id: ${{ secrets. AWS_ACCESS_KEY_ID }}  aws-secret-access-key: ${{ secrets. AWS_SECRET_ACCESS_KEY }}  aws-region: us-east-1There’s no visual clue that this worked, so commit and jump on to Step 3c. Step 3c. Deploy Application to Elastic Beanstalk: Elastic Beanstalk is so easy to use from the command prompt using the CLI. Remember those two CLI commands I shared at the very start and said not to run? We are going to include these commands in our pipeline and build our first Elastic Beanstalk application and environment. # Create the Elastic Beanstalk application- name: Create EBS application run: |  eb init -p python-3. 6 hello-world --region us-east-1# Create the Elastic Beanstalk environment- name: Create test environment run: |  eb create test-environmentCommit your changes, and you should see your application successfully running on Elastic Beanstalk. Check the command line for confirmation of the endpoint: And verify the same in the AWS Console.  And here’s the application successfully running on AWS… Well done, you’ve successfully run a build, test, deploy to Elastic Beanstalk. Step 4. Avoiding failure on subsequent job executions: Our workflow is working and has successfully created the initial Elastic Beanstalk application and environment. The first version has been installed. But what happens when we make a change and push that to master branch. Unfortunately, the eb create test-environment will fail when it is run for a second time — we need to detect that scenario, and run an eb deploy instead. We can solve that problem with a bit of logic syntax and the eb status test-environment command. If our “test-environment” already exists, the command passes and we can deploy to it with eb deploy. Otherwise, if it fails, then we create “test-environment”. Our updated command now looks like this. (eb use test-environment &amp;&amp; eb status test-environment &amp;&amp; eb deploy) || eb create test-environmentIf we make a change (I edited “Hello World!” to “Hello Dave!”), we can see from the screenshot below that the GitHub Actions has deployed the new Application version to the “test-environment”.  Hitting the endpoint in the browser confirms the deployment was successful.  Well done, that’s the end of the demonstration. You have successfully built and deployed your application to Elastic Beanstalk and been through the cycle of a code change and seen that change reflected in AWS almost immediately (my testing showed it took around 1 minute 30 seconds from commit to seeing the updated code running on AWS). You can download the final main. yml from GitHub. Conclusion: Article first written May 2020 and things change over time. At the time of writing, here were my findings… + Positives:    Easy to get started with a generous 2,000 minutes free tier.     The fact we’re building upon GitHub where we are likely to already have our git hosting, project pages, issues, team management, etc. is an obvious bonus straight off the bat.     GitHub Actions provides multiple out-of-the-box workflow examples for a wide range of coding languages, and these are augmented with ready-made third party Actions available via the Marketplace.     Extensible framework — you can create your own Actions inline, or publish an Action and reference in your projects.     Jobs are started up quickly and there is no observable lag.     GitHub-hosted Runners come pre-built with a lot of software, including the CLI for AWS, Google Cloud Platform, and Azure cloud platforms.     Fully feature GitHub Actions API, and support for Web Hooks.     The extensive documentation is developer focussed and well presented.  - Negatives:    At the time of writing there isn’t a built-in ability to chain workflows to create more complex pipelines, or to embed/re-use whole workflows.     No support for manual triggers. While a user can re-run a failed job, that’s the limit to the user-initiated actions. For example, a feature integrated directly into most other CI/CD platforms is the ability to implement a manual push-button stage for deployments.     To address both of the above, we can use the GitHub Actions API and Web Hooks, triggering events from another tool or system (e. g, Slack, postman) or use custom branches and use the merge from one to another as a means to trigger the “next stage”.  Part 2: Enjoyed this article? Check out Part 2, a look at the Buddy workflow platform. A note from the author: Thank you for reading this article — I hope you found it useful and please read the remainder of the series as I compare other CI/CD tools. All source code for this demonstration can be downloaded at GitHub, along with the final main. yml. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 10,
    "url": "https://goatsintrees.net/run-jenkins-in-a-docker-container-part-3-run-as-root-user/",
    "title": "Run Jenkins in a Docker container — part 3 — run as root user",
    "body": "2020/10/13 - This small series of guides will walk through three solutions for installing Jenkins in a Docker container on Windows, along with the configuration necessary to spin up dynamic build slaves also using Docker containers. Running locally on a personal device is perfect for individual users, freelancers, or developers looking to do local Jenkinsfile or Shared Library development and testing before pushing to a central CI/CD platform. “You said three solutions?” Yes — this article demonstrates running the Jenkins container as root user instead of jenkins user. Part 1 was using Docker-in-Docker and Part 2 was to replace with a socat container. Read both of those articles below or scroll to continue with the final article in the series. Run Jenkins in a Docker container - part 1 - Docker-in-Docker Run Jenkins in a Docker container - part 2 - socat What was wrong with using Docker-in-Docker: This article discusses the pros and cons of Docker-in-Docker and argues against using it for CI systems like Jenkins. It suggests a better pattern would be to provide the container access to the host daemon directly via sharing a volume for its unix socket /var/run/docker. sock— and this is possible on Windows too since we are using WSL — the primary obstacle we face is that the jenkins user doesn’t have the required permissions. Further complexity follows as the jenkins user obviously doesn’t exist on the Windows host, only within the context of the Jenkins container — which rules out using usermod -AG docker jenkins. The command has zero effect. And the setfacl approach to give the ‘jenkins’ user read-write access to /var/run/docker. sock works —yay! — but doesn’t persist between restarts of Docker daemon on the host Windows machine. So to conclude — this article will set up Jenkins to run as root user and map the /var/run/docker. sock into the container. If you balk at the thought of running as root, even locally, please check out parts 1 and 2 of this series. Running Jenkins container as root: The steps we will go through;    Command line, step-by-step to set up Jenkins.     Configure Jenkins via the Console UI and set up the “docker” plugin.     Verify the set up via a couple of test jobs.     Translate the command lines into a Docker Compose template.  Prerequisites: We assume you have Docker installed — for this demo I am running on a Windows 10 laptop with WSL enabled (Hyper-V is also okay). Volumes: Before we start up, create a volume that you will use for your Jenkins home directory — that way it will persist between restarts. docker volume create jenkins-dataNetworking: We’re going to use a bridge network called jenkins which we create by running the command docker network create jenkins. In the rest of our configuration, we will attach our container to this network. Running Jenkins: We’re going to use the jenkinsci/blueocean image that comes pre-built with Blue Ocean. We will map /var/run/docker. sock to allow our container access to the Docker host, and important we specify -u root so that we can talk to the socket without additional configuration. docker container run --name jenkins-blueocean \ --detach --restart unless-stopped \ --network jenkins \ --user root \ --volume jenkins-data:/var/jenkins_home \ --volume /var/run/docker. sock:/var/run/docker. sock \ --publish 8080:8080 --publish 50000:50000 \ jenkinsci/blueoceanNow that Jenkins is started, head over to http://localhost:8080 to go through the initial set up wizard. Jenkins Set Up Wizard: We won’t go through the Jenkins set up wizard in any detail — the important thing to note however is how to access your admin user unlock code, which you can find this from your running container via the following command: docker exec jenkins-blueocean cat /var/jenkins_home/secrets/initialAdminPasswordConfiguring Docker Cloud: Once you have your Jenkins set up complete, we need to set up running our Jenkins slaves as Docker containers. Head over to “Manage Plugins” and install the docker plugin.  Once installed, go to Manage Nodes and Clouds and then Configure Clouds and Add a new cloud. The type of “docker” should automatically appear in the dropdown. You want to set the Docker Host URI to unix:///var/run/docker. sock and then run a Test Connection.  This is great — now we can reach the Docker daemon. Configuring Docker Slaves/Agents: Next step is to set up an agent to run our pipelines against. Once you start developing, you can start building your own build containers and attaching these instead. For now, we shall use the jenkins/agent image. Relevant fields below include giving it a label and name. The label allows you to associate a job to a particular agent — so you can always run a Maven build on an Agent that has maven installed, for example.  Our Connect Method will be Attach Docker Container. This runs the Docker container on the host machine. Save everything and we are now ready to create a test job. Verifying it all works: You are now ready to create a job to check that all is set up and working correctly. Create a new freestyle job. I’ve called mine “test-agent”. You want to select the agent that this job will run on.  Down under build, write a simple “hello world” message as a test.  Save and build your new job. Jenkins will download the image for your container and run the job upon it. On completion you should get output such as that shown below.  All done. Success! Docker Compose: Command line is all well and good, but it’s much easier to define as a docker compose yaml file. Start up with docker-compose up -d. version: '3. 8'networks: jenkins-network:  name: jenkinsvolumes: data:  name: jenkins-dataservices: jenkins:  container_name: jenkins-blueocean  image: jenkinsci/blueocean  restart: unless-stopped  user: root  networks:   - jenkins-network  ports:   - 8080:8080   - 50000:50000  volumes:   - data:/var/jenkins_home   - /var/run/docker. sock:/var/run/docker. sockA note from the author: Thank you for reading this article and I hope you found time to read the original part 1 and could contrast against the solution in part 2 as well — I hope you found them useful and via one of the articles you settled on a technique that suited your personal set up. All source code for these articles can be found on GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 11,
    "url": "https://goatsintrees.net/run-jenkins-in-a-docker-container-part-2-socat/",
    "title": "Run Jenkins in a Docker container — part 2 — socat",
    "body": "2020/10/13 - This small series of guides will walk through three solutions for installing Jenkins in a Docker container on Windows, along with the configuration necessary to spin up dynamic build slaves also using Docker containers. Running locally on a personal device is perfect for individual users, freelancers, or developers looking to do local Jenkinsfile or Shared Library development and testing before pushing to a central CI/CD platform. “You said three solutions?” Yes — this article demonstrates using socat (further reading at socat). Part 1 was using Docker-in-Docker and Part 3 is to run the Jenkins container as root user. Jump to those articles below. Run Jenkins in a Docker container - part 1 - Docker-in-Docker Run Jenkins in a Docker container - part 3 - run as root user What was wrong with using Docker-in-Docker: This article discusses the pros and cons of Docker-in-Docker and argues against using it for CI systems like Jenkins. It suggests a better pattern would be to provide the container access to the host daemon directly via sharing a volume for its unix socket /var/run/docker. sock — and this is possible on Windows too since we are using WSL — the primary obstacle we face is that the jenkins user doesn’t have the required permissions. Part 3 of this series set up Jenkins to run as root user; this article will demonstrate an alternative using socat. Solution using socat: The steps we will go through;    Command line, step-by-step to set up Jenkins.     Configure Jenkins via the Console UI and set up the “docker” plugin.     Verify the set up via a couple of test jobs.     Translate the command lines into a Docker Compose template.  Prerequisites: We assume you have Docker installed — for this demo I am running on a Windows 10 laptop with WSL enabled (Hyper-V is also okay). Volumes: Before we start up, create a volume that you will use for your Jenkins home directory — that way it will persist between restarts. docker volume create jenkins-dataNetworking: We’re going to use a bridge network called jenkins which we create by running the command docker network create jenkins. In the rest of our configuration, we will attach all our containers to this network. In order to give Jenkins access to the Docker daemon running on the host machine (Windows), we will use a socat container to publish the unix socket /var/run/docker. sock (to the Docker daemon) as port 2375. docker container run --name jenkins-docker \ --detach --restart unless-stopped \ --network jenkins --network-alias docker \ --volume /var/run/docker. sock:/var/run/docker. sock \ --publish 2375:2375 \ alpine/socat \ tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker. sockAfter running this command, the socat container will be listening on port 2375 and since we gave it the network alias of docker then we will be able to reach it from Jenkins on tcp://docker:2375. Running Jenkins: We’re going to use the jenkinsci/blueocean image that comes pre-built with Blue Ocean and you will see that we specify tcp://docker:2375 to communicate with the Docker host. docker container run --name jenkins-blueocean \ --detach --restart unless-stopped \ --network jenkins \ --env DOCKER_HOST= tcp://docker:2375  \ --env DOCKER_TLS_VERIFY=   \ --volume jenkins-data:/var/jenkins_home \ --publish 8080:8080 --publish 50000:50000 \ jenkinsci/blueoceanNow that Jenkins is started, head over to http://localhost:8080 to go through the initial set up wizard. Jenkins Set Up Wizard: We won’t go through the Jenkins set up wizard in any detail — the important thing to note however is how to access your admin user unlock code, which you can find this from your running container via the following command: docker exec jenkins-blueocean cat /var/jenkins_home/secrets/initialAdminPasswordConfiguring Docker Cloud: Once you have your Jenkins set up complete, we need to set up running our Jenkins slaves as Docker containers. Head over to “Manage Plugins” and install the docker plugin.  Once installed, go to Manage Nodes and Clouds and then Configure Clouds and Add a new cloud. The type of “docker” should automatically appear in the dropdown. You want to set the Docker Host URI to tcp://docker:2375 and then run a Test Connection.  This is great — now we can reach the Docker daemon. Configuring Docker Slaves/Agents: Next step is to set up an agent to run our pipelines against. Once you start developing, you can start building your own build containers and attaching these instead. For now, we shall use the jenkins/agent image. Relevant fields below include giving it a label and name. The label allows you to associate a job to a particular agent — so you can always run a Maven build on an Agent that has maven installed, for example.  Our Connect Method will be Attach Docker Container. This runs the Docker container on the host machine. Save everything and we are now ready to create a test job. Verifying it all works: You are now ready to create a job to check that all is set up and working correctly. Create a new freestyle job. I’ve called mine “test-agent”. You want to select the agent that this job will run on.  Down under build, write a simple “hello world” message as a test.  Save and build your new job. Jenkins will download the image for your container and run the job upon it. On completion you should get output such as that shown below.  All done. Success! Docker Compose: Command line is all well and good, but it’s much easier to define as a docker compose yaml file. Start up with docker-compose up -d. version: '3. 8'networks: jenkins-network:  name: jenkinsvolumes: data:  name: jenkins-dataservices: socat:  container_name: jenkins-docker  image: alpine/socat  restart: unless-stopped  networks:   jenkins-network:    aliases:     - docker  expose:   -  2375   volumes:   - /var/run/docker. sock:/var/run/docker. sock  command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker. sock jenkins:  container_name: jenkins-blueocean  image: jenkinsci/blueocean  restart: unless-stopped  networks:   - jenkins-network  ports:   - 8080:8080   - 50000:50000  volumes:   - data:/var/jenkins_home  environment:   - DOCKER_HOST=tcp://docker:2375   - DOCKER_TLS_VERIFY=  A note from the author: Thank you for reading this article and I hope you found time to read the original part 1 and could contrast against the solution in part 3 as well — I hope you found them useful and via one of the articles you settled on a technique that suited your personal set up. All source code for these articles can be found on GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 12,
    "url": "https://goatsintrees.net/run-jenkins-in-a-docker-container-part-1-docker-in-docker/",
    "title": "Run Jenkins in a Docker container — part 1 — Docker-in-Docker",
    "body": "2020/10/11 - This small series of guides will walk through three solutions for installing Jenkins in a Docker container on Windows, along with the configuration necessary to spin up dynamic build slaves also using Docker containers. Running locally on a personal device is perfect for individual users, freelancers, or developers looking to do local Jenkinsfile or Shared Library development and testing before pushing to a central CI/CD platform. “You said three solutions?” Yes — this article demonstrates using Docker-in-Docker (further reading at dind). Part 2 is using a socat container and Part 3 is to run Jenkins as root. Jump to those articles below. Run Jenkins in a Docker container - part 2 - socat Run Jenkins in a Docker container - part 3 - run as root user Solution using Docker-in-Docker: The steps we will go through;    Command line, step-by-step to set up Jenkins.     Configure Jenkins via the Console UI and set up the “docker” plugin.     Verify the set up via a couple of test jobs.     Translate the command lines into a Docker Compose template.  Prerequisites: We assume you have Docker installed — for this demo I am running on a Windows 10 laptop with WSL enabled (Hyper-V is also okay). Volumes: Before we start up, create a volume that you will use for your Jenkins home directory — that way it will persist between restarts. docker volume create jenkins-dataWe’re going to create another volume for persisting and sharing the certificates so that Jenkins can talk to Docker-in-Docker. docker volume create jenkins-docker-certsNetworking: We’re going to use a bridge network called jenkins which we create by running the command docker network create jenkins. In the rest of our configuration, we will attach all our containers to this network. In order to give Jenkins access to the Docker daemon running on the host machine (Windows), we will use Docker-in-Docker. docker container run --name jenkins-docker \ --detach --restart unless-stopped \ --privileged \ --network jenkins --network-alias docker \  --env DOCKER_TLS_CERTDIR= /certs  \ --volume jenkins-docker-certs:/certs/client \ --volume jenkins-data:/var/jenkins_home \ docker:dindAfter running this command, a Docker-in-Docker container will be listening on port 2376 and since we gave it the network alias of docker then we will be able to reach it from Jenkins on tcp://docker:2376. Notes:    on initial start-up, Docker will create client and server certificates under /certs — you will need these later when configuring the docker cloud.     variation. to skip TLS and use port 2375, set DOCKER_TLS_CERTDIR=    Running Jenkins: We’re going to use the jenkinsci/blueocean image that comes pre-built with Blue Ocean. You will see that we specify tcp://docker:2376 as the Docker host and for security we need to verify certificates to the host — since we share the same volume we can find them in /certs/client (created by the host) which we map as read-only. docker container run --name jenkins-blueocean \ --detach --restart unless-stopped \ --network jenkins \ --env DOCKER_HOST= tcp://docker:2376  \ --env DOCKER_CERT_PATH=/certs/client \ --env DOCKER_TLS_VERIFY=1 \ --volume jenkins-docker-certs:/certs/client:ro \ --volume jenkins-data:/var/jenkins_home \ --publish 8080:8080 --publish 50000:50000 \ jenkinsci/blueoceanVariation. Should you have chosen to skip TLS and are using port 2375 instead then these are the environment variables to use.  --env DOCKER_HOST= tcp://docker:2375  --env DOCKER_CERT_PATH=   --env DOCKER_TLS_VERIFY=  Now that Jenkins is started, head over to http://localhost:8080 to go through the initial set up wizard. Jenkins Set Up Wizard: We won’t go through the Jenkins set up wizard in any detail — the important thing to note however is how to access your admin user unlock code, which you can find this from your running container via the following command: docker exec jenkins-blueocean cat /var/jenkins_home/secrets/initialAdminPasswordConfiguring Docker Cloud: Once you have your Jenkins set up complete, we need to set up running our Jenkins slaves as Docker containers. Head over to “Manage Plugins” and install the docker plugin.  Once installed, go to Manage Nodes and Clouds and then Configure Clouds and Add a new cloud. . The type of “docker” should automatically appear in the dropdown. You want to set the Docker Host URI to tcp://docker:2376 and then run a Test Connection. Unfortunately, you will see the following (misleading?) error and to solve this we need to set up the Server Credentials. (if you decided to skip TLS and are using port 2375, your test should have worked and you will be able to jump over the next steps).  Click on the “Add” drop down and create an “X. 509 Client Certificate”. Remember that /certs directory? At start-up the Docker-in-Docker container generates client/server keys and we need three pieces of information to successfully talk to it over TLS.    Client Key.     Client Certificate.     Server CA Certificate.  We can obtain these by running the following commands: docker exec jenkins-docker cat /certs/client/key. pemdocker exec jenkins-docker cat /certs/client/cert. pemdocker exec jenkins-docker cat /certs/server/ca. pemAfter creating your Credential, repeat the Test Connection and you should now be successful.  This is great — now we can reach the Docker daemon. Configuring Docker Slaves/Agents: Next step is to set up an agent to run our pipelines against. Once you start developing, you can start building your own build containers and attaching these instead. For now, we shall use the jenkins/agent image. Relevant fields below include giving it a label and name. The label allows you to associate a job to a particular agent — so you can always run a Maven build on an Agent that has maven installed, for example.  Our Connect Method will be Attach Docker Container. This runs the Docker container on the host machine (in our case, our host is Docker-in-Docker so that means we are running slaves inside the Docker-in-Docker container). Save everything and you are now ready to create a test job. Verifying it all works: To check that all is set up and working correctly, create a new freestyle job. You want to select the agent that this job will run on.  And down under build, write a simple “hello world” message as a test.  Save and build your new job. Jenkins will download the image for your container and run the job upon it. On completion you should get output such as that shown below.  All done. Success! What else?: If during execution you are interested to see which containers are running on your Docker-in-Docker container, run the following command. docker exec -it jenkins-docker docker psDocker Compose: Command line is all well and good, but it’s much easier to define as a docker compose yaml file. Start up with docker-compose up -d. version: '3. 8'networks: jenkins-network:  name: jenkinsvolumes: data:  name: jenkins-data certs:  name: jenkins-docker-certsservices: dind:  container_name: jenkins-docker  image: docker:dind  privileged: true  restart: unless-stopped  networks:   jenkins-network:    aliases:     - docker  volumes:   - data:/var/jenkins_home   - certs:/certs/client  environment:   - DOCKER_TLS_CERTDIR=/certs   jenkins:  container_name: jenkins-blueocean  image: jenkinsci/blueocean  restart: unless-stopped  networks:   - jenkins-network  ports:   - 8080:8080   - 50000:50000  volumes:   - data:/var/jenkins_home   - certs:/certs/client:ro  environment:   - DOCKER_HOST=tcp://docker:2376   - DOCKER_CERT_PATH=/certs/client   - DOCKER_TLS_VERIFY=1Alternatives to Docker-in-Docker: This article discusses the pros and cons of Docker-in-Docker and argues against using it for CI systems like Jenkins. I’ve followed up this article with two other approaches, parts 2 and 3 of this small series, that discuss alternatives that avoid using Docker-in-Docker. Run Jenkins in a Docker container - part 2 - socat Run Jenkins in a Docker container - part 3 - run as root user A note from the author: Thank you for reading this article and I hope you found time to read parts 2 and 3 as well — I hope you found them useful and via one of the articles you settled on a technique that suited your personal set up. All source code for these articles can be found on GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 13,
    "url": "https://goatsintrees.net/build-an-rpm-using-maven-and-spring-boot/",
    "title": "Build an RPM using Maven and Spring Boot",
    "body": "2020/06/15 - The goal of this article is to package a Spring Boot application as an RPM using the rpm-maven-plugin. We’ll not only cover the basics of creating the RPM, but include pre- and post-installation scripts to ensure you’re up and running and ready to get stuck in to build your own application. Let’s get started. Spring Boot application: For the purpose of this demonstration we are going to use the Hello World example detailed on the Spring Boot Quickstart. You can bring your own application or get started by using the source code that has been prepared for this article and can be cloned from GitHub. I’m running on CentOS 7 using Java 11 and Maven 3. 6. 3. I’ve also got the rpm-build package installed to my server. Build using mvn package and verify the application runs as expected by using java -jar target/demo-application-0. 0. 1-SNAPSHOT. jar (or just use mvn spring-boot:run if you prefer). Assuming all went well, you should be able to hit http://localhost:8080/hello in a browser to see your running Spring Boot application.  Now let’s work through packaging this application as an RPM. Multi-module project structure: Personal preference here; I like to keep separate pom. xml as I’m not a fan of side effects or a pom that creates multiple artifacts (if a pom. xml says it creates a jar, that’s personally what I expect it to do). My source structure will be set up with two modules — app (for the Java code, building the Spring Boot executable jar) and rpm (for packaging as RPM). my-project/appmy-project/app/src/main/java/com/example/demo/DemoApplication. javamy-project/app/pom. xmlmy-project/rpmmy-project/rpm/pom. xmlmy-project/pom. xmlThe parent pom. xml merely specifics app and rpm as modules. The app pom. xml is the vanilla Spring Boot example with the following configuration added to spring-boot-maven-plugin. &lt;configuration&gt; &lt;executable&gt;true&lt;/executable&gt;&lt;/configuration&gt;Using this option allows you to run the jar directly, i. e. without java -jar, and just saves a little bit of configuration later when deploying the application using yum/rpm. From here on, I’ll focus on setting up the RPM package. Setting up the RPM package: As we begin to configure rpm/pom. xml first step is to specify the packaging. &lt;packaging&gt;rpm&lt;/packaging&gt;Next, we need to ensure that we define that we are dependent upon the build of the application jar that we separated into the app module. Here I have referenced demo-application-app and I’ve used properties for the other values. &lt;dependencies&gt;. . .  &lt;dependency&gt;  &lt;groupId&gt;${project. groupId}&lt;/groupId&gt;  &lt;artifactId&gt;demo-application-app&lt;/artifactId&gt;  &lt;version&gt;${project. version}&lt;/version&gt; &lt;/dependency&gt;. . . &lt;/dependencies&gt;Now we can set up and configure the rpm-maven-plugin itself. We’ll cover the configuration entries next. Ensure you explicitly set extensions=true as highlighted because this is what allows us to have set packaging to “rpm” earlier. &lt;build&gt; &lt;plugins&gt; . . .   &lt;plugin&gt;   &lt;groupId&gt;org. codehaus. mojo&lt;/groupId&gt;   &lt;artifactId&gt;rpm-maven-plugin&lt;/artifactId&gt;   &lt;version&gt;2. 2. 0&lt;/version&gt;   &lt;extensions&gt;true&lt;/extensions&gt;   &lt;configuration&gt;   . . .    &lt;/configuration&gt;  &lt;/plugin&gt; . . .  &lt;/plugins&gt;&lt;/build&gt;Configuring the rpm-maven-plugin: All the following changes will go into the &lt;configuration&gt; section. We can use default values for most properties; however, you will need to set a value for &lt;group&gt; as it is mandatory and doesn’t come with a default value. &lt;group&gt;${project. groupId}&lt;/group&gt;With that out of the way, we’re onto file mappings and setting up pre- and post- install scriptlets. The fun stuff. For the purpose of this demonstration, I’m going to configure the application install directory to be /var/demo-application and the user/group will be demo-application. Since this user doesn’t exist by default, we have to create it in a pre-install scriptlet — we’ll come to that later. All file mappings are defined inside &lt;mappings&gt; as a series of &lt;mapping&gt; entries. These can take the form of a single source-to-target file copy, or a whole directory. Our first file will be the application jar itself which comes from our build dependencies. The format should be self-explanatory, and including stripVersion=true will ensure that the jar name is consistent. &lt;mapping&gt; &lt;directory&gt;/var/demo-application/&lt;/directory&gt; &lt;filemode&gt;755&lt;/filemode&gt; &lt;username&gt;demo-application&lt;/username&gt; &lt;groupname&gt;demo-application&lt;/groupname&gt; &lt;dependency&gt;  &lt;stripVersion&gt;true&lt;/stripVersion&gt;  &lt;includes&gt;   &lt;include&gt;${project. groupId}:demo-application-app&lt;/include&gt;  &lt;/includes&gt; &lt;/dependency&gt;&lt;/mapping&gt;At this stage, you have the basic structure. We can even test it now; run mvn package and you should see the initial RPM generated.  Verify its current contents using rpm -qpl and you will see the basic filesystem structure is coming together. /var/demo-application/var/demo-application/demo-application-app. jarInstalling this RPM would definitely work, but we have a couple of things missing still;    Our user / group doesn’t exist — so installing at this stage would default the directory and jar to root user.     Our application won’t currently run as a service, it won’t start automatically when the server reboots, and so on.  Next we will demonstrate how to achieve both of those steps using pre- and post- install scriptlets respectively. Scriptlets: There are a number of scripts that can be included in the RPM package which are executed as part of the installation, removal, or verification of the package. We’re going to focus on preinstallScriptlet, which runs before the package is installed and corresponds to the %pre tag in a spec file, and postinstallScriptlet, which runs after the package is installed and corresponds to the %post tag. Let’s add these references to the configuration and write the two shell scripts. &lt;preinstallScriptlet&gt; &lt;scriptFile&gt;src/main/resources/preinstall. sh&lt;/scriptFile&gt; &lt;fileEncoding&gt;utf-8&lt;/fileEncoding&gt; &lt;filter&gt;true&lt;/filter&gt;&lt;/preinstallScriptlet&gt;&lt;postinstallScriptlet&gt; &lt;scriptFile&gt;src/main/resources/postinstall. sh&lt;/scriptFile&gt; &lt;fileEncoding&gt;utf-8&lt;/fileEncoding&gt; &lt;filter&gt;true&lt;/filter&gt;&lt;/postinstallScriptlet&gt;Beginning with our preinstall script, we want to ensure that the user and group are created so we’ll use the useradd command. /usr/sbin/useradd -c  Demo Application  -U \ -s /sbin/nologin -r \ -d /var/demo-application demo-application 2&gt; /dev/null || :With our postinstall script, we want to ensure that we enable the service the first time that the RPM is installed. if [ $1 -eq 1 ] ; then  # Initial installation  systemctl enable demo-application. service &gt;/dev/null 2&gt;&amp;1 || : fiTalking of which, let’s look at the service itself. Since I’m targeting CentOS 7, then this demo includes the steps for using systemd. We’re going to include the demo-application. service file in our source repository and mappings. Create the demo-application. service file in your source code repository in directory src/main/resources/systemd. [Unit]Description=demo-applicationAfter=syslog. target[Service]User=demo-applicationExecStart=/var/demo-application/demo-application-app. jarSuccessExitStatus=143[Install]WantedBy=multi-user. targetAdd a new mapping that defines we want to copy this file over to the /etc/systemd/system directory on installation of the RPM. &lt;mapping&gt; &lt;directory&gt;/etc/systemd/system&lt;/directory&gt; &lt;filemode&gt;755&lt;/filemode&gt; &lt;username&gt;root&lt;/username&gt; &lt;groupname&gt;root&lt;/groupname&gt; &lt;sources&gt;  &lt;source&gt;   &lt;location&gt;src/main/resources/systemd&lt;/location&gt;  &lt;/source&gt; &lt;/sources&gt;&lt;/mapping&gt;Build, package, and install: Now we are ready to do our test build of the Spring Boot application (jar) and package the application as an RPM ready for installation. Build and package the application by running mvn package. You could run a mvn deploy to distribute the RPM — e. g. to Nexus or Artifactory. On successful completion, you will have an RPM in the target directory. Test it locally by running yum localinstall as shown below. yum localinstall rpm/target/rpm/demo-application-package/RPMS/noarch/demo-application-package-0. 0. 1-SNAPSHOT20200614204300. noarch. rpmOnce the RPM has been installed to your server, let’s start the application using systemctl start demo-application (applicable to CentOS, RHEL, Amazon Linux 2) and check that it is running using systemctl status demo-application. Once running, we can test the application works. Local command using curl. # curl http://localhost:8080/helloHello World!And finally in a browser using the public IP address.  Conclusion: In this post we touched upon the technique for using the rpm-maven-plugin for building an RPM for a Spring Boot application and installing to a server. We barely scratched the surface, and using the same patterns you can continue to explore the techniques of configuration, logging, and including more resources, scripts, and installation commands. Good luck with your application. References:    Spring Boot installation docs     rpm-maven-plugin docs  A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. Source code is available from GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 14,
    "url": "https://goatsintrees.net/build-a-docker-image-using-maven-and-spring-boot/",
    "title": "Build a Docker image using Maven and Spring Boot",
    "body": "2020/06/07 - The three approaches walked through in this article are;    Using the integrated Spring Boot build-image goal.     Using the jib-maven-plugin from Google.     Using the dockerfile-maven-plugin from Spotify.  Let’s get started. Spring Boot application: For the purpose of this demonstration we are going to use the Hello World example detailed on the Spring Boot Quickstart. You can bring your own application or get started by using the source code that has been prepared for this article and can be cloned from GitHub. I’m using Java 11 and Maven 3. 6. 3. Build using mvn package and verify the application runs as expected by using java -jar target/demo-application-0. 0. 1-SNAPSHOT. jar (or just use mvn spring-boot:run if you prefer). Assuming all went well, you should be able to hit http://localhost:8080/hello in a browser to see your running Spring Boot application.  Now let’s work through options to turn this into a Docker image. #1 Using the integrated Spring Boot build-image goal: Spring Boot comes pre-shipped with its own plugin for building Docker images and you don’t need to make any changes, as it’s available through the standard spring-boot-starter-parent that is included in your pom. xml. As an added bonus, you do not need to write a Dockerfile either and the plugin takes care of Spring-recommended security, memory, and performance optimizations. But be warned; if there is a Dockerfile located within your source code repository, it will be ignored. So with no further code changes necessary, simply run the command mvn spring-boot:build-image On completion you should see a message indicating the Docker image has been successfully created.  By default, you can see that Spring Boot creates the image with the artifactId as the name and the version as the tag. If you want to change these details, you can provide an extra argument on the command line as follows -Dspring-boot. build-image. imageName=myrepo/myimage Once built, we can run our new Docker image. docker run -p 9090:8080 -t demo-application:0. 0. 1-SNAPSHOT (note change of port to 9090).  #2 Using the jib-maven-plugin from Google: Jib is a Maven and Gradle plugin for creating Docker images for Java applications. A benefit is that it does not require Docker to be installed locally* which can be useful for Continuous Integration / build server — the jib-maven-plugin will build and push the image straight to the Docker registry of choice. *(although if you do have Docker installed, you can use it). Just like the native Spring Boot example, it does not require you to have written a Dockerfile. In fact, you will have noticed that we have yet to make any code changes at all beyond our initial class and pom. xml files. Since we are not pushing to a Docker registry in this tutorial, we will tell Jib to use our local Docker installation by using the dockerBuild goal. To build the Docker image, run the command mvn compile com. google. cloud. tools:jib-maven-plugin:2. 3. 0:dockerBuild On completion you should see a message indicating the Docker image has been successfully created.  Once built, we can run our new Docker image. docker run -p 9091:8080 -t demo-application:0. 0. 1-SNAPSHOT (note change of port to 9091).  #3 Using the dockerfile-maven-plugin from Spotify: The final walk through in this post is to build our Docker image using the dockerfile-maven-plugin from Spotify. This is the first of our three approaches where we need to have a Dockerfile — for the purpose of demonstration there’s a Dockerfile ready-made in the GitHub repository. Add your Dockerfile into your root directory alongside your pom. xml. FROM adoptopenjdk/openjdk11:alpineRUN addgroup -S spring &amp;&amp; adduser -S spring -G springUSER spring:springVOLUME /tmpARG JAR_FILEADD ${JAR_FILE} /app/app. jarEXPOSE 8080ENTRYPOINT [ java , -Djava. security. egd=file:/dev/. /urandom , -jar , /app/app. jar ]Next we will add some configuration into the pom. xml, specifically repository tag and an argument for the JAR_FILE which you can see we reference in the Dockerfile above. &lt;plugin&gt; &lt;groupId&gt;com. spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1. 4. 13&lt;/version&gt; &lt;executions&gt;  &lt;execution&gt;   &lt;id&gt;default&lt;/id&gt;   &lt;goals&gt;    &lt;goal&gt;build&lt;/goal&gt;    &lt;goal&gt;push&lt;/goal&gt;   &lt;/goals&gt;  &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt;  &lt;repository&gt;${project. artifactId}&lt;/repository&gt;  &lt;tag&gt;${project. version}&lt;/tag&gt;  &lt;buildArgs&gt;   &lt;JAR_FILE&gt;target/${project. build. finalName}. jar&lt;/JAR_FILE&gt;  &lt;/buildArgs&gt; &lt;/configuration&gt;&lt;/plugin&gt;Run the standard command mvn package and on completion you should see a message indicating the Docker image has been successfully created.  Once built, we can run our new Docker image. docker run -p 9092:8080 -t demo-application:0. 0. 1-SNAPSHOT (note change of port to 9092).  Conclusion: In this tutorial we walked through three techniques for using Maven plugins to build a Docker image for your Spring Boot application. We walked through using Spring Boot natively, and the Jib plugin from Google, both of which will work without needing to create a Dockerfile. Finally, we looked briefly at the Dockerfile-maven plugin written by Spotify which is now mature and stable, but no longer being actively enhanced.  References: For all configuration options check out these references.    Google jib-maven-plugin     Spotify dockerfile-maven plugin  A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. Source code is available from GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 15,
    "url": "https://goatsintrees.net/expand-ec2-boot-disk-space-without-server-downtime/",
    "title": "Expand EC2 boot disk space without server downtime",
    "body": "2020/05/22 - Like I did this week, at some stage you will will find you have run out of disk on your EC2 instance. I was installing a suite of new packages via yum and got the dreaded errors that my disk was full. Nothing to fear… (this article is how to expand the disk, rather than finding out where it all went). tl;dr:    Increase the size of the disk in AWS Management Console.     Confirm filesystem is “xfs” by using df -Th     Verify partition size and mount using lsblk     Extend the partition using sudo growpart /dev/xvda 1     Grow the filesystem using sudo xfs_growfs -d /  Full instructions: Using the command df -Th we can see the filesystems and their types. Ours is /dev/xvda1 of type xfs (slightly different instructions if ext2, ext3, ext4). [ec2-user@ip-172-31-63-153 ~]**$ df -Th**Filesystem   Type   Size Used Avail Use% Mounted ondevtmpfs    devtmpfs 475M   0 475M  0% /devtmpfs     tmpfs   492M   0 492M  0% /dev/shmtmpfs     tmpfs   492M 404K 492M  1% /runtmpfs     tmpfs   492M   0 492M  0% /sys/fs/cgroup**/dev/xvda1   xfs    8. 0G 1. 4G 6. 7G 18% /**tmpfs     tmpfs   99M   0  99M  0% /run/user/1000We’re going to resize the boot disk from 8GB to 16GB. Head over to the AWS Management Console and we can see our boot disk in the EC2 instance configuration details —from there we can follow the link to EBS to the particular volume — right click, and hit “Modify Volume”. Before 8GB Expand to the new size and save. It may take a few seconds while AWS is optimizing the new disk. After 16GB Head back to your EC2 instance and you will see from df -Th that the disk remains at its old size. That’s because we have a few commands to run yet. Firstly, run lsblk to confirm we can see the new disk size. [ec2-user@ip-172-31-63-153 ~]**$ lsblk**NAME  MAJ:MIN RM SIZE RO TYPE MOUNTPOINTxvda  202:0  0 16G 0 disk └─xvda1 202:1  0  8G 0 part /Now we can grow the partition using sudo growpart /dev/xvda 1 [ec2-user@ip-172-31-63-153 ~]**$ sudo growpart /dev/xvda 1**CHANGED: partition=1 start=4096 old: size=16773087 end=16777183 new: size=33550303 end=33554399Still not quite there. If we run df -Th again we will still see that our / mount is showing as 8GB. The final command is to grow the xfs filesystem — we do this by running the command sudo xfs_growfs -d /. [ec2-user@ip-172-31-63-153 ~]**$ sudo xfs_growfs -d /**meta-data=/dev/xvda1       isize=512  agcount=4, agsize=524159 blks     =            sectsz=512  attr=2, projid32bit=1     =            crc=1    finobt=1 spinodes=0data   =            bsize=4096  blocks=2096635, imaxpct=25     =            sunit=0   swidth=0 blksnaming  =version 2       bsize=4096  ascii-ci=0 ftype=1log   =internal        bsize=4096  blocks=2560, version=2     =            sectsz=512  sunit=0 blks, lazy-count=1realtime =none          extsz=4096  blocks=0, rtextents=0data blocks changed from 2096635 to 4193787And that’s done. Check again with df -Th and we will see the resized disk available to our filesystem. Quick, easy, and with no server downtime. [ec2-user@ip-172-31-63-153 ~]**$ df -Th**Filesystem   Type   Size Used Avail Use% Mounted ondevtmpfs    devtmpfs 475M   0 475M  0% /devtmpfs     tmpfs   492M   0 492M  0% /dev/shmtmpfs     tmpfs   492M 408K 492M  1% /runtmpfs     tmpfs   492M   0 492M  0% /sys/fs/cgroup**/dev/xvda1   xfs    16G 1. 4G  15G  9% /**tmpfs     tmpfs   99M   0  99M  0% /run/user/1000Conclusion: In this article we quickly walked through expanding the boot disk of an EC2 instance without taking the server offline. It’s quick and trivial to do — so no need to panic, stay calm and work through the simple steps. Thank you for reading this article — I hope you found it useful — you can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 16,
    "url": "https://goatsintrees.net/build-your-docker-images-automatically-when-pushing-new-code-to-github/",
    "title": "Build your Docker images automatically when pushing new code to GitHub",
    "body": "2020/05/10 - Overview: This article will outline two techniques for automating the build of Docker images after every push or merge to the master branch in GitHub. The built images will be pushed to a Docker Hub container repository. The two approaches being demonstrated are;    using Docker Hub – use GitHub webhooks to notify Docker Hub about code changes and trigger the build of a new Docker image within Docker Hub itself.     using GitHub Actions – using GitHub’s service for running Continuous Integration pipelines, we will build the new Docker image within GitHub machines and push the image to Docker Hub.  Prerequisites: To begin this tutorial, you will need to set yourself up with accounts on GitHub and Docker Hub.    GitHub account (register here)   Within your GitHub account, create a repository with a Dockerfile and any additional source code. I’m using a simple Hello World example for the purposes of this article.   “Hello World” Dockerfile     Docker Hub account (register here)  Approach #1 — using Docker Hub to build image: In the first approach, we will configure Docker Hub to receive notifications from GitHub whenever there are any changes to our source repository. This is achieved using GitHub webhooks. On receipt of the notification, Docker Hub will build the new Docker image and publish it for consumption.  Step 1. Associate your GitHub and Docker Hub accounts. : Within Docker Hub visit Account Settings &gt; Linked Accounts and click “Connect” to allow access to your source repositories.  Step 2. Create container repository in Docker Hub: Within Docker Hub create a new repository and under “Build Settings” click on the GitHub icon to associate your source code repository.  From the drop-down select your GitHub organisation (this will default to your username) and the source code repository. Step 3. Configure Build Rules: There are many options available, and several examples are displayed in the help. For the purpose of this demo, we’ll keep the default settings and set up the trigger to be on pushes to the master branch and use the latest docker tag.  Other options exist to create Docker images from tags or release branches, and the pattern matching allows you to create dynamic image tags. Example Build Rule options Click “Create &amp; Build” to set up your new container repository and build your first Docker image — the first build will trigger automatically. Step 5. Viewing builds. : Within Docker Hub you will find the build status in the “General” tab of your repository. Viewing the “Builds” tab will show more information about each build. From here you will see the status of jobs and view build logs.  Once built we can pull and run the newly built image. $ docker run davelms/hello-world:latestHello world!Step 6. Push new source code changes: Final step is to simulate a code change — I’ll do this by editing the “Hello world!” message — and push the new commit up to GitHub. GitHub will send a notification to Docker Hub about the code change; Docker Hub will initiate a new build; and we can run that new image once completed. GitHub commit log Immediately after pushing the code, we see a new build initiated in Docker Hub. You can see that the commit sha is referenced — tip: you can use this in tags should you wish instead of “latest”. Docker Hub build history When the build has finished, update locally with docker pull and re-run your container. This time we see the output from our new source code. $ docker run davelms/hello-world:latestHello Dave!Approach #2 — using GitHub Actions to build Docker images: This time we will use GitHub’s custom CI/CD platform — GitHub Actions — to build the Docker image after every push to the source code repository. The workflow will define a single job that builds the image and pushes the new image to Docker Hub.  Step 1. Create a Docker Hub security access token: First of all, within Docker Hub create yourself an access token by visiting Settings &gt; Security. Give it a name you recognise it later.  Once created, head over to GitHub and create secrets within your source code repository for DOCKER_HUB_USERNAME and DOCKER_HUB_TOKEN, along with DOCKER_HUB_REPOSITORY.  Step 2. Create a GitHub Action to build and push images: Keeping within GitHub, head into the “Actions” tab of your source code repository. It’s likely that it will have detected the Dockerfile and will recommend you Docker-related workflow examples to get you started. Since I will share an example, skip these helpers for now and select “set up a workflow yourself”. Use the workflow definition below and commit the file. name: Docker Image CIon: push:  branches: [ master ]jobs: build:  runs-on: ubuntu-latest  steps:  - uses: actions/checkout@v2  - name: Build the Docker image   run: |    echo  ${{ secrets. DOCKER_HUB_TOKEN }}  | docker login -u  ${{ secrets. DOCKER_HUB_USERNAME }}  --password-stdin docker. io    docker build . --file Dockerfile --tag docker. io/${{ secrets. DOCKER_HUB_USERNAME }}/${{ secrets. DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA    docker push docker. io/${{ secrets. DOCKER_HUB_USERNAME }}/${{ secrets. DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA   What does this CI pipeline do?    Defines that we will trigger a job after all pushes to master branch.     Configures a single job called “build” to run on an Ubuntu machine.     The source code repository will be checked out and will run three inline commands: 1. docker login 2. docker build 3. docker push. These will use our secrets we saved off earlier.     Note that the example uses the commit sha as the image tag — GitHub Actions makes the sha available in an environment variable called GITHUB_SHA. Of course, you could stick with “latest” like we did in the first example should you wish to do so.  Step 3. View the build: When the build has finished, the image will appear over on Docker Hub.  Now verify you can pull the new image down successfully. $ docker run davelms/hello-world:8517bf9c40f4cf198ea3313bd5ec3cc43176bd85 Unable to find image 'davelms/hello-world:8517bf9c40f4cf198ea3313bd5ec3cc43176bd85' locally8517bf9c40f4cf198ea3313bd5ec3cc43176bd85: Pulling from davelms/hello-worldd9cbbca60e5f: Already existsDigest: sha256:f8808c8b2ae19f6f3700e51a127e04d8366a1285bdfc6e4006092807f0eced1bStatus: Downloaded newer image for davelms/hello-world:8517bf9c40f4cf198ea3313bd5ec3cc43176bd85Hello Dave!Step 4. Push new source code changes: Final step is to simulate a code change — I’ll do this by editing the message back to “Hello world!” — and push the new commit up to GitHub. Commit log After the push, a new CI/CD workflow containing our “build” job is run. CI Pipeline history The new image will have been pushed to Docker Hub successfully.  This time we see the output from our new source code — the message is back to “Hello World!”. All is working as expected. $ docker run davelms/hello-world:a26c53183fac84a9c7ce128ce6ae6250fae26c7dUnable to find image 'davelms/hello-world:a26c53183fac84a9c7ce128ce6ae6250fae26c7d' locallya26c53183fac84a9c7ce128ce6ae6250fae26c7d: Pulling from davelms/hello-worldd9cbbca60e5f: Already existsDigest: sha256:3cfa80d4fa8c51271928f0294d89293a7a7fc7022a416a1758fc37394bc12808Status: Downloaded newer image for davelms/hello-world:a26c53183fac84a9c7ce128ce6ae6250fae26c7dHello World!Conclusion: In this article we walked through two techniques for automating the build of Docker images after every source code change in GitHub. In both examples, we pushed the images into Docker Hub. Firstly, we used webhooks and built the image directly within Docker Hub where the “free plan” offers the ability to create images (the limitation with this plan is that it limits to building one image at a time). Next, we used GitHub Actions to build a Continuous Integration pipeline and push the built image to Docker Hub — we could extend this pipeline later to include some unit and integration tests. GitHub Actions provides 2,000 minutes of machine time per month under its “free plan”. Thank you for reading this article — I hope you found it useful — you can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 17,
    "url": "https://goatsintrees.net/five-minutes-create-a-free-static-website-using-google-sites/",
    "title": "Create a free website in 5 minutes using Google Sites",
    "body": "2020/05/08 - Setting up a basic yet professional looking static website could not be easier with Google Sites. Let’s explore how in this quick introduction, where we will create a free website and host it off our own custom domain. Your site could be up and running inside 5 minutes. Although Google Sites is mainly used for intranet sites within an organisation, where it can embed the contents of Google Docs or Google Sheets, it’s also a perfectly acceptable proposition for building up an individual portfolio, sporting team website, or a shop like a restaurant.  You can embed third party content (technically, this gets implemented inside an iframe) and style this content to fit your chosen theme. You can use this for contact forms, for example. A site generated with Google Sites is mobile-first, and will render correctly on mobile, tablet, and laptop without any configuration from the user. You can also hook in Google Analytics for visitor tracking and analysis of your user behaviour and how they found your site. Finally, you can configure to run off your own custom domain. The one pre-requisite is that you have a Google Account. Let’s get started. Create your first Google Site: Visit Google Sites. I’d recommend that you stick with “New”, but you can drop back into “Classic” if you wish. Classic has more templates/themes but is more clunky to work with, whereas New is cleaner and fresher but the options are limited. Templates: Start off with a blank page or click on a template to create your first site. At the time of writing, the following templates are available to choose from in the categories of Personal, Work, and Education. Personal:  Portfolio Restaurant Work:  Event Team Help Centre Project Education:  Class Club Student portfolio Themes: There are 6 themes available to select and these apply across all the Templates. A theme will provide a basic style (font, headers, and colours) and you can pick pre-selected options or choose your own colour from a picker.  Simple Aristotle Diplomat Vision Level ImpressionThere’s a lot of trial-and-error involved to select the theme that matches your personal preferences — I recommend working through them all. Creating a new page: New pages can be created using the menu, and these can be inserted underneath a parent page to create a drop-down hierarchy. All pages will appear in the navigation unless you select to hide.  Talking of the Navigation, you can choose to position your page navigation at the top or side of your page.  Embedding content: You can embed images, documents, spreadsheets, and **slides **into your pages. These can be hosted on your Google Drive, or you can upload content directly from your local machine, or from a URL. Images can also be inserted from a Google search — good feature is that Google will automatically filter only those that are licensed for free use. You can include HTML in your page, including JavaScript and CSS stylesheets. This is useful when embedding third party content — such as a email contact form or a mailing list sign up sheet. Company Logo and favicon: Google Sites gives you full control to brand your site. Under the Settings, you can configure your company logo to appear in the Navigation bar and you can also configure the favicon that appears in the browser tab.  Once set your company logo will appear in the Navigator bar at the top of the page.  Publish: When you are ready to publish your site, hit the Publish button to the upper right. Select a name for your site and that’s it; your site will now be live to the world on the https://sites. google. com/view/your-site-name address. Should you edit your site, you’ll need to re-publish your changes. This allows you to work on your site and publish a new version once you are ready. Using your own domain: Running your site off a custom domain requires a little bit of technical know-how, but I shall walk through the steps. For those super technical, you can’t host off a domain apex — example. com — instead it has to have a subdomain, so www. example. com is acceptable. First of all, you need to prove to Google that you own the domain. If you already have your sites listed in Google Webmaster Central you will be familiar with the technique, and it’s the same here. Within the Settings panel, you can add up to 10 URLs that can be used for your Google Site. Enter the first in the box.  If Google knows that you own the domain it will automatically configure it. Otherwise, you will see a warning to ask you to verify your ownership.  Follow the instructions, and you’ll be asked to add a TXT record to your DNS. google-site-verification=Zakx97MOoIOGmDq_kXNOo133YvVqUNq0DJl9HsyWCP4Don’t worry if you never did this before, Google provides instructions for most major hosting providers. Once you’ve added this TXT record, it will take Google approximately 5 minutes to detect and confirm your ownership. While you’re on your hosting provider site, stay within the DNS settings for the next step too — you need to create a CNAME record too. This has to be subdomain (e. g. www. yourdomain. com) and the value will be ghs. googlehosted. com. Create and save this CNAME record and this will point your custom domain to your Google hosted website. A note from the author: Thank you for reading this article — I hope you found it useful. Sometimes the easiest options are the best and for a free solution, you can’t go far wrong with Google Sites. Yes, it’s basic, and yes, there are other platforms out there like Wix. But it gets the job done and it’s really easy to use. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 18,
    "url": "https://goatsintrees.net/solve-a-sudoku-using-javascript/",
    "title": "Solve a Sudoku with JavaScript",
    "body": "2020/05/02 - Rules of the game: A brief summary for those that are not familiar with the rules of Sudoku. A completed Sudoku is a 9 x 9 grid, where each row, column, and each 3 x 3 square must contain each of the numbers 1–9 (and once only). The grid below depicts a single row, column and square adhering to these constraints.  Games are presented with a partially completed grid and degrees of complexity given how many cells are pre-populated and how those are positioned (offering ‘clues’). They range from Easy through to Expert. Game input: For the benefit of the challenge, we can assume we will begin the game with an array of length 9 (rows). Each row is itself an array of length 9 (cells). Values will be integers 0–9, where zero indicates a blank. Example Sudoku taken from Wikipedia Helper functions: Regardless of algorithm (I will talk later through two approaches I took), a few helper functions are always useful. Source code is shared at the end.    get_row(board, row) — since the game board is an array of rows, this is trivial and just a luxury method !     get_column(board, column) — return all cells for the provided column.     get_square(board, square) — we can logically split the board into nine 3 x 3 squares, and given (row,col) coordinates we can identify the square and then return all the cells from within the same square.     is_solved(board) — given a completed board, is it correct i. e. have we finished the puzzle? This function should check every row, every column, and every square has the full set of values 1–9.     print_board(board) — a function that helps visualize the board/grid. The output below is the starting position of the board for the game array pictured earlier.  |=======|=======|=======|| . . . | . . . | . . . || . . . | . . 3 | . 8 5 || . . 1 | . 2 . | . . . ||=======|=======|=======|| . . . | 5 . 7 | . . . || . . 4 | . . . | 1 . . || . 9 . | . . . | . . . ||=======|=======|=======|| 5 . . | . . . | . 7 3 || . . 2 | . 1 . | . . . || . . . | . 4 . | . . 9 ||=======|=======|=======|Two approaches: I came to this challenge from two perspectives and in the end I used both techniques — using a combination was quicker for complex Sudoku puzzles. Approach 1. The first approach is applying brute force to the problem. We can apply a backtracking algorithm by iterating over all the empty cells, beginning with the value 1 and checking it’s valid. We then move to the next empty cell, set that to 1 and check it’s valid. And so on. At any point we encounter an impossible solution (a cell with no valid values), we try the next value, and then the next. Should we exhaust all combinations in the current cell, we backtrack a loop, increment the previous cell value and begin again. With such an approach, it’s not uncommon to backtrack all the way to start several times. To implement this approach we can use a simple loop and apply recursion (a function that calls itself with an altered or cut-down problem space) — the logic is only a few lines. I found a brute force approach solved all Sudoku games — but that came at a performance cost. (albeit a fraction of a second). Approach 2. That’s why I introduced a second approach — one value cell constraint function — which when used on its own solved all the Easy-Medium problems I threw at it, without ever needing to fallback to brute force at all. Here we apply techniques a person solving a puzzle would use — as such the logic is quite a bit longer. We take each empty cell and identify the possible values it can hold — updating each blank cell as an array of its possible values. If any cell can contain one value only, we formally set it to that value in the master game grid. So in this rather trivial example, the cell marked “?” only has a single possible value (9).  What if we have more than one possible value for a cell? In this case, we compare each possible value x to the others cells in the row, column, and square. If x only appears as a possibility for this cell, regardless of what other possibilities this cell may have, it is guaranteed that this cell has this value. To elaborate, in this next example the cell marked “?” could at first-pass have the possibilities [1, 2, 3, 4, 5, 8, 9]. However, when we look at either the row or the square, we would not find the value 1 as a possibility in any other cell (marked as an ‘x’). Therefore we know 100% that the cell “?” has to be where the 1 gets placed. A human would simply apply this rule in their head.  Approach two takes iterating these two loops throughout the table until no more updates can be applied — and it solves most simple to medium complexity Sudoku problems. It also makes cracking a complex Sudoku— via brute force — much quicker too, because we have pre-populated many of the cells ahead of time. Final solution: The solution is a combination of the two approaches;    One value cell constraint — most non-complex solutions are solved here.     Brute force / backtracking — within here we can re-use logic from (1), i. e. each time we pick a value we can then “look ahead” and fill in any cells that now only have a single possible value, cutting the problem space down (or quickly eliminating the selection we made as impossible).  The combination of both approaches solved my earlier performance issue with complex Sudoku challenges with using brute force alone. The code: The entry function is solved(board) and this calls functions implementing the two approaches previously described. We iterate over our one value cell constraint function, so long as updates are still being applied (i. e. cells are being filled in because they have only one possible value). That loop ends when we have either solved the Sudoku or it is no longer filling in any cells. If we have not solved the Sudoku, we then fall back to the brute force function. We assume the grid can be solved — i. e. no trying to catch us out — so at the end we return the board as a completed solution. function solve(board) { let updated = true, solved = false   while (updated &amp;&amp; !solved) {  updated = one_value_cell_constraint(board)  solved = is_solved(board) } if (!solved) {  board = backtrack_based(board) } return board}In the one_value_cell_constraint function (shown below) we keep record of updated and any change to the grid keeps us going for another iteration — as we are live editing the game grid, each loop within the function builds upon the previous updates. Internally, we make use of a function called complete_cell which we also use in the brute force logic later. This function looks for all possible values for the cell — if there is a single value, it sets the cell to that value; if there are multiple values it sets the cell to an array containing all its possible values (e. g. [1, 4, 5]). The next section picks up any cells having a range of possibilities and looks at the corresponding row, column, and square to see if a possible value appears_once_only. If it does then we set the cell to that value. function one_value_cell_constraint(board) { updated = false // Convert every gap into an array of possibilities for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (board[r][c] == 0) {    updated = complete_cell(board, r, c) || updated   }  } } // Look out for any possibility that appears as a possibility // once-only in the row, column, or quadrant.  for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (Array. isArray(board[r][c])) {    let possibilities = board[r][c]    updated =      appears_once_only(board, possibilities,        get_row(board, r), r, c) ||     appears_once_only(board, possibilities,        get_column(board, c), r, c) ||     appears_once_only(board, possibilities,        get_square(board, square_coordinates[r][c]), r, c) || updated   }  } } // Reinitialize gaps back to zero before ending for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (Array. isArray(board[r][c])) {    board[r][c] = 0   }  } } return updated}Finally, the brute force logic is in our backtrack_based function. As we may follow dead ends, we work against copies of the array — even though JavaScript passes the board in by value, all the objects within the array are references — the JSON library helps deep-clone the array. We iterate each empty cell and initially re-use the complete_cell function we referred to earlier. This will fill in the cell if it’s only got a single possible value — useful when we get to the very last empty cell, this will essentially solve the board so we include a is_solved check here and return the solved board. Assuming instead we get a list of possible values, we iterate these possibilities in turn — set the cell to the first value in the list, and recursively call the function. At some stage, one recursion will end successfully (with complete_cell filling in the final cell), but most likely we will hit a ‘dead end’. Should we exhaust all possible values, we return false and backtrack. function backtrack_based(orig_board) { // Create a temporary board for our recursion.  let board = JSON. parse(JSON. stringify(orig_board)); for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (board[r][c] == 0) {    complete_cell(board, r, c)    if (is_solved(board)) return board;    let cell = board[r][c]    if (Array. isArray(cell)) {     for (let i = 0; i &lt; cell. length; i++) {      // Create a temporary board for each recursion.       let board_2 = JSON. parse(JSON. stringify(board));      // Choose a value      board_2[r][c] = cell[i]      // Recurse again using new board      if (completed_board = backtrack_based(board_2)) {       return completed_board;      }     }     return false // dead end    }   }  } } return false;}Conclusion and examples: First of all — the code worked, including against the “Sudoku designed to work against the brute force algorithm”. Applying two different techniques together was a valuable learning point.  The code could be optimized, although performance is sub-second even for the most difficult. Copying arrays per recursion introduces a memory overhead that I could have worked through. Some example games and solutions:  Further Reading:    Source code for this solution can be found on GitHub.     Wikipedia article on “Sudoku solving algorithms”  A note from the author: Thanks for reading and I hope you enjoyed the article and found it useful. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 19,
    "url": "https://goatsintrees.net/use-ansible-to-create-and-configure-ec2-instances-on-aws/",
    "title": "Use Ansible to create and configure EC2 instances on AWS",
    "body": "2020/04/25 - Inspiration: I’ve been using the awesome A Cloud Guru “Cloud Playground” sandboxes to progress through some of their training content. These environments are fully-functional AWS accounts and allow the user to follow along the Cloud certification tutorials and training — a great new feature of A Cloud Guru. A sandbox lasts 5 hours, which is usually ample enough time to follow along, but every so often I find that I come back the next day and want to recreate some baseline infrastructure and pick up from where I left off. That inspired me to look at Infrastructure as Code and build out a repeatable platform from code blueprints. I’ve experience using Terraform by Hashicorp, and I understand using Terraform and Ansible together. I’m also aware of AWS CloudFormation, and also how I could snapshot an image and create an AMI. However, since Ansible is also a cloud infrastructure provisioning tool, my use-case looked a good challenge to demonstrate using the Ansible modules for AWS. Let’s get stuck in. Dependencies: On your machine, have the following installed.  Ansible Python ≥ 2. 6, with boto, boto3, and botocore. I also have AWS CLI, and have configured the “Cloud Sandbox” AWS Access Key and AWS Secret Id using aws configure. You don’t have to do this way, but I tend to find having the AWS CLI to hand as well is easier all round — follow the boto installation instructions for other options. What we’re going to create: Our goal is to create an EC2 instance in the default VPC — located in North Virginia (us-east-1) in the case of the Cloud Sandboxes. We’re going to make that EC2 instance accessible over ssh from our IP only. For that we will need to create an EC2 key pair. We’re going to ensure that the instance has a few tools — for the purpose of demonstration, we’ll let Ansible install packages onto the EC2 instance. Steps to follow:    Create an EC2 key pair (if one does not already exist — Ansible has built-in idempotency, one of is many plus points) and save the private key to file.     Determine information about the default VPC and its subnets. Randomly select a subnet from the list to host our EC2 instance.     Determine our public IP address and create a security group allowing ssh access from our IP address (only).     Create an EC2 instance in the selected subnet and associated with the security group, and we’ll update our inventory with the new host.     Install git and php to the jumpbox.     Check out our new instance.  Initialize project and set up our variables: I’ve started off the exercise with a brand new, empty project directory and in there created three empty directories: inventory/roles/keys/Navigate into inventory/ and create a file called local as shown below. [local]127. 0. 0. 1 ansible_connection=localCreate another file called ec2 with just the following contents. The playbooks will append host information into here later. [jumpbox]When we run our commands, we can specify this inventory directory with the option -i inventory and Ansible will pick up the contents from here. I created a roles/ directory and in there ran ansible-galaxy init create-ec2-instances to create a basic *roles *outline structure to manage the tasks. For the purposes of these tutorial, I’ve referenced the following variables which helps avoid some hard-coding in the tasks. The AMI is that of a standard Amazon Linux 2 in us-east-1. region_name: 'us-east-1'key_name: 'my_keypair'ami_id: 'ami-0323c3dd2da7fb37d'instance_type: 't2. micro'instance_name: 'jumpbox'Step 1. Create a new EC2 key pair. : Our first step is to let Ansible create a new EC2 key pair. We register the output and then we can write the private_key contents into a local pem file in the keys/ directory. Don’t forget the file permissions. - name: Create a new EC2 key pair ec2_key:  name:     region:     register: ec2_key- name: Save private key copy: content=   dest= . /keys/. pem  mode=0600 when: ec2_key. changedStep 2. Obtain networking information from AWS. : There are two pieces of AWS network information we want to know, and fortunately Ansible provides a way to query for both of these.    Default VPC     Subnets in that default VPC  If we were building out a larger piece of infrastructure — i. e. in our own AWS Account — we would probably want to create a brand new VPC and subnets, which is also feasible with Ansible. Here though, finding and using the default is sufficient. In the yaml below you will see three tasks.    Firstly, we filter the VPC list on whether it has the isDefault flag set. You can filter on all kinds of attributes and these match against the AWS CLI. We save the resultant response into default_vpc — you will see from the next step, it is an array (in our case, of 1 entry) and it is the value of vpc_id that we are interested in.     Secondly, we query the subnets to extract all those associated with the default VPC using the vpc_id as a filter. We register this as subnet_info     Finally, we use jinja to extract all the subnet id values from subnet_info into a list, and then select one at random — that’ll be the subnet we’ll create our instance into.  - name: Obtain default VPC information ec2_vpc_net_facts:  filters:    isDefault :  true  register: default_vpc- name: Obtain subnets for default VPC ec2_vpc_subnet_facts:  filters:   vpc-id:  {{ default_vpc['vpcs'][0]['vpc_id'] }}  register: subnet_info# Use jinja to select a random subnet from the list of subnet ids- set_fact:  vpc_id:  {{ default_vpc['vpcs'][0]['vpc_id'] }}   random_subnet:  {{ subnet_info. subnets|map(attribute='id')|list|random }} Step 3. Secure our instance. : We want to ensure that only we can access our new server. First of all, we ask Ansible what our public IP is. We can do this by using the ipify_facts module (top tip: you can run this straight from a command line using ansible -m ipify_facts to quickly get at this information too). Once we have our public IP address, we can create a new Security Group that allows SSH access only from that IP. ---# Gather IP facts from ipify. org, will be saved to ipify_public_ip- name: Get my public IP ipify_facts:# Create Security Group and save the output into security_group- name: Create Security Group ec2_group:  name:  -sg   description: Security Group for   vpc_id:     region:     rules:   - proto: tcp    ports:     - 22    cidr_ip:  /32     rule_desc:  allow port 22 from   register: security_groupStep 4. Create the EC2 instances. : We’re now ready for the final step and can create an EC2 instance. Most of the values we’ve set up as variables or picked up along the way (e. g. the vpc_subnet_id) so it’s just filling in the blanks at this stage. You’ll see we can combine exact_count=1 with instance_tags and count_tag to ensure that if we re-run the playbook, we will not create more instances. I’ve noticed through experimentation that this is applied *within the same subnet — *my random subnet selector means I create a few more instances than I wanted, but we could hard-code the subnet and ensure we do only get one. - name: Create EC2 instances ec2:  key_name:     region:     instance_type:     image:     vpc_subnet_id:     group:  -sg   wait: yes  instance_tags:   Name: jumpbox   Env: sandbox  count_tag:    Name: jumpbox   Env: sandbox  exact_count: 1  assign_public_ip: yes register: ec2 - name: Add the newly created EC2 instance(s) to the local host group local_action: lineinfile         path= inventory/ec2         regexp=         insertafter= [jumpbpox]          line=  ansible_user=ec2-user ansible_ssh_private_key_file=keys/. pem ansible_ssh_extra_args='-o StrictHostKeyChecking=no'  with_items:   Once the instance has been created, we append the new host in our inventory/ec2 file that we created at the start. You should find you get something like this.  Because we are dynamically updating the inventory, we can optionally include refresh_inventory and a pause for 30 seconds — gives AWS just enough time to start the VM up and ensure sshd is running. These are important if you’re heading straight into the configuration. - meta: refresh_inventory- pause:  seconds: 30Bringing Steps 1–4 together: At this stage, we’ve got all our tasks set up inside the create-ec2-instances role and our roles/create-ec2-instances/tasks/main. yml looks like this: - name: Create jumpbox in default VPC block:  - import_tasks: key-pair. yml  - import_tasks: network-information. yml  - import_tasks: security-group. yml  - import_tasks: ec2. yml  - meta: refresh_inventory  - pause:    seconds: 30We can create a playbook in the root project directory (call it what you like, I called mine create-ec2. yml). Note that we specify hosts: local for the AWS infrastructure tasks. # Create jumpbox on an EC2 instance- hosts: local gather_facts: False roles:  - role: create-ec2-instancesSimply run the command ansible-playbook create-ec2. yml -i inventory to run through the playbook to create our key, security group, and instance. Step 5. Configure the jumpbox. : In this playbook, we’re going to update yum so that the server is up to date and install a couple of packages — selected at random to demonstrate a step to set up of our jumpbox. I opted for git and php — it’s only a demo. Note our new playbook references hosts: jumpbox which picks out the hosts from the inventory/ec2 file that we dynamically appended to in Step 4. - hosts: jumpbox become: True gather_facts: True tasks:  - name: Upgrade all yum packages   yum:    name: '*'    state: latest  - name: Install packages   yum:    name:      vars:    packages:     - git     - phpStep 6. Check it all out. : At this stage;    we should have a jumpbox created in the default VPC in our AWS Account.     our instance should be accessible over ssh only to our IP address.     the jumpbox should have our packages pre-installed and ready to use.  Let’s check it out, just to be sure and ssh into it — remember the auto-generated private key has been saved into the keys/ directory.  All completed — the server is created and accessible. A quick check and we have git and php installed as expected. Job done. A note from the author: Thank you for reading this article — I hope you found it useful. As mentioned in the introduction, there are many ways to accomplish this task and I look forward to your comments and feedback. All source code for this demonstration can be downloaded at GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 20,
    "url": "https://goatsintrees.net/convert-an-old-php-application-to-docker-containers/",
    "title": "Convert an old PHP application to Docker containers",
    "body": "2020/04/19 - Backstory: PHP was always the go-to language for me. I started exploring PHP in 2000, around the time of PHP 4 launch, in an effort to convert an increasingly popular website that was written as lots of individual, static HTML pages. I began work on writing a Content Management System from scratch, and the CMS was still distributing 30,000 articles some 17 years later. This walk through is about another PHP application — it was written using the latest PHP 5 features at the time, and was left happily running in a single Amazon EC2 “classic” instance since 2015. When I started this migration, the last commit was July 30th 2016. Existing platform: The frontend was left running a variety of third party libraries, notably Bootstrap 3. 3 and jQuery 2. 2. The server-side was Symfony 3. 1 and templates were Twig 1. x. Data was obtained from an external API using guzzle and locally cached using doctrine cache 1. 6. Logging was done with monolog. Build toolkit included Node. js for npm (package manager), Grunt (javascript task runner), and Bower (web package manager). CSS stylesheets were written in sass, so we included a parser to create css, concatenate, and minimize. Javascript was consolidated, obfuscated (“uglified”), and also minified. To get a feel for the build steps, here’s the “build-all” grunt task: grunt. registerTask('build-all', ['clean:all', 'concat', 'uglify', 'sass', 'cssmin', 'bowercopy', 'copy', 'string-replace:version', 'composer:dist:install:optimize-autoloader', 'composer:dist:update:optimize-autoloader']);And finally, as you can see from the above, I was using Composer for PHP dependency management. And finally the server itself was using Nginx with php-fpm. Everything was stuck in the year 2015. My development server had long since been destroyed. I couldn’t do any changes; I didn’t even have PHP installed any more. Aspiration:    Upgrade front end to the latest versions of Bootstrap and jQuery.     Update server side runtime libraries to the latest version — apply necessary code changes required as a result, but no other functional changes.     Upgrade the built-kit to the latest versions — but no other changes to the tasks or tools used.     Deploy to an up-to-date operating system, with PHP 7. x.  My overarching constraint for the above was that I didn’t want to have to work through and recreate an entire development platform to do it. I wanted to find a way I could achieve the upgrade without needing to stack my Windows laptop with grunt, bower, nodejs, composer, PHP, just for this one task. I turned to Docker to save the day. Creating a build container: My build server required the following tools to be installed:  PHP 7. x Composer Node. js Grunt BowerTransitively, for SASS parsing, I subsequently found I needed:  RubyPHP and Composer (build time): Given this was used at build time, I wasn’t concerned about having a bloated image so I decided to go for a full PHP installation as the base image. I used the latest PHP image (on Alpine) for my build environment (which I gave the label “buildenv”). FROM php:7. 4. 1-alpine AS buildenvUnfortunately, Composer isn’t included by default and while I could follow the Composer installation instructions, we only need the final ‘composer’ binary file. Luckily, there’s an official Composer image to use. So we reference that image in our layers, and copy the composer binary over into our buildenv. FROM composer:1. 9. 1 AS composer FROM php:7. 4. 1-alpine AS buildenv COPY --from=composer /usr/bin/composer /usr/bin/composerAt this stage, we have an Alpine 3. 10 operating system with PHP 7. 4. 1 and Composer 1. 9 available. Node. js, Grunt, Bower (build time): Building upon the previous layer, I then looked to the other build tools I required. These I grouped into a single layer, and installed the packages and ensured npm was up to date. Job done. RUN apk add --update nodejs npm &amp;&amp; \  npm update -g npm &amp;&amp; \  npm install -g grunt-cli &amp;&amp; \  npm install -g bower Ruby (build time): A bit of trial and error determined that the sass parser required Ruby. This is in its own layer to keep it separate while I worked out the packages I needed to install. More than I thought and remembered. RUN apk add --update ruby ruby-bundler ruby-dev \  ruby-rdoc build-base gcc &amp;&amp; \  gem install sassCreate a working directory: I always like to do everything inside a working directory, so I create /app and work inside here for all subsequent stages. RUN mkdir /app WORKDIR /appDownloading the application dependencies: Remarkable looking back at all of the stages to get through before we can commence the application build. In summary, we have these steps for downloading the build time and runtime dependencies we require;    npm install — download the javascript build libraries into node_modules     composer install — PHP server side runtime dependencies, such as Twig and Symfony.     bower update — download all the client side runtime libraries, such as jQuery.  # Copy npm dependenciesCOPY package. json . RUN npm install # Copy composer and download dependenciesCOPY composer. json . RUN composer install # Copy bower and download dependenciesRUN apk add --update gitRUN echo '{  allow_root : true }' &gt; /root/. bowerrcCOPY bower. json . RUN bower updateTo briefly explain the above techniques.    I copy only the files needed for the job (e. g. package. json, composer. json, or bower. json) — that’s so that I can isolate changes and limit recreating layers unnecessarily. Some of these steps can take 5 minutes to complete so I don’t want to trigger a full rebuild because I changed an unrelated file.     Bower — needs git, so this stage I added that. Secondly, bower throws a warning when it is run as ‘root’ user — so the second line suppresses that.  So now we have a build environment that has an Alpine 3. 10 operating system with PHP 7. 4. 1 and Composer 1. 9 available. It has the latest versions of nodejs, grunt, and bower, and we have downloaded all the build-time and runtime dependencies that we require. I think now we’re ready to build the application. Building the application: In Docker terms, this may feel like an anticlimax. Because I’m using grunt, the existing build-all task that I created back in 2015 will still do the job. # Copy all the remaining source codeCOPY src/ /app/srcCOPY Gruntfile. js . RUN grunt build-allThis is where all the work is done. You can skip this if you want, but I’ve shared so you can see the original build task. You might find this snippet useful to take some concepts into your own project. In summary, what we are doing is:    cleaning our build directory     concatenating all javascript source code into a single file.     obfuscating the javascript code     using sass parser to create our css stylesheets     minifying the css stylesheets     copying all our javascript and css, third party css, and fonts over to our final distribution directory     copy all server-side source code into the directory ready for Composer     update version number placeholder     run Composer to create an optimized production-ready runtime  It wasn’t all plain sailing when I upgraded my libraries to their latest version. Some changes were easy, sure, but a few required code updates — after all, I had to accommodate 5 years of deprecated features and changes. But all in all, it wasn’t as painful as I expected. module. exports = function (grunt) {  // Project configuration.   grunt. initConfig({    pkg: grunt. file. readJSON('package. json'),    clean: {      all: {        src: [ dist ,  tmp ,  . sass-cache ],        options: {          force: true        }      },      web: {        src: [ dist/web ,  dist/templates ,  dist/compilation_cache ,  dist/doctrine_cache ,  tmp ,  . sass-cache ],        options: {          force: true        }      }    },    jshint: {      all: ['src/js/*. js']    },    concat: {      all: {        src: ['src/js/*. js'],        dest: 'tmp/js/&lt;%= pkg. name %&gt;. js'      }    },    sass: {      dist: {        files: {          'tmp/css/&lt;%= pkg. name %&gt;. css': 'src/scss/main. scss'        }      }    },    cssmin: {      target: {        options: {          banner: '/*! &lt;%= pkg. name %&gt; &lt;%= grunt. template. today( yyyy-mm-dd ) %&gt; */'        },        files: [{          expand: true,          cwd: 'tmp/css/',          src: ['*. css', '!*. min. css'],          dest: 'dist/web/css/',          ext: '. min. css'        }]      }    },    uglify: {      options: {        banner: '/*! &lt;%= pkg. name %&gt; &lt;%= grunt. template. today( yyyy-mm-dd ) %&gt; */\n'      },      build: {        src: 'tmp/js/&lt;%= pkg. name %&gt;. js',        dest: 'dist/web/js/&lt;%= pkg. name %&gt;. min. js'      }    },    bowercopy: {      javascript: {        options: {          destPrefix: 'dist/web/js'        },        files: {          'jquery. min. js': 'jquery/dist/jquery. min. js',          'bootstrap. min. js': 'bootstrap/dist/js/bootstrap. min. js',          'html5shiv. min. js': 'html5shiv/dist/html5shiv. min. js',          'respond. min. js': 'respond/dest/respond. min. js',          'underscore. min. js': 'underscore/underscore-min. js',          'jquery. dataTables. min. js': 'datatables. net/js/jquery. dataTables. min. js',          'dataTables. bootstrap. min. js': 'datatables. net-bs/js/dataTables. bootstrap. min. js'        }      },      css: {        options: {          destPrefix: 'dist/web/css'        },        files: {          'bootstrap. min. css': 'bootstrap/dist/css/bootstrap. min. css',          'bootstrap-theme. min. css': 'bootstrap/dist/css/bootstrap-theme. min. css',          'font-awesome. min. css': 'components-font-awesome/css/font-awesome. min. css',          'dataTables. bootstrap. min. css': 'datatables. net-bs/css/dataTables. bootstrap. min. css'        }      },      bootstrap_fonts: {        files: {          'dist/web/fonts': 'bootstrap/dist/fonts/*. *'        }      },      font_awesome_fonts: {        files: {          'dist/web/fonts': 'components-font-awesome/fonts/*. *'        }      }    },    copy: {      main: {        files: [          {expand: true, cwd: 'src/php/web/', src: ['**'], dest: 'dist/web/'},          {expand: true, cwd: 'src/php/lib/', src: ['**'], dest: 'dist/lib/'},          {expand: true, cwd: 'src/static/', src: ['**'], dest: 'dist/web/'},          {expand: true, cwd: 'src/ico/', src: ['**'], dest: 'dist/web/ico/'},          {expand: true, cwd: 'src/img/', src: ['**'], dest: 'dist/web/img/'},          {expand: true, cwd: 'src/css/', src: ['**'], dest: 'dist/web/css/'},          {expand: true, cwd: 'src/config/', src: ['**'], dest: 'dist/config/'},          {expand: true, cwd: 'src/templates/', src: ['**'], dest: 'dist/templates/'},          {src: ['composer. json'], dest: 'dist/'}        ]      }    },    composer: {      dist: {        options: {          cwd: 'dist'        }      }    },    'string-replace': {      version: {        files: {          'dist/config/settings. ini': 'dist/config/settings. ini'        },        options: {          replacements: [{            pattern: '%APPLICATION_VERSION%',            replacement: '&lt;%= pkg. version %&gt;-&lt;%= grunt. template. today( yyyymmdd ) %&gt;'          }]        }      }    }  });  grunt. loadNpmTasks('grunt-contrib-clean');  grunt. loadNpmTasks('grunt-contrib-concat');  grunt. loadNpmTasks('grunt-contrib-sass');  grunt. loadNpmTasks('grunt-contrib-jshint');  grunt. loadNpmTasks('grunt-contrib-uglify');  grunt. loadNpmTasks('grunt-contrib-cssmin');  grunt. loadNpmTasks('grunt-composer');  grunt. loadNpmTasks('grunt-bowercopy');  grunt. loadNpmTasks('grunt-contrib-copy');  grunt. loadNpmTasks('grunt-string-replace');  grunt. registerTask('build-all', ['clean:all', 'concat', 'uglify', 'sass', 'cssmin', 'bowercopy', 'copy', 'string-replace:version', 'composer:dist:install:optimize-autoloader', 'composer:dist:update:optimize-autoloader']);  grunt. registerTask('build-web', ['clean:web', 'concat', 'uglify', 'sass', 'cssmin', 'bowercopy', 'copy', 'string-replace:version']);  grunt. registerTask('run-composer', ['composer:dist:install:optimize-autoloader', 'composer:dist:update:optimize-autoloader']);};At this stage we have a buildenv containing a directory /app/dist/ with all our final build output ready to be run. Building the runtime image: So far everything we have done is in our ‘buildenv’. But we don’t want all this bloatware in our final image. So we start again with a slim operating system, and I chose Alpine. On top of that, we need PHP but this time we only need a small subset of the PHP packages to run this application (your mileage will vary). And for this particular runtime, we still have Nginx. As you will see later, I am also using supervisor to manage the processes to ensure php-fpm and nginx are kept running. # Create final imageFROM alpine:3. 11. 0 # Install packages RUN apk upgrade &amp;&amp; apk --no-cache add php7 php7-fpm \  php7-json php7-openssl \  nginx supervisor curlConfiguration: Our three processes each need some configuration, so we copy those into place in the image. These trivial configuration files have been shared in the example repository on GitHub. In summary, the configuration is simply to allow Nginx to listen on port 8080 and redirect requests to php-fpm to handle. # Configure nginx COPY config/nginx. conf /etc/nginx/nginx. conf # Configure php-fpm COPY config/fpm-pool. conf /etc/php7/php-fpm. d/www. conf COPY config/php. ini /etc/php7/conf. d/zzz_custom. ini  # Configure supervisord COPY config/supervisord. conf /etc/supervisor/conf. d/supervisord. confUse nobody user to own directories: We’re using the ‘nobody’ user, so we need to ensure directory permissions are aligned. RUN chown -R nobody. nobody /run &amp;&amp; \  chown -R nobody. nobody /var/lib/nginx &amp;&amp; \  chown -R nobody. nobody /var/log/nginxWe now have our runtime image, with PHP and Nginx installed and configured. But we still need our application. Copy the final distribution from the build environment: Let’s create the directory /var/www/html and switch to the nobody user, then copy all the distribution contents over from the ‘buildenv’. # Setup document root RUN mkdir -p /var/www/html  # Switch to use a non-root user from here on USER nobody # Add application WORKDIR /var/www/html COPY --chown=nobody --from=buildenv /app/dist/ /var/www/html/Expose the port: Nearly at the end now — we need to expose the port that Nginx will be reachable on. # Expose the port nginx is reachable on EXPOSE 8080Ensure services are running: The final stage now is to use supervisord to ensure that Nginx and php-fpm are running and able to serve requests. # Let supervisord start nginx &amp; php-fpm CMD [ /usr/bin/supervisord ,  -c ,  /etc/supervisor/conf. d/supervisord. conf ]Conclusion: And that’s it — we can build the image and run it locally. Our runtime has a low footprint because we used a slim base image and only installed the absolute bare-minimum packages we required to run our application. Your situation will obviously be different, the libraries you use may be more complex — but I hope the overview gives a sense of what can be accomplished. For build automation, I then combined **GitHub **with **DockerHub **so that future commits automatically kick off a build and the creation of a new versioned image. And I deployed that into Google Cloud Run, where the exact same image I ran on my laptop I could also happily run in Production. I killed off my Amazon EC2 “classic” micro instance after almost 6 years, and reduced the cost of running to pennies a year. And by choosing Docker, I had a maintainable platform to keep on top of. The final Dockerfile: I ended up with a single Dockerfile, applying the Docker multi-stage build technique, and ultimately created a cut-down, slim runtime image. I could have consolidated a few layers, and break out, but once the initial set up was done, I could spin out new images in under a minute so I left it all as one. FROM composer:1. 9. 1 AS composerFROM php:7. 4. 1-alpine AS buildenvCOPY --from=composer /usr/bin/composer /usr/bin/composerRUN apk add --update nodejs npm &amp;&amp; \  npm update -g npm &amp;&amp; \  npm install -g grunt-cli &amp;&amp; \  npm install -g bowerRUN apk add --update ruby ruby-bundler ruby-dev ruby-rdoc build-base gcc &amp;&amp; \  gem install sassRUN mkdir /app WORKDIR /app# Copy npm dependenciesCOPY package. json . RUN npm install # Copy composer and download dependenciesCOPY composer. json . RUN composer install# Copy bower and download dependenciesRUN apk add --update gitRUN echo '{  allow_root : true }' &gt; /root/. bowerrcCOPY bower. json . RUN bower update # Copy all the remaining source codeCOPY src/ /app/srcCOPY Gruntfile. js . RUN grunt build-all# Create final imageFROM alpine:3. 11. 0# Install packages RUN apk upgrade &amp;&amp; apk --no-cache add php7 php7-fpm php7-json php7-openssl \  nginx supervisor curl# Configure nginx COPY config/nginx. conf /etc/nginx/nginx. conf# Configure PHP-FPM COPY config/fpm-pool. conf /etc/php7/php-fpm. d/www. conf COPY config/php. ini /etc/php7/conf. d/zzz_custom. ini # Configure supervisord COPY config/supervisord. conf /etc/supervisor/conf. d/supervisord. conf# Make sure files/folders needed by the processes are accessable when they run under the nobody user RUN chown -R nobody. nobody /run &amp;&amp; \    chown -R nobody. nobody /var/lib/nginx &amp;&amp; \  chown -R nobody. nobody /var/log/nginx# Setup document root RUN mkdir -p /var/www/html # Create cache directoriesRUN mkdir /var/www/html/compilation_cache &amp;&amp; chown nobody. nobody /var/www/html/compilation_cache &amp;&amp; \  mkdir /var/www/html/doctrine_cache &amp;&amp; chown nobody. nobody /var/www/html/doctrine_cache# Switch to use a non-root user from here on USER nobody# Add application WORKDIR /var/www/html COPY --chown=nobody --from=buildenv /app/dist/ /var/www/html/# Expose the port nginx is reachable on EXPOSE 8080 # Let supervisord start nginx &amp; php-fpm CMD [ /usr/bin/supervisord ,  -c ,  /etc/supervisor/conf. d/supervisord. conf ] # Configure a healthcheck to validate that everything is up &amp; running HEALTHCHECK --timeout=10s CMD curl --silent --fail http://127. 0. 0. 1:8080/fpm-pingA note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. Source code is available from this example repository on GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 21,
    "url": "https://goatsintrees.net/schedule-aws-lambda-to-download-a-file-from-the-internet-and-save-it-to-s3/",
    "title": "Schedule cron in Lambda to download a file and save it to S3",
    "body": "2020/04/06 - This post will explain how to use AWS Lambda to download a file each day and save the file into an S3 bucket. Why did I pick Lambda? The task runs for ~1 second every day and I don’t really need a virtual machine for that — or any infrastructure in fact— I sought the cheapest and simplest solution that I could natively trigger my code on a schedule (cron). AWS Lambda ticked all the right boxes for me, and the cost of the solution is less than a $1 a year. Here’s the work brief that we’ll go through in this short tutorial.    The process should run once every hour of the day.     The process should download a file from an internet location (https) and save it into an S3 bucket.     The object metadata should include the content type from the origin.     It should be designed/written in such a way as we can run the same code for different source files.  I’m going to demonstrate how to do this directly via the Console, and I’ll follow up with setting up a development environment and using the AWS Serverless Application Model and command line to achieve the same. All the source code for this article can be downloaded from GitHub. Approach 1 — AWS Console: The steps we’re going to follow are;    Create an S3 bucket to hold our file     Create a Lambda function     Update IAM to allow our Lambda function to write to S3     Write our code and set some dynamic properties (source file, target bucket, and the target filename).     Create a Test and verify everything is working.     Configure a Schedule so the Lambda function will run every day.  First step is to create our bucket in AWS S3 — I selected all the default options, and I’ll be using a bucket called “our-lambda-demo”.  Next step is to head over to AWS Lambda and “Create function” where we are going to select to “Author from scratch”.  I’ve named the function “downloadFileToS3” and left all the defaults in place.  Once your function has been created, go to the “Permissions” tab and follow the link to our Execution Role in the AWS IAM Management Console.  Once in IAM, you will see that the default setup only has basic Lambda execution permissions. This includes the ability to write out to CloudWatch logs, but little else. So we need to give write access to S3.  I’m going the “Add inline policy” route, but you could also go through “Attach policies” and add an existing managed policy such as “AmazonS3FullAccess”. You can use the dialogue to create a policy, or download the JSON if you want to just copy mine (remember to change the bucket name). {   Version :  2012-10-17 ,   Statement : [    {       Sid :  VisualEditor0 ,       Effect :  Allow ,       Action :  s3:PutObject ,       Resource :  arn:aws:s3:::our-lambda-demo/*     }  ]}Give your Policy a name and save. Back in Lambda, navigate to the code on the Configuration tab and we’re going to upload some code and its dependencies. Lambda provides us with access to the ‘aws-sdk’ automatically but anything else you will need to upload — the AWS SAM instructions show a more complete solution that allows you how to develop the code further, package everything together automatically, and deploy. For now, select “Upload a . zip file” and upload the provided index. zip file (shared on GitHub).  Once uploaded, you should see the index. js file with the function code and its dependencies under node_modules.  You’ll see that this piece of code requires three environment variables to be set which you can add from the Configuration tab also.    SOURCE_URI — the full path to the internet source we are downloading     S3_BUCKET — the bucket we are writing to     S3_KEY — the name of the file we are going to write into S3  Scroll down to “Manage environment variables” and add the variables. Once done, you should have something that looks like this.  We should now be ready to test out our function. You can find “Test” at the upper right of the screen, use that to create a dummy test execution. Leave all the defaults in place and call it “MyTestEvent” and hit Create.  Ensure your new test event is selected in the drop down and click “Test” again to run your test.  All going well, and you should see output like this showing the completion of your test along with metadata about the job execution and a link to the CloudWatch logs.  Heading back to AWS S3 and we can see that our file was saved.  And opening the file displays its contents.  Awesome! Next and final stage is to configure a schedule so that our job will continue to run and update the file in our S3 bucket. Back in the Configuration tab, select “+ Add trigger”.  From here we’re going to choose “CloudWatch Events/EventBridge” and create a new rule.  Give your rule a name and create the schedule using Cron or rate expressions.  Save and you should find your Trigger is now generated.  I updated mine to every minute to demo the success of running on a schedule.  So that’s it. Congratulations. We’ve written a Lambda function that runs on a schedule, will download a file and save that file to an S3 bucket. Approach 2 — AWS Serverless Application Model: This section will repeat the same process — from scratch — using AWS Serverless Application Model, which is an “open-source framework that you can use to build serverless applications on AWS”. AWS SAM can be used for local running and testing of our NodeJS Lambda function, and helps us to build and deploy the application. It’s a really cool tool, and behind the scenes it creates a declarative CloudFormation template that defines the stack and all the associated services required. Setting up your development environment:    Install AWS SAM     Install Node. js (because our example was written in Node. js).  Although we don’t need the AWS CLI installed to use AWS SAM, I have version 2 installed. If you don’t have the CLI installed, you’ll need to create a credentials file or set your AWS credentials as environment variables. I’ve just reinstalled everything today, so here’s what I have. $ aws --versionaws-cli/2. 0. 6 Python/3. 7. 5 Windows/10 botocore/2. 0. 0dev10$ sam --versionSAM CLI, version 0. 47. 0$ node -vv12. 16. 1$ npm -v6. 13. 4Just a reminder that all the source code for this article can be downloaded from GitHub. I’m going to assume no prior knowledge of Node. js development, but feel free to skip through the project initialization stage. We start off in an empty directory and we initialize using npm. $ npm initGive your application a name and you can leave all the other defaults as-is. On completion, you will get a default “package. json” file. We need to add a few dependencies for our function to work (‘request-promise’ and ‘request’), so follow these two commands to get going. $ npm install request-promise --save$ npm install request --saveYou’ll notice that your “package. json” has been updated and you should have a file structure like this.  With our project initialized and dependencies in place, we are now going to create the Lambda function code, so create a file called “index. js” and copy the contents from here: index. js on GitHub Function source code: The code is trivial to meet our objectives. const request = require('request-promise')const aws = require('aws-sdk');const s3 = new aws. S3();exports. handler = async (event, context, callback) =&gt; { const options = {  uri: process. env. SOURCE_URI,  encoding: null,  resolveWithFullResponse: true }; const response = await request(options) const s3Response = await s3. upload({  Bucket: process. env. S3_BUCKET,  Key: process. env. S3_KEY,  ContentType: response. headers['content-type'],  Body: response. body }). promise() return callback(null, s3Response);};We take in three parameters (as environment variables) and use these to download a file and save to S3. If you skipped the Console demonstration, a reminder of those three parameters that you can see in the code;    SOURCE_URI — the full path to the internet source we are downloading     S3_BUCKET — the bucket we are writing to     S3_KEY — the name of the file we are going to write into S3  So far we’ve got our code and dependencies in place. Now to begin looking at AWS Serverless Application Model or SAM. All AWS SAM operations require a template file (“template. yaml” by default) that defines all the resources we require. This is an extension to CloudFormation so you’ll recognise there are plenty of overlaps. To get started we need create our SAM template in our root directory. Call the file template. yaml. AWSTemplateFormatVersion : '2010-09-09'Transform: AWS::Serverless-2016-10-31Resources: downloadFileToS3:  Type: AWS::Serverless::Function  Properties:   Handler: index. handler   Runtime: nodejs12. x   Policies: AmazonS3FullAccess   Timeout: 10   Environment:    Variables:     SOURCE_URI: https://raw. githubusercontent. com/davelms/medium-articles/master/lamda-download-example/test. txt     S3_BUCKET: our-lambda-demo     S3_KEY: our-example-file   MemorySize: 128This initial template file was very basic and is just to get started. You’ll see we define our handler and runtime, the policy we’re using (the managed AmazonS3FullAccess), and the environment variables. Your project folder should now look like this.  Create target bucket in S3: I’m using the command line throughout for this part of the tutorial, but you can use the Console of course. Remember earlier that I installed the AWS CLI. $ aws s3 mb s3://our-lambda-demomake_bucket: our-lambda-demo$ aws s3 ls2020-04-10 16:15:38 our-lambda-demoOptional Stage — local testing with AWS SAM — requires Docker: Local testing requires Docker to be installed on your workstation. AWS SAM provides a local Docker execution environment that allows for the testing of your Lambda function code without needing to upload to AWS — this is invoked using the sam local invoke command. sam local invoke --no-eventAll being well, you should see output like this showing the Lambda function has been successfully executed on your local workstation and the file was created successfully. The code shared previously returns the output from the s3. upload() command, and we can see that in the JSON in the screenshot.  Let’s view the contents of the bucket to confirm our file was saved. $ aws s3 ls s3://our-lambda-demo2020-04-10 16:17:03     85 our-example-fileBuild, package, and deploy to AWS: Now that we have successfully created our function, and optionally tested it, we are ready to deploy it to AWS. AWS SAM provides a couple of guided stages here:  sam build followed by sam deployFirst of all, lets use AWS SAM to build our application. $ sam buildBuilding resource 'downloadFileToS3'Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:NpmInstallRunning NodejsNpmBuilder:CleanUpNpmrcBuild SucceededBuilt Artifacts : . aws-sam\buildBuilt Template  : . aws-sam\build\template. yamlCommands you can use next=========================[*] Invoke Function: sam local invoke[*] Deploy: sam deploy --guidedYou’ll notice a new directory has been created called . aws-sam/ Now we’re going to deploy to AWS and use all the default options. $ sam deploy --guidedYou’ll see a lot of output as resources are generated (too much to paste here). Also notice how it’s backed off to CloudFormation. We have a stack, function, and role created for us automatically. And if you head over to S3, you’ll find a new bucket was created automatically to contain your versioned, packaged code.  So lets go to the Console to see what was created. And we can run a simple test there too. Starting off in CloudFormation we can see the sam-app stack.  And across in AWS Lambda we can see the function was created successfully. Let’s create a Test and run it to verify… all good… Adding a schedule trigger: So far, we are invoking our new function manually, let’s update the template to include our Schedule under Events. We’re going to add this code.  Events:  downloadFileToS3ScheduledEvent:   Type: Schedule   Properties:    Schedule: rate(1 minute)Your code should look like this with the additional schedule included.  Next we follow the same process as before, click through all the defaults. $ sam build $ sam deploy --guidedYou’ll see that CloudFormation will only update for the changes it recognises need to be applied. Heading back to the Console and we can see that the “CloudWatch Events/EventBridge” Trigger has been created for us.  And a quick check of the job run history, and we can see that it’s run a few times so that’s all worked as expected.  Conclusion: In this article, I demonstrated;    a Lambda function that will download a file from the internet and save it to an S3 bucket, and we passed in parameters so we can re-use the code.     how to schedule Lambda functions to run on a standard cron schedule.     how to achieve all the requirements within the AWS Management Console and by using the AWS Serverless Application Module.  A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. A reminder that all the source code for this article can be downloaded from GitHub. If you want to learn more about AWS SAM, check out the AWS Serverless Application Module documentation. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 22,
    "url": "https://goatsintrees.net/7-youtube-channels-every-cloud-engineer-should-subscribe-to/",
    "title": "7 YouTube channels every Cloud Engineer should subscribe to",
    "body": "2020/03/29 - Let’s be honest at the start; these are the YouTube channels that I subscribe to for training, resources, and to help me keep abreast of all the latest AWS, GCP, and Cloud service news. So jumping straight in and in no particular order… #1 — Google Cloud Platform: First up is the Google Cloud Platform. There’s quite a few playlists in here and videos are uploaded almost daily. Selecting three of my favourites playlists;    “Get Cooking In Cloud” — love these short 5-minute videos, a whole series of “How to…” scenarios and topics, and there are regular sub-series such as Pub/Sub. These videos take the form of practical scenario based problems, and talk through the solutions within the Google Cloud Platform; they teach us the recipe in keeping with the cooking analogy.     “This Week in Cloud” — keeping abreast of the latest updates across the various Cloud platforms is never easy. They move so fast. That’s why I find weekly 2-minute catch-ups like “This Week In Cloud” very useful. Here we get to hear about all the features releases, enhancements, and changes within the Google Cloud Platform.     “Stack Doctor” — not yet a playlist, but regular themed content around Service Level Indicators (SLIs), Service Level Objectives (SLOs), and the Cloud Monitoring suite (formerly Stackdriver). These resonate for me as Site Reliability Engineering Lead in the UK in our organisation.  Finally, “Cloud Minute” is a series of 73 videos demonstrating — all in under one minute each — how to accomplish tasks on the Google Cloud Platform. An awesome reference to bookmark, although no longer being added to. #2 — A Cloud Guru: For those that don’t know it, A Cloud Guru is an awesome cloud training platform and the A Cloud Guru YouTube channel contains some of their free-to-all weekly updates. I’m focussed myself right now on AWS and GCP, but there’s plenty of Azure content as well.    “AWS This Week” — as the name suggests, this is a weekly breakdown of all the latest updates to the AWS platform. It is complemented by “GCP This Month”, a similar playlist but for the Google Cloud Platform — although “This Week In Cloud” on the Google Cloud Platform channel has likely beaten A Cloud Guru to it for many new announcements.     “Kubernetes This Month” — a monthly roundup by Nigel Poulton, author of Docker Deep Dive and The Kubernetes Book, with a catch up of all the recent announcements in all things Kubernetes, Docker, and related domain. The videos always include a deep dive into one or two announcements per month to focus on the important new releases.  #3 — Amazon Web Services: Now a jump across to AWS and the “Amazon Web Services” channel. Every few days I tend to sift through all the new videos and select those that appeal — it’s a very busy channel — looking back at my viewing history, those I select often fall into one of these two playlists.  “AWS Demos” and “AWS Knowledge Center Videos” — I can’t quite work out the difference between these two playlists as the content always seems to overlap in context. Similar to Google’s “Cloud Minute” and “Get Cooking In Cloud”, these playlists feature videos that are short and cover solving specific tasks on Amazon Web Services platform. There is a broad mix of troubleshooting guides for common issues and solutions that focus towards the SysOps Administrator audience, and some that are aimed to Developers and Architects. At times, it’s well worth just working through all the AWS videos to find something that is interesting — some of the discussions with companies around their solutions (“This is My Architecture”) yields some interesting content. #4 — Google Developers: Over 2 million people subscribe to the “Google Developers” channel. Wow. Not specifically “Cloud”, but plenty of overlaps and tool chains — we develop, host, and run applications at the end of the day — so worth adding a developer channel to the list. There are videos and playlists here covering a wide variety of topics, including Android and Flutter development.  “The Google Developer Show” — this is the main playlist I like to check out each week on the Google Developers channel— as the name suggests, it’s a weekly update on the latest developer news from across Google. #5 — Hashicorp: As Cloud Engineers, we’re probably all familiar of Terraform that many of us will have used as our Infrastructure as Code provisioning tool. Likewise, we may also have used Vault for Secrets Management. The “Hashicorp” channel is updated regularly and has a lot of Cloud-related content and some great videos describing the fundamentals of building out secure infrastructures. There are plenty of in-depth “how to” guides and patterns, spanning virtual machines and Containers, and some of my favourites include explaining how to do secure introduction when building out the Cloud platform. #6 — KodeKloud: The “KodeKloud” channel is updated monthly and covers a variety of hands-on training series. I subscribe mostly for the “Docker” and “Kubernetes” series, which have videos pitched toward Absolute Beginners and work themselves through to the more expert topics. They are well-presented and I have many of them saved for reference. There are other topics such as Ansible, Puppet, and OpenShift. #7 — Serverless: I first came across “Serverless” thinking it was a channel dedicated to the serverless topic . In fact they are the creators of the Serverless Framework which “gives you everything you need to develop, deploy, monitor and secure serverless applications on any cloud. ” Despite not being what I was looking for, I stuck around; not because I use the Serverless Framework, but because their video content is great. I reference this channel for general guidance, serverless patterns, and developer how to references in topics such as auth0 and AWS services such as Lambda or DynamoDB (among many others). It’s easy to abstract the core topic away from the Serverless Framework itself. So that’s my current top 7 — but what else?: With “Linux Academy” recent sync-up with A Cloud Guru, it’s not clear what is happening to their YouTube channel. It’s not had any new videos since the announcement in 2019. All the same — some really useful archives of content here that are useful to any Cloud Engineer to keep as a reference. “AWS Online Tech Talks” are much longer episodes and discuss various topics in more details. These need more time commitment as they can often be over an hour in length. The “Docker” channel is also worth keeping in your subscription lists. A note from the author: Thank you for reading this article — I hope you found it useful. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 23,
    "url": "https://goatsintrees.net/3-aws-platform-questions-and-answers/",
    "title": "3 AWS Solutions Architect certification questions",
    "body": "2020/03/28 - In my role as the DevOps Practice Lead at my workplace in the UK, I’ve been trying to think of challenges for the Cloud engineering team that are a little different from those we see in our normal day-to-day work situation. An aim to keep sharp with our skills and technical thinking, and keeping abreast of the ever changing Cloud services platforms. The team already did a great job at pulling together a bank of Google Cloud Platform questions to help them with the GCP Associate Cloud Engineer exam. And that got me thinking about the same for Amazon Web Services. Couple that with being on lock-down — and so we have this article; I’m going to find sample questions and attempt to answer them. Every week, I’ll post an article with another set of questions. And I’ll use these back at work to discuss with the team. I’ve chosen a sample of 3 x recent real-world requests for help and sample questions posted on the Facebook groups “AWS Cloud” and “Amazon Web Services (AWS)”. Disclaimer; I am not advocating that my answers are the correct ones — indeed, I’ve chosen two examples where the online responses were mixed. I will explain my thinking and how I came to the answer I did. I welcome all feedback in the comments.   Scenario 1. An application uses an Amazon RDS MySQL cluster for the database layer. Database growth requires periodic resizing of the instance. Currently, administrators check the available disk space manually once a week. How can this process be improved?A. Use the largest instance type for the database. B. Use AWS CloudTrail to monitor storage capacity. C. Use Amazon CloudWatch to monitor storage capacity. D. Use Auto Scaling to increase storage size. — posted to “AWS Cloud” on 27th March. This question looks like the style of those found in Associate certification exams. A technique that’s useful when taking Amazon AWS Certifications is to first eliminate any obvious wrong answers. This question is no exception and we immediately eliminate the reference in (B) to AWS CloudTrail. Often confused with CloudWatch, however AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. As such, it does not monitor storage capacity and is irrelevant to this question. The next option to eliminate is (A) for using the largest instance type for the database.  A. Use the largest instance type for the database. Why? The question has two statements; firstly, around database growth requiring periodic resizing of the instance. And secondly, around checking for available disk space. Three answers focus in on the disk space. It’s true that with AWS RDS you manually scale up the underlying instance — and that is already happening periodically per the problem statement. How can this be further improved? Well, to some extent picking the largest instance size as (A) suggests would be a hammer to crack that particular nut; a rather expensive hammer though. But it would not help at all with the disk space challenge — maximum storage for MySQL is constant at 64 TiB for each of the** **Latest Generation Standard (m5) or Memory Optimized (r5) Instance Classes — scaling to the largest type in its Class will not offer us an increase in disk storage. Additionally, if the problem statement was around read performance, the introduction of “read replicas” or Amazon ElastiCache would help in many solutions before opting for vertical scaling. So we eliminate (A). As it is, our focus in this question is on the statement around disk usage.  Currently, administrators check the available disk space manually once a week. We have two options remaining, (C) and (D), both of which are improvements on manual checking. But one is better than the other, as I will explain next.  C. Use Amazon CloudWatch to monitor storage capacity.  D. Use Auto Scaling to increase storage size. Let’s think through the process those administrators are taking today — first, they manually check disk usage, and secondly (we have to assume that) if disk usage is above threshold they are increasing the disk allocation manually. Option (C) states to use CloudWatch to monitor storage capacity. There are a couple of techniques for this, including;    Monitor the FreeStorageSpace metric by creating a CloudWatch Alarm, with an SNS topic, and a Subscription to alert the team automatically.     Checking for predefined RDS Events for low storage, again in combination with an SNS topic and Subscription.  How is (C) an improvement? Well, it means the administrators are no longer having a weekly task in their diaries to sign on to manually check the storage usage — now, they will get notified automatically — that’s better detection — but, with (C) it still requires them to take manual action to fix the situation. That leaves us with option (D) which is to use RDS Auto Scaling option to automatically increase the storage allocation. Released in June 2019, RDS Storage Auto Scaling automatically scales storage capacity in response to growing database workloads, with zero downtime. With storage autoscaling enabled, Amazon RDS will trigger a scaling event when these factors apply:    Free available space is less than 10% of the allocated storage.     The low-storage condition lasts at least five minutes.     At least six hours have passed since the last storage modification.  When the event triggers, Amazon RDS will add additional storage in increments of whichever of the following is greater:    5 GiB     10% of currently allocated storage     Storage growth prediction based on the FreeStorageSpace metric change in the past hour.  In this question, our RDS database engine is MySQL — so the auto-scaling will keep going up to a maximum allocation of 64 TiB for all instance types in the latest generation class (m5 or r5). So that’s our answer; we can detect and fix the problem automatically.  D. Use Auto Scaling to increase storage size. So how does my answer (D) compare with those on the “AWS Cloud” group — the group was roughly equal 50–50 split between options (C) and (D). Leave a comment if you would have chosen (C) in this example.   Scenario 2. A company has a popular multi-player mobile game hosted in its on-premises datacenter. The current infrastructure can no longer keep up with demand and the company is considering a move to the cloud. Which solution should a Solutions Architect recommend as the MOST scalable and cost-effective solution to meet these needs?A. Amazon EC2 and an Application Load BalancerB. Amazon S3 and Amazon CloudFrontC. Amazon EC2 and Amazon Elastic TranscoderD. AWS Lambda and Amazon API Gateway— posted to “AWS Cloud” on 26th March. This is both a great and terrible question to pick next! It lacks information about the current solution and what else it relies upon; what databases, user identification and access management, session handling, and so on. What we can do is state our assumptions — and progress from there. What is good and clear about this question is the emphasis on choosing the option that is “MOST scalable and cost-effective”. Let’s get it out of the way to start with — Amazon GameLift, the dedicated game server hosting platform, is not listed as an option. First of all, as before, let’s eliminate the obviously wrong answers. We shall eliminate options (B) and (C).  B. Amazon S3 and Amazon CloudFront  C. Amazon EC2 and Amazon Elastic Transcoder We eliminate option (C) as Amazon Elastic Transcoder is for media transcoding in the cloud — not particularly relevant to a multi-player mobile game. Option (B) is a more interesting idea. The original problem lacks information about where the scaling issues are occurring in the current on-premise solution — it would be true that a hybrid solution could be explored whereby all static content is hosted on S3 and served through a Content Delivery Network using Amazon CloudFront, and we leave all the dynamic application and database where it is on the existing servers on-premise. For static content, using S3 and a CDN is definitely highly scalable and cost effective. It’s also useful in Single Page Applications like ReactJS or Angular. And yes, it could also be part of a phased approach to shift static content first, and even in a fully Cloud-native replacement solution hosted entirely in AWS, we are likely to end up with some use of S3 and Amazon CloudFront. However, given the crux of this question, and the fact that it’s a “mobile game” (can we infer an iOS or Android native app?) let’s assume that the problem with scaling is with the backend dynamic logic and database and we’ll eliminate option (B). We’re now left with two viable options.  A. Amazon EC2 and an Application Load Balancer  D. AWS Lambda and Amazon API Gateway Remember our core requirement; we’re looking for the “MOST scalable and cost-effective”. Both of these options allow us to create scalable solutions — so we have some more thinking to pick (A) or (D). I’ve made an assumption before I start; the current solution uses an RDBMS database and we’ll migrate that over to Amazon RDS — and I’ve assumed that the reason this isn’t mentioned in the problem statement is that we’ll end up picking the same solution for the database regardless of option chosen. So all we have to consider is the application logic and its compute requirements. With EC2, we can create auto-scaling groups and use Spot instances to achieve the “cost-effective”. When considering option (A), we have to pay attention to the original problem statement — the demand has out stripped the infrastructure they can use in their on-premise data center. So that statement tells us that there’s a lot of compute currently in-use and demand is increasing. I’ve inferred, a lot. Therefore, migrating as-is to EC2 will also require a lot of compute resources — like for like — and while costs can be managed through a combination of Reserved and Spot instances, that much compute is still going to have a price tag with it. We add the cost of the Application Load Balancer on top. In terms of effort required to migrate the application, moving across to EC2 is likely to require fewer application code changes — it could even “lift and shift”. Next to consider is option (D), which would likely require us to rewrite our code as AWS Lambda functions (although naturally stateless, we can handle state and combine API Gateway with web sockets). AWS Lambda lets us run our code without provisioning or managing servers, and we only pay for the compute time we consume. It naturally scales up to meet the peak demands, and we don’t pay for idle instances. In terms of costs, as we scale out to the volumes implied in the problem statement, running AWS Lambda is likely to be cheaper than running the equivalent load through a fleet of EC2 instances. The downside is that we would likely need to re-develop our solution (but not doing so wasn’t stated as a constraint — so assume a greenfield). In conclusion, for the “MOST scalable and cost-effective” solution I’d pick (D) — AWS Lambda and API Gateway.  D. AWS Lambda and Amazon API Gateway So how does my answer (D) compare with those on the “AWS Cloud” group — the group was pretty much all going for (A), nearly everyone liked the EC2 option. So my answer definitely leaves me in the minority. Leave a comment if you would also have chosen (A) in this example and let me know why.   Scenario 3. When will you incur costs with an Elastic IP address (EIP)?A. When an EIP is allocated. B. When it is allocated and associated with a running instance. C. When it is allocated and associated with a stopped instance. D. Costs are incurred regardless of whether the EIP is associated with a running instance. A quick one to close off for this week. This question was in a bank of AWS sample questions, and resonated with me earlier this week because I found I was paying for an Elastic IP Address in my own AWS account. So I’m sharing this answer out of personal experience. An Elastic IP is free, but only as long as it is being used by a running instance. So of these options the answer is (C); we would pay for an Elastic IP address when it is allocated and associated with a stopped instance. A note from the author: So there we have it. Three example AWS questions and answers. Thanks for reading and I hope you enjoyed the article and found it useful. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 24,
    "url": "https://goatsintrees.net/using-a-custom-domain-in-cloudfront-with-an-ssl-certificate-and-route-53/",
    "title": "Configure a custom domain in AWS CloudFront",
    "body": "2020/02/09 - CloudFront is Amazon’s low-latency Content Delivery Network (CDN). Using a CDN speeds up the distribution of content to visitors by serving content from edge locations that are closest to the user. Delivering content from Amazon S3 using CloudFront edge locations At the time of writing, Amazon has 216 Points of Presence (205 Edge Locations and 11 Regional Edge Caches) in 84 cities across 42 countries. When you set up your CloudFront distribution, straight out of the box with the default settings you will have your own cloudfront. net domain.  (that is assuming you have already configured CloudFront in front of an S3 bucket that holds your static web content, but if not check out this guide on serving static content from S3 using CloudFront and come back) But what if you want to serve your content from my-custom-domain. com. To use a custom domain requires a combination of Route 53 — Amazon’s highly available and scalable cloud DNS web service — and some additional configuration CloudFront. It doesn’t take too long to set up. Before we start, I assume that you have your domain managed in Route 53; it doesn’t matter if you don’t, but this guide assumes you do. There is an initial step to obtain an SSL Certificate within Certificate Manager. This allows you to serve your content over https and is a service provided by Amazon for free, and they’ll also take care of its renewal. Within the Certificate Manager service, make sure you change your region to North Virginia; I cannot emphasize this one enough as it’s caught me out many a time. Then Request a Certificate. The form is pretty self explanatory and you’ll need to provide a means to prove you own the domain — if you’re using Route 53, and we assume you are, then selecting the option that Amazon automatically manages the validation is the simplest approach. The process usually takes a few minutes. Now head over to CloudFront and set up your custom domain. This can be done at the time of creating the distribution, but don’t worry if you forgot — you can go back and edit all these settings later. However, you do have to complete the setting in CloudFront before you finish off the setup in Route 53. The first setting is to list all your Alternative Domain Names in the CloudFront distribution settings. Add all your domain names to CloudFront distribution settings The second setting is to reference the SSL Certificate you created. Check the Custom SSL Certificate (example. com) option and pick your SSL Certificate from the list. Warning; your Alternate Domain Names must match those you specified in the SSL Certificate provisioning request — so if you don’t see your certificate in the list, that is probably the reason. With these settings done, the final step is to configure the DNS in Route 53. In your domain hosted zone in Route 53, select to Create Record Set. We will be creating as an A record for IPv4 and we’ll select the Alias option. In the Alias Target, you will find your CloudFront distribution — select and save. Warning; your Alternate Domain Names you configured in CloudFront must match the record set name — so if you don’t see your CloudFront distribution in the target drop down list, that is probably the reason.  Repeat to create an AAAA record for IPv6. And that’s it. Success. You will find that you are now able to view your website using my-custom-domain. com, with all the added benefits of CloudFront providing edge locations around the world to reduce latency for your visitors. You will also have an SSL Certificate that is managed by Amazon and will be automatically renewed for you (at the time of writing, it’s free).  Variations. If you don’t use Route 53, the final step will be to add a CNAME entry in your DNS settings and set the value to your CloudFront domain. A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your feedback. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 25,
    "url": "https://goatsintrees.net/serving-static-content-from-s3-using-cloudfront-and-origin-access-identity/",
    "title": "Serve private static content from S3 with CloudFront and Origin Access Identity",
    "body": "2020/02/08 - Using Amazon Simple Storage Service (Amazon S3) is a cheap and effective way to host static websites and other web content. You can do so directly from the S3 console by enabling the Static website hosting feature and you’ll get a website of the form http://my-bucket-name. s3-website-eu-west-1. amazonaws. com. You can also create an A-record ALIAS in Route 53 to use your own custom domain. Basic set up — hosting a static website in S3 Some solutions may stop here. For some, this is *good enough. *(but this is not the purpose of our article; it would be incredibly short if it was). More likely the solution will evolve toward serving content from edge cache locations using CloudFront — Amazon’s low-latency Content Delivery Network (CDN). Using a CDN both speeds up the distribution of content to visitors and will also reduce the overall cost for a busy site. Introducing CloudFront as our Content Delivery Network Even with the CDN our visitors can still access the S3 bucket directly, and the Solution Architect will now be asked “how do we restrict access to the S3 bucket so that our html, css, and images, are only accessible through CloudFront?” (this question is the purpose of this article).  The answer is to use Origin Access Identity (OAI). We can restrict public access to objects in the S3 bucket (as of today, this is the default setting) and we grant permission to the OAI to distribute the content through CloudFront to our visitors from around the world. The steps we follow to achieve this solution are;    Create the S3 bucket with default settings and upload an index. html file (index. html will not be accessible directly from S3).     Create a CloudFront distribution with the S3 bucket as its origin (index. html still cannot be accessed).     Set up the OAI, and configure a policy that permits CloudFront to serve the index. html file (now it all works).  Steps 2 and 3 would normally be applied at the same time, but I’ll demonstrate separately to show the individual steps and how the OAI is the bit of magic sugar in the solution. Step 1. Create the bucket in S3 that will hold the static content and use all the default settings. The bucket and its objects are not accessible to the public. S3 Bucket holding our static website content When we attempt to reach the index. html file in a browser, we get an Access Denied error as expected. Hitting index. html on S3 endpoint receives an Access Denied error Step 2. In CloudFront, create a Web distribution and select the S3 bucket as the origin. At this stage, leave everything else as the default settings, scroll to the bottom and create the distribution. You’ll have to wait until it’s deployed and as this can take 10 minutes, go grab a coffee… Our CloudFront distribution for our S3 origin Once its status has changed to “Deployed”, it’s ready. At this stage, the index. html page is not accessible on the CloudFront domain. Hitting index. html on CloudFront endpoint receives an Access Denied error Step 3. Create the Origin Access Identity and configure the policy in S3 that grants the OAI permission to access objects. Since we’re doing this in two stages, we have to edit our existing Origin to access the OAI option; however, usually you would do this at the same time as creating your Web distribution and it’s on the initial list of options. Select our origin and click Edit On the next page, select Restrict Bucket Access, allow Amazon to Create a New Identity, and choose Yes, Update Bucket Policy. Select the options and save Wait for the status to change to “Deployed” again, and then refresh the page and the index. html page will now be displayed. Our static content is now served correctly via CloudFront All set. Success. We are now using CloudFront edge locations to serve our static content uploaded to S3. And no one can hit the content in S3 directly. Check the direct S3 endpoint again just to be sure that remains blocked, and view the Bucket Policy in S3 that was added automatically to learn more. What we didn’t cover today. Most solutions are likely to require a custom domain that is configured in Route 53 and CloudFront; an SSL certificate from Amazon Certificate Manager so that content can be served over https; and have cache expiry limits set on the objects. In addition, if you have built a Single Page Application (SPA), like Angular or ReactJS, then you may need to configure CORS. Finally, we didn’t cover off creating signed URLs, which is useful if you are distributing paid-for content and want to limit access to your edge location caches. A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. You can follow me on Twitter and connect on LinkedIn. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});