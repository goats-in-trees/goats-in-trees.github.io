
var documents = [{
    "id": 0,
    "url": "https://goatsintrees.net/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://goatsintrees.net/about",
    "title": "Goats in Trees, articles about Technology",
    "body": "Goats in Trees offers articles about Cloud technologies, DevOps, AWS, GCP, and other topics that interest its author Dave Sugden. "
    }, {
    "id": 2,
    "url": "https://goatsintrees.net/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://goatsintrees.net/contact",
    "title": "Contact",
    "body": "  Please send your message to Goats in Trees. We will reply as soon as possible!   "
    }, {
    "id": 4,
    "url": "https://goatsintrees.net/",
    "title": "Home",
    "body": "                                                                                      Build your Docker images automatically when pushing new code to GitHub              :       In this tutorial, we will explore two techniques to automatically build and publish Docker images to Docker Hub every time you push new code into your GitHub source repository. :                                                                                       Dave                    10 May 2020                                                                                                                  Create a free website in 5 minutes using Google Sites              :       Quick start guide to using Google Sites for building a static site for your school, sports club, business, or personal profile, and hosting it on your own custom domain. :                                                                                       Dave                    08 May 2020                                                                                                                  Solve a Sudoku with JavaScript              :       A walk through of an algorithm to solve popular Sudoku number challenges, with examples written in JavaScript. :                                                                                       Dave                    02 May 2020                                                                                                                  Use Ansible to create and configure EC2 instances on AWS              :       Quick how to tutorial to using Ansible to stand up EC2 instances on AWS. Over the next 5 minutes, we’ll create an initial jumpbox in our default VPC and install. . . :                                                                                       Dave                    25 Apr 2020                                                                                                                  Convert an old PHP application to Docker containers              :       Take an old PHP 5 web application and convert it to Docker containers, using the latest PHP 7, Composer, Node. js, Grunt, and Bower. :                                                                                       Dave                    19 Apr 2020                                                                                                                  Schedule cron in Lambda to download a file and save it to S3              :       How to write an AWS Lambda (Node. js) function and schedule it daily to download a file from the internet and save it into an S3 bucket. :                                                                                       Dave                    06 Apr 2020                                   &laquo;        1        2       &raquo; "
    }, {
    "id": 5,
    "url": "https://goatsintrees.net/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ &#8220;sitemap. xml&#8221;   absolute_url }}   "
    }, {
    "id": 6,
    "url": "https://goatsintrees.net/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "https://goatsintrees.net/build-your-docker-images-automatically-when-pushing-new-code-to-github/",
    "title": "Build your Docker images automatically when pushing new code to GitHub",
    "body": "2020/05/10 - Overview: This article will outline two techniques for automating the build of Docker images after every push or merge to the master branch in GitHub. The built images will be pushed to a Docker Hub container repository. The two approaches being demonstrated are;    using Docker Hub – use GitHub webhooks to notify Docker Hub about code changes and trigger the build of a new Docker image within Docker Hub itself.     using GitHub Actions – using GitHub’s service for running Continuous Integration pipelines, we will build the new Docker image within GitHub machines and push the image to Docker Hub.  Prerequisites: To begin this tutorial, you will need to set yourself up with accounts on GitHub and Docker Hub.    GitHub account (register here)   Within your GitHub account, create a repository with a Dockerfile and any additional source code. I’m using a simple Hello World example for the purposes of this article.   “Hello World” Dockerfile     Docker Hub account (register here)  Approach #1 — using Docker Hub to build image: In the first approach, we will configure Docker Hub to receive notifications from GitHub whenever there are any changes to our source repository. This is achieved using GitHub webhooks. On receipt of the notification, Docker Hub will build the new Docker image and publish it for consumption.  Step 1. Associate your GitHub and Docker Hub accounts. : Within Docker Hub visit Account Settings &gt; Linked Accounts and click “Connect” to allow access to your source repositories.  Step 2. Create container repository in Docker Hub: Within Docker Hub create a new repository and under “Build Settings” click on the GitHub icon to associate your source code repository.  From the drop-down select your GitHub organisation (this will default to your username) and the source code repository. Step 3. Configure Build Rules: There are many options available, and several examples are displayed in the help. For the purpose of this demo, we’ll keep the default settings and set up the trigger to be on pushes to the master branch and use the latest docker tag.  Other options exist to create Docker images from tags or release branches, and the pattern matching allows you to create dynamic image tags. Example Build Rule options Click “Create &amp; Build” to set up your new container repository and build your first Docker image — the first build will trigger automatically. Step 5. Viewing builds. : Within Docker Hub you will find the build status in the “General” tab of your repository. Viewing the “Builds” tab will show more information about each build. From here you will see the status of jobs and view build logs.  Once built we can pull and run the newly built image. $ docker run davelms/hello-world:latestHello world!Step 6. Push new source code changes: Final step is to simulate a code change — I’ll do this by editing the “Hello world!” message — and push the new commit up to GitHub. GitHub will send a notification to Docker Hub about the code change; Docker Hub will initiate a new build; and we can run that new image once completed. GitHub commit log Immediately after pushing the code, we see a new build initiated in Docker Hub. You can see that the commit sha is referenced — tip: you can use this in tags should you wish instead of “latest”. Docker Hub build history When the build has finished, update locally with docker pull and re-run your container. This time we see the output from our new source code. $ docker run davelms/hello-world:latestHello Dave!Approach #2 — using GitHub Actions to build Docker images: This time we will use GitHub’s custom CI/CD platform — GitHub Actions — to build the Docker image after every push to the source code repository. The workflow will define a single job that builds the image and pushes the new image to Docker Hub.  Step 1. Create a Docker Hub security access token: First of all, within Docker Hub create yourself an access token by visiting Settings &gt; Security. Give it a name you recognise it later.  Once created, head over to GitHub and create secrets within your source code repository for DOCKER_HUB_USERNAME and DOCKER_HUB_TOKEN, along with DOCKER_HUB_REPOSITORY.  Step 2. Create a GitHub Action to build and push images: Keeping within GitHub, head into the “Actions” tab of your source code repository. It’s likely that it will have detected the Dockerfile and will recommend you Docker-related workflow examples to get you started. Since I will share an example, skip these helpers for now and select “set up a workflow yourself”. Use the workflow definition below and commit the file. name: Docker Image CIon: push:  branches: [ master ]jobs: build:  runs-on: ubuntu-latest  steps:  - uses: actions/checkout@v2  - name: Build the Docker image   run: |    echo  ${{ secrets. DOCKER_HUB_TOKEN }}  | docker login -u  ${{ secrets. DOCKER_HUB_USERNAME }}  --password-stdin docker. io    docker build . --file Dockerfile --tag docker. io/${{ secrets. DOCKER_HUB_USERNAME }}/${{ secrets. DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA    docker push docker. io/${{ secrets. DOCKER_HUB_USERNAME }}/${{ secrets. DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA   What does this CI pipeline do?    Defines that we will trigger a job after all pushes to master branch.     Configures a single job called “build” to run on an Ubuntu machine.     The source code repository will be checked out and will run three inline commands: 1. docker login 2. docker build 3. docker push. These will use our secrets we saved off earlier.     Note that the example uses the commit sha as the image tag — GitHub Actions makes the sha available in an environment variable called GITHUB_SHA. Of course, you could stick with “latest” like we did in the first example should you wish to do so.  Step 3. View the build: When the build has finished, the image will appear over on Docker Hub.  Now verify you can pull the new image down successfully. $ docker run davelms/hello-world:8517bf9c40f4cf198ea3313bd5ec3cc43176bd85 Unable to find image 'davelms/hello-world:8517bf9c40f4cf198ea3313bd5ec3cc43176bd85' locally8517bf9c40f4cf198ea3313bd5ec3cc43176bd85: Pulling from davelms/hello-worldd9cbbca60e5f: Already existsDigest: sha256:f8808c8b2ae19f6f3700e51a127e04d8366a1285bdfc6e4006092807f0eced1bStatus: Downloaded newer image for davelms/hello-world:8517bf9c40f4cf198ea3313bd5ec3cc43176bd85Hello Dave!Step 4. Push new source code changes: Final step is to simulate a code change — I’ll do this by editing the message back to “Hello world!” — and push the new commit up to GitHub. Commit log After the push, a new CI/CD workflow containing our “build” job is run. CI Pipeline history The new image will have been pushed to Docker Hub successfully.  This time we see the output from our new source code — the message is back to “Hello World!”. All is working as expected. $ docker run davelms/hello-world:a26c53183fac84a9c7ce128ce6ae6250fae26c7dUnable to find image 'davelms/hello-world:a26c53183fac84a9c7ce128ce6ae6250fae26c7d' locallya26c53183fac84a9c7ce128ce6ae6250fae26c7d: Pulling from davelms/hello-worldd9cbbca60e5f: Already existsDigest: sha256:3cfa80d4fa8c51271928f0294d89293a7a7fc7022a416a1758fc37394bc12808Status: Downloaded newer image for davelms/hello-world:a26c53183fac84a9c7ce128ce6ae6250fae26c7dHello World!Conclusion: In this article we walked through two techniques for automating the build of Docker images after every source code change in GitHub. In both examples, we pushed the images into Docker Hub. Firstly, we used webhooks and built the image directly within Docker Hub where the “free plan” offers the ability to create images (the limitation with this plan is that it limits to building one image at a time). Next, we used GitHub Actions to build a Continuous Integration pipeline and push the built image to Docker Hub — we could extend this pipeline later to include some unit and integration tests. GitHub Actions provides 2,000 minutes of machine time per month under its “free plan”. Thank you for reading this article — I hope you found it useful — you can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 8,
    "url": "https://goatsintrees.net/five-minutes-create-a-free-static-website-using-google-sites/",
    "title": "Create a free website in 5 minutes using Google Sites",
    "body": "2020/05/08 - Setting up a basic yet professional looking static website could not be easier with Google Sites. Let’s explore how in this quick introduction, where we will create a free website and host it off our own custom domain. Your site could be up and running inside 5 minutes. Although Google Sites is mainly used for intranet sites within an organisation, where it can embed the contents of Google Docs or Google Sheets, it’s also a perfectly acceptable proposition for building up an individual portfolio, sporting team website, or a shop like a restaurant.  You can embed third party content (technically, this gets implemented inside an iframe) and style this content to fit your chosen theme. You can use this for contact forms, for example. A site generated with Google Sites is mobile-first, and will render correctly on mobile, tablet, and laptop without any configuration from the user. You can also hook in Google Analytics for visitor tracking and analysis of your user behaviour and how they found your site. Finally, you can configure to run off your own custom domain. The one pre-requisite is that you have a Google Account. Let’s get started. Create your first Google Site: Visit Google Sites. I’d recommend that you stick with “New”, but you can drop back into “Classic” if you wish. Classic has more templates/themes but is more clunky to work with, whereas New is cleaner and fresher but the options are limited. Templates: Start off with a blank page or click on a template to create your first site. At the time of writing, the following templates are available to choose from in the categories of Personal, Work, and Education. Personal:  Portfolio Restaurant Work:  Event Team Help Centre Project Education:  Class Club Student portfolio Themes: There are 6 themes available to select and these apply across all the Templates. A theme will provide a basic style (font, headers, and colours) and you can pick pre-selected options or choose your own colour from a picker.  Simple Aristotle Diplomat Vision Level ImpressionThere’s a lot of trial-and-error involved to select the theme that matches your personal preferences — I recommend working through them all. Creating a new page: New pages can be created using the menu, and these can be inserted underneath a parent page to create a drop-down hierarchy. All pages will appear in the navigation unless you select to hide.  Talking of the Navigation, you can choose to position your page navigation at the top or side of your page.  Embedding content: You can embed images, documents, spreadsheets, and **slides **into your pages. These can be hosted on your Google Drive, or you can upload content directly from your local machine, or from a URL. Images can also be inserted from a Google search — good feature is that Google will automatically filter only those that are licensed for free use. You can include HTML in your page, including JavaScript and CSS stylesheets. This is useful when embedding third party content — such as a email contact form or a mailing list sign up sheet. Company Logo and favicon: Google Sites gives you full control to brand your site. Under the Settings, you can configure your company logo to appear in the Navigation bar and you can also configure the favicon that appears in the browser tab.  Once set your company logo will appear in the Navigator bar at the top of the page.  Publish: When you are ready to publish your site, hit the Publish button to the upper right. Select a name for your site and that’s it; your site will now be live to the world on the https://sites. google. com/view/your-site-name address. Should you edit your site, you’ll need to re-publish your changes. This allows you to work on your site and publish a new version once you are ready. Using your own domain: Running your site off a custom domain requires a little bit of technical know-how, but I shall walk through the steps. For those super technical, you can’t host off a domain apex — example. com — instead it has to have a subdomain, so www. example. com is acceptable. First of all, you need to prove to Google that you own the domain. If you already have your sites listed in Google Webmaster Central you will be familiar with the technique, and it’s the same here. Within the Settings panel, you can add up to 10 URLs that can be used for your Google Site. Enter the first in the box.  If Google knows that you own the domain it will automatically configure it. Otherwise, you will see a warning to ask you to verify your ownership.  Follow the instructions, and you’ll be asked to add a TXT record to your DNS. google-site-verification=Zakx97MOoIOGmDq_kXNOo133YvVqUNq0DJl9HsyWCP4Don’t worry if you never did this before, Google provides instructions for most major hosting providers. Once you’ve added this TXT record, it will take Google approximately 5 minutes to detect and confirm your ownership. While you’re on your hosting provider site, stay within the DNS settings for the next step too — you need to create a CNAME record too. This has to be subdomain (e. g. www. yourdomain. com) and the value will be ghs. googlehosted. com. Create and save this CNAME record and this will point your custom domain to your Google hosted website. A note from the author: Thank you for reading this article — I hope you found it useful. Sometimes the easiest options are the best and for a free solution, you can’t go far wrong with Google Sites. Yes, it’s basic, and yes, there are other platforms out there like Wix. But it gets the job done and it’s really easy to use. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 9,
    "url": "https://goatsintrees.net/solve-a-sudoku-using-javascript/",
    "title": "Solve a Sudoku with JavaScript",
    "body": "2020/05/02 - Rules of the game: A brief summary for those that are not familiar with the rules of Sudoku. A completed Sudoku is a 9 x 9 grid, where each row, column, and each 3 x 3 square must contain each of the numbers 1–9 (and once only). The grid below depicts a single row, column and square adhering to these constraints.  Games are presented with a partially completed grid and degrees of complexity given how many cells are pre-populated and how those are positioned (offering ‘clues’). They range from Easy through to Expert. Game input: For the benefit of the challenge, we can assume we will begin the game with an array of length 9 (rows). Each row is itself an array of length 9 (cells). Values will be integers 0–9, where zero indicates a blank. Example Sudoku taken from Wikipedia Helper functions: Regardless of algorithm (I will talk later through two approaches I took), a few helper functions are always useful. Source code is shared at the end.    get_row(board, row) — since the game board is an array of rows, this is trivial and just a luxury method !     get_column(board, column) — return all cells for the provided column.     get_square(board, square) — we can logically split the board into nine 3 x 3 squares, and given (row,col) coordinates we can identify the square and then return all the cells from within the same square.     is_solved(board) — given a completed board, is it correct i. e. have we finished the puzzle? This function should check every row, every column, and every square has the full set of values 1–9.     print_board(board) — a function that helps visualize the board/grid. The output below is the starting position of the board for the game array pictured earlier.  |=======|=======|=======|| . . . | . . . | . . . || . . . | . . 3 | . 8 5 || . . 1 | . 2 . | . . . ||=======|=======|=======|| . . . | 5 . 7 | . . . || . . 4 | . . . | 1 . . || . 9 . | . . . | . . . ||=======|=======|=======|| 5 . . | . . . | . 7 3 || . . 2 | . 1 . | . . . || . . . | . 4 . | . . 9 ||=======|=======|=======|Two approaches: I came to this challenge from two perspectives and in the end I used both techniques — using a combination was quicker for complex Sudoku puzzles. Approach 1. The first approach is applying brute force to the problem. We can apply a backtracking algorithm by iterating over all the empty cells, beginning with the value 1 and checking it’s valid. We then move to the next empty cell, set that to 1 and check it’s valid. And so on. At any point we encounter an impossible solution (a cell with no valid values), we try the next value, and then the next. Should we exhaust all combinations in the current cell, we backtrack a loop, increment the previous cell value and begin again. With such an approach, it’s not uncommon to backtrack all the way to start several times. To implement this approach we can use a simple loop and apply recursion (a function that calls itself with an altered or cut-down problem space) — the logic is only a few lines. I found a brute force approach solved all Sudoku games — but that came at a performance cost. (albeit a fraction of a second). Approach 2. That’s why I introduced a second approach — one value cell constraint function — which when used on its own solved all the Easy-Medium problems I threw at it, without ever needing to fallback to brute force at all. Here we apply techniques a person solving a puzzle would use — as such the logic is quite a bit longer. We take each empty cell and identify the possible values it can hold — updating each blank cell as an array of its possible values. If any cell can contain one value only, we formally set it to that value in the master game grid. So in this rather trivial example, the cell marked “?” only has a single possible value (9).  What if we have more than one possible value for a cell? In this case, we compare each possible value x to the others cells in the row, column, and square. If x only appears as a possibility for this cell, regardless of what other possibilities this cell may have, it is guaranteed that this cell has this value. To elaborate, in this next example the cell marked “?” could at first-pass have the possibilities [1, 2, 3, 4, 5, 8, 9]. However, when we look at either the row or the square, we would not find the value 1 as a possibility in any other cell (marked as an ‘x’). Therefore we know 100% that the cell “?” has to be where the 1 gets placed. A human would simply apply this rule in their head.  Approach two takes iterating these two loops throughout the table until no more updates can be applied — and it solves most simple to medium complexity Sudoku problems. It also makes cracking a complex Sudoku— via brute force — much quicker too, because we have pre-populated many of the cells ahead of time. Final solution: The solution is a combination of the two approaches;    One value cell constraint — most non-complex solutions are solved here.     Brute force / backtracking — within here we can re-use logic from (1), i. e. each time we pick a value we can then “look ahead” and fill in any cells that now only have a single possible value, cutting the problem space down (or quickly eliminating the selection we made as impossible).  The combination of both approaches solved my earlier performance issue with complex Sudoku challenges with using brute force alone. The code: The entry function is solved(board) and this calls functions implementing the two approaches previously described. We iterate over our one value cell constraint function, so long as updates are still being applied (i. e. cells are being filled in because they have only one possible value). That loop ends when we have either solved the Sudoku or it is no longer filling in any cells. If we have not solved the Sudoku, we then fall back to the brute force function. We assume the grid can be solved — i. e. no trying to catch us out — so at the end we return the board as a completed solution. function solve(board) { let updated = true, solved = false   while (updated &amp;&amp; !solved) {  updated = one_value_cell_constraint(board)  solved = is_solved(board) } if (!solved) {  board = backtrack_based(board) } return board}In the one_value_cell_constraint function (shown below) we keep record of updated and any change to the grid keeps us going for another iteration — as we are live editing the game grid, each loop within the function builds upon the previous updates. Internally, we make use of a function called complete_cell which we also use in the brute force logic later. This function looks for all possible values for the cell — if there is a single value, it sets the cell to that value; if there are multiple values it sets the cell to an array containing all its possible values (e. g. [1, 4, 5]). The next section picks up any cells having a range of possibilities and looks at the corresponding row, column, and square to see if a possible value appears_once_only. If it does then we set the cell to that value. function one_value_cell_constraint(board) { updated = false // Convert every gap into an array of possibilities for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (board[r][c] == 0) {    updated = complete_cell(board, r, c) || updated   }  } } // Look out for any possibility that appears as a possibility // once-only in the row, column, or quadrant.  for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (Array. isArray(board[r][c])) {    let possibilities = board[r][c]    updated =      appears_once_only(board, possibilities,        get_row(board, r), r, c) ||     appears_once_only(board, possibilities,        get_column(board, c), r, c) ||     appears_once_only(board, possibilities,        get_square(board, square_coordinates[r][c]), r, c) || updated   }  } } // Reinitialize gaps back to zero before ending for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (Array. isArray(board[r][c])) {    board[r][c] = 0   }  } } return updated}Finally, the brute force logic is in our backtrack_based function. As we may follow dead ends, we work against copies of the array — even though JavaScript passes the board in by value, all the objects within the array are references — the JSON library helps deep-clone the array. We iterate each empty cell and initially re-use the complete_cell function we referred to earlier. This will fill in the cell if it’s only got a single possible value — useful when we get to the very last empty cell, this will essentially solve the board so we include a is_solved check here and return the solved board. Assuming instead we get a list of possible values, we iterate these possibilities in turn — set the cell to the first value in the list, and recursively call the function. At some stage, one recursion will end successfully (with complete_cell filling in the final cell), but most likely we will hit a ‘dead end’. Should we exhaust all possible values, we return false and backtrack. function backtrack_based(orig_board) { // Create a temporary board for our recursion.  let board = JSON. parse(JSON. stringify(orig_board)); for (let r = 0; r &lt; 9; r++) {  for (let c = 0; c &lt; 9; c++) {   if (board[r][c] == 0) {    complete_cell(board, r, c)    if (is_solved(board)) return board;    let cell = board[r][c]    if (Array. isArray(cell)) {     for (let i = 0; i &lt; cell. length; i++) {      // Create a temporary board for each recursion.       let board_2 = JSON. parse(JSON. stringify(board));      // Choose a value      board_2[r][c] = cell[i]      // Recurse again using new board      if (completed_board = backtrack_based(board_2)) {       return completed_board;      }     }     return false // dead end    }   }  } } return false;}Conclusion and examples: First of all — the code worked, including against the “Sudoku designed to work against the brute force algorithm”. Applying two different techniques together was a valuable learning point.  The code could be optimized, although performance is sub-second even for the most difficult. Copying arrays per recursion introduces a memory overhead that I could have worked through. Some example games and solutions:  Further Reading:    Source code for this solution can be found on GitHub.     Wikipedia article on “Sudoku solving algorithms”  A note from the author: Thanks for reading and I hope you enjoyed the article and found it useful. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 10,
    "url": "https://goatsintrees.net/use-ansible-to-create-and-configure-ec2-instances-on-aws/",
    "title": "Use Ansible to create and configure EC2 instances on AWS",
    "body": "2020/04/25 - Inspiration: I’ve been using the awesome A Cloud Guru “Cloud Playground” sandboxes to progress through some of their training content. These environments are fully-functional AWS accounts and allow the user to follow along the Cloud certification tutorials and training — a great new feature of A Cloud Guru. A sandbox lasts 5 hours, which is usually ample enough time to follow along, but every so often I find that I come back the next day and want to recreate some baseline infrastructure and pick up from where I left off. That inspired me to look at Infrastructure as Code and build out a repeatable platform from code blueprints. I’ve experience using Terraform by Hashicorp, and I understand using Terraform and Ansible together. I’m also aware of AWS CloudFormation, and also how I could snapshot an image and create an AMI. However, since Ansible is also a cloud infrastructure provisioning tool, my use-case looked a good challenge to demonstrate using the Ansible modules for AWS. Let’s get stuck in. Dependencies: On your machine, have the following installed.  Ansible Python ≥ 2. 6, with boto, boto3, and botocore. I also have AWS CLI, and have configured the “Cloud Sandbox” AWS Access Key and AWS Secret Id using aws configure. You don’t have to do this way, but I tend to find having the AWS CLI to hand as well is easier all round — follow the boto installation instructions for other options. What we’re going to create: Our goal is to create an EC2 instance in the default VPC — located in North Virginia (us-east-1) in the case of the Cloud Sandboxes. We’re going to make that EC2 instance accessible over ssh from our IP only. For that we will need to create an EC2 key pair. We’re going to ensure that the instance has a few tools — for the purpose of demonstration, we’ll let Ansible install packages onto the EC2 instance. Steps to follow:    Create an EC2 key pair (if one does not already exist — Ansible has built-in idempotency, one of is many plus points) and save the private key to file.     Determine information about the default VPC and its subnets. Randomly select a subnet from the list to host our EC2 instance.     Determine our public IP address and create a security group allowing ssh access from our IP address (only).     Create an EC2 instance in the selected subnet and associated with the security group, and we’ll update our inventory with the new host.     Install git and php to the jumpbox.     Check out our new instance.  Initialize project and set up our variables: I’ve started off the exercise with a brand new, empty project directory and in there created three empty directories: inventory/roles/keys/Navigate into inventory/ and create a file called local as shown below. [local]127. 0. 0. 1 ansible_connection=localCreate another file called ec2 with just the following contents. The playbooks will append host information into here later. [jumpbox]When we run our commands, we can specify this inventory directory with the option -i inventory and Ansible will pick up the contents from here. I created a roles/ directory and in there ran ansible-galaxy init create-ec2-instances to create a basic *roles *outline structure to manage the tasks. For the purposes of these tutorial, I’ve referenced the following variables which helps avoid some hard-coding in the tasks. The AMI is that of a standard Amazon Linux 2 in us-east-1. region_name: 'us-east-1'key_name: 'my_keypair'ami_id: 'ami-0323c3dd2da7fb37d'instance_type: 't2. micro'instance_name: 'jumpbox'Step 1. Create a new EC2 key pair. : Our first step is to let Ansible create a new EC2 key pair. We register the output and then we can write the private_key contents into a local pem file in the keys/ directory. Don’t forget the file permissions. - name: Create a new EC2 key pair ec2_key:  name:     region:     register: ec2_key- name: Save private key copy: content=   dest= . /keys/. pem  mode=0600 when: ec2_key. changedStep 2. Obtain networking information from AWS. : There are two pieces of AWS network information we want to know, and fortunately Ansible provides a way to query for both of these.    Default VPC     Subnets in that default VPC  If we were building out a larger piece of infrastructure — i. e. in our own AWS Account — we would probably want to create a brand new VPC and subnets, which is also feasible with Ansible. Here though, finding and using the default is sufficient. In the yaml below you will see three tasks.    Firstly, we filter the VPC list on whether it has the isDefault flag set. You can filter on all kinds of attributes and these match against the AWS CLI. We save the resultant response into default_vpc — you will see from the next step, it is an array (in our case, of 1 entry) and it is the value of vpc_id that we are interested in.     Secondly, we query the subnets to extract all those associated with the default VPC using the vpc_id as a filter. We register this as subnet_info     Finally, we use jinja to extract all the subnet id values from subnet_info into a list, and then select one at random — that’ll be the subnet we’ll create our instance into.  - name: Obtain default VPC information ec2_vpc_net_facts:  filters:    isDefault :  true  register: default_vpc- name: Obtain subnets for default VPC ec2_vpc_subnet_facts:  filters:   vpc-id:  {{ default_vpc['vpcs'][0]['vpc_id'] }}  register: subnet_info# Use jinja to select a random subnet from the list of subnet ids- set_fact:  vpc_id:  {{ default_vpc['vpcs'][0]['vpc_id'] }}   random_subnet:  {{ subnet_info. subnets|map(attribute='id')|list|random }} Step 3. Secure our instance. : We want to ensure that only we can access our new server. First of all, we ask Ansible what our public IP is. We can do this by using the ipify_facts module (top tip: you can run this straight from a command line using ansible -m ipify_facts to quickly get at this information too). Once we have our public IP address, we can create a new Security Group that allows SSH access only from that IP. ---# Gather IP facts from ipify. org, will be saved to ipify_public_ip- name: Get my public IP ipify_facts:# Create Security Group and save the output into security_group- name: Create Security Group ec2_group:  name:  -sg   description: Security Group for   vpc_id:     region:     rules:   - proto: tcp    ports:     - 22    cidr_ip:  /32     rule_desc:  allow port 22 from   register: security_groupStep 4. Create the EC2 instances. : We’re now ready for the final step and can create an EC2 instance. Most of the values we’ve set up as variables or picked up along the way (e. g. the vpc_subnet_id) so it’s just filling in the blanks at this stage. You’ll see we can combine exact_count=1 with instance_tags and count_tag to ensure that if we re-run the playbook, we will not create more instances. I’ve noticed through experimentation that this is applied *within the same subnet — *my random subnet selector means I create a few more instances than I wanted, but we could hard-code the subnet and ensure we do only get one. - name: Create EC2 instances ec2:  key_name:     region:     instance_type:     image:     vpc_subnet_id:     group:  -sg   wait: yes  instance_tags:   Name: jumpbox   Env: sandbox  count_tag:    Name: jumpbox   Env: sandbox  exact_count: 1  assign_public_ip: yes register: ec2 - name: Add the newly created EC2 instance(s) to the local host group local_action: lineinfile         path= inventory/ec2         regexp=         insertafter= [jumpbpox]          line=  ansible_user=ec2-user ansible_ssh_private_key_file=keys/. pem ansible_ssh_extra_args='-o StrictHostKeyChecking=no'  with_items:   Once the instance has been created, we append the new host in our inventory/ec2 file that we created at the start. You should find you get something like this.  Because we are dynamically updating the inventory, we can optionally include refresh_inventory and a pause for 30 seconds — gives AWS just enough time to start the VM up and ensure sshd is running. These are important if you’re heading straight into the configuration. - meta: refresh_inventory- pause:  seconds: 30Bringing Steps 1–4 together: At this stage, we’ve got all our tasks set up inside the create-ec2-instances role and our roles/create-ec2-instances/tasks/main. yml looks like this: - name: Create jumpbox in default VPC block:  - import_tasks: key-pair. yml  - import_tasks: network-information. yml  - import_tasks: security-group. yml  - import_tasks: ec2. yml  - meta: refresh_inventory  - pause:    seconds: 30We can create a playbook in the root project directory (call it what you like, I called mine create-ec2. yml). Note that we specify hosts: local for the AWS infrastructure tasks. # Create jumpbox on an EC2 instance- hosts: local gather_facts: False roles:  - role: create-ec2-instancesSimply run the command ansible-playbook create-ec2. yml -i inventory to run through the playbook to create our key, security group, and instance. Step 5. Configure the jumpbox. : In this playbook, we’re going to update yum so that the server is up to date and install a couple of packages — selected at random to demonstrate a step to set up of our jumpbox. I opted for git and php — it’s only a demo. Note our new playbook references hosts: jumpbox which picks out the hosts from the inventory/ec2 file that we dynamically appended to in Step 4. - hosts: jumpbox become: True gather_facts: True tasks:  - name: Upgrade all yum packages   yum:    name: '*'    state: latest  - name: Install packages   yum:    name:      vars:    packages:     - git     - phpStep 6. Check it all out. : At this stage;    we should have a jumpbox created in the default VPC in our AWS Account.     our instance should be accessible over ssh only to our IP address.     the jumpbox should have our packages pre-installed and ready to use.  Let’s check it out, just to be sure and ssh into it — remember the auto-generated private key has been saved into the keys/ directory.  All completed — the server is created and accessible. A quick check and we have git and php installed as expected. Job done. A note from the author: Thank you for reading this article — I hope you found it useful. As mentioned in the introduction, there are many ways to accomplish this task and I look forward to your comments and feedback. All source code for this demonstration can be downloaded at GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 11,
    "url": "https://goatsintrees.net/convert-an-old-php-application-to-docker-containers/",
    "title": "Convert an old PHP application to Docker containers",
    "body": "2020/04/19 - Backstory: PHP was always the go-to language for me. I started exploring PHP in 2000, around the time of PHP 4 launch, in an effort to convert an increasingly popular website that was written as lots of individual, static HTML pages. I began work on writing a Content Management System from scratch, and the CMS was still distributing 30,000 articles some 17 years later. This walk through is about another PHP application — it was written using the latest PHP 5 features at the time, and was left happily running in a single Amazon EC2 “classic” instance since 2015. When I started this migration, the last commit was July 30th 2016. Existing platform: The frontend was left running a variety of third party libraries, notably Bootstrap 3. 3 and jQuery 2. 2. The server-side was Symfony 3. 1 and templates were Twig 1. x. Data was obtained from an external API using guzzle and locally cached using doctrine cache 1. 6. Logging was done with monolog. Build toolkit included Node. js for npm (package manager), Grunt (javascript task runner), and Bower (web package manager). CSS stylesheets were written in sass, so we included a parser to create css, concatenate, and minimize. Javascript was consolidated, obfuscated (“uglified”), and also minified. To get a feel for the build steps, here’s the “build-all” grunt task: grunt. registerTask('build-all', ['clean:all', 'concat', 'uglify', 'sass', 'cssmin', 'bowercopy', 'copy', 'string-replace:version', 'composer:dist:install:optimize-autoloader', 'composer:dist:update:optimize-autoloader']);And finally, as you can see from the above, I was using Composer for PHP dependency management. And finally the server itself was using Nginx with php-fpm. Everything was stuck in the year 2015. My development server had long since been destroyed. I couldn’t do any changes; I didn’t even have PHP installed any more. Aspiration:    Upgrade front end to the latest versions of Bootstrap and jQuery.     Update server side runtime libraries to the latest version — apply necessary code changes required as a result, but no other functional changes.     Upgrade the built-kit to the latest versions — but no other changes to the tasks or tools used.     Deploy to an up-to-date operating system, with PHP 7. x.  My overarching constraint for the above was that I didn’t want to have to work through and recreate an entire development platform to do it. I wanted to find a way I could achieve the upgrade without needing to stack my Windows laptop with grunt, bower, nodejs, composer, PHP, just for this one task. I turned to Docker to save the day. Creating a build container: My build server required the following tools to be installed:  PHP 7. x Composer Node. js Grunt BowerTransitively, for SASS parsing, I subsequently found I needed:  RubyPHP and Composer (build time): Given this was used at build time, I wasn’t concerned about having a bloated image so I decided to go for a full PHP installation as the base image. I used the latest PHP image (on Alpine) for my build environment (which I gave the label “buildenv”). FROM php:7. 4. 1-alpine AS buildenvUnfortunately, Composer isn’t included by default and while I could follow the Composer installation instructions, we only need the final ‘composer’ binary file. Luckily, there’s an official Composer image to use. So we reference that image in our layers, and copy the composer binary over into our buildenv. FROM composer:1. 9. 1 AS composer FROM php:7. 4. 1-alpine AS buildenv COPY --from=composer /usr/bin/composer /usr/bin/composerAt this stage, we have an Alpine 3. 10 operating system with PHP 7. 4. 1 and Composer 1. 9 available. Node. js, Grunt, Bower (build time): Building upon the previous layer, I then looked to the other build tools I required. These I grouped into a single layer, and installed the packages and ensured npm was up to date. Job done. RUN apk add --update nodejs npm &amp;&amp; \  npm update -g npm &amp;&amp; \  npm install -g grunt-cli &amp;&amp; \  npm install -g bower Ruby (build time): A bit of trial and error determined that the sass parser required Ruby. This is in its own layer to keep it separate while I worked out the packages I needed to install. More than I thought and remembered. RUN apk add --update ruby ruby-bundler ruby-dev \  ruby-rdoc build-base gcc &amp;&amp; \  gem install sassCreate a working directory: I always like to do everything inside a working directory, so I create /app and work inside here for all subsequent stages. RUN mkdir /app WORKDIR /appDownloading the application dependencies: Remarkable looking back at all of the stages to get through before we can commence the application build. In summary, we have these steps for downloading the build time and runtime dependencies we require;    npm install — download the javascript build libraries into node_modules     composer install — PHP server side runtime dependencies, such as Twig and Symfony.     bower update — download all the client side runtime libraries, such as jQuery.  # Copy npm dependenciesCOPY package. json . RUN npm install # Copy composer and download dependenciesCOPY composer. json . RUN composer install # Copy bower and download dependenciesRUN apk add --update gitRUN echo '{  allow_root : true }' &gt; /root/. bowerrcCOPY bower. json . RUN bower updateTo briefly explain the above techniques.    I copy only the files needed for the job (e. g. package. json, composer. json, or bower. json) — that’s so that I can isolate changes and limit recreating layers unnecessarily. Some of these steps can take 5 minutes to complete so I don’t want to trigger a full rebuild because I changed an unrelated file.     Bower — needs git, so this stage I added that. Secondly, bower throws a warning when it is run as ‘root’ user — so the second line suppresses that.  So now we have a build environment that has an Alpine 3. 10 operating system with PHP 7. 4. 1 and Composer 1. 9 available. It has the latest versions of nodejs, grunt, and bower, and we have downloaded all the build-time and runtime dependencies that we require. I think now we’re ready to build the application. Building the application: In Docker terms, this may feel like an anticlimax. Because I’m using grunt, the existing build-all task that I created back in 2015 will still do the job. # Copy all the remaining source codeCOPY src/ /app/srcCOPY Gruntfile. js . RUN grunt build-allThis is where all the work is done. You can skip this if you want, but I’ve shared so you can see the original build task. You might find this snippet useful to take some concepts into your own project. In summary, what we are doing is:    cleaning our build directory     concatenating all javascript source code into a single file.     obfuscating the javascript code     using sass parser to create our css stylesheets     minifying the css stylesheets     copying all our javascript and css, third party css, and fonts over to our final distribution directory     copy all server-side source code into the directory ready for Composer     update version number placeholder     run Composer to create an optimized production-ready runtime  It wasn’t all plain sailing when I upgraded my libraries to their latest version. Some changes were easy, sure, but a few required code updates — after all, I had to accommodate 5 years of deprecated features and changes. But all in all, it wasn’t as painful as I expected. module. exports = function (grunt) {  // Project configuration.   grunt. initConfig({    pkg: grunt. file. readJSON('package. json'),    clean: {      all: {        src: [ dist ,  tmp ,  . sass-cache ],        options: {          force: true        }      },      web: {        src: [ dist/web ,  dist/templates ,  dist/compilation_cache ,  dist/doctrine_cache ,  tmp ,  . sass-cache ],        options: {          force: true        }      }    },    jshint: {      all: ['src/js/*. js']    },    concat: {      all: {        src: ['src/js/*. js'],        dest: 'tmp/js/&lt;%= pkg. name %&gt;. js'      }    },    sass: {      dist: {        files: {          'tmp/css/&lt;%= pkg. name %&gt;. css': 'src/scss/main. scss'        }      }    },    cssmin: {      target: {        options: {          banner: '/*! &lt;%= pkg. name %&gt; &lt;%= grunt. template. today( yyyy-mm-dd ) %&gt; */'        },        files: [{          expand: true,          cwd: 'tmp/css/',          src: ['*. css', '!*. min. css'],          dest: 'dist/web/css/',          ext: '. min. css'        }]      }    },    uglify: {      options: {        banner: '/*! &lt;%= pkg. name %&gt; &lt;%= grunt. template. today( yyyy-mm-dd ) %&gt; */\n'      },      build: {        src: 'tmp/js/&lt;%= pkg. name %&gt;. js',        dest: 'dist/web/js/&lt;%= pkg. name %&gt;. min. js'      }    },    bowercopy: {      javascript: {        options: {          destPrefix: 'dist/web/js'        },        files: {          'jquery. min. js': 'jquery/dist/jquery. min. js',          'bootstrap. min. js': 'bootstrap/dist/js/bootstrap. min. js',          'html5shiv. min. js': 'html5shiv/dist/html5shiv. min. js',          'respond. min. js': 'respond/dest/respond. min. js',          'underscore. min. js': 'underscore/underscore-min. js',          'jquery. dataTables. min. js': 'datatables. net/js/jquery. dataTables. min. js',          'dataTables. bootstrap. min. js': 'datatables. net-bs/js/dataTables. bootstrap. min. js'        }      },      css: {        options: {          destPrefix: 'dist/web/css'        },        files: {          'bootstrap. min. css': 'bootstrap/dist/css/bootstrap. min. css',          'bootstrap-theme. min. css': 'bootstrap/dist/css/bootstrap-theme. min. css',          'font-awesome. min. css': 'components-font-awesome/css/font-awesome. min. css',          'dataTables. bootstrap. min. css': 'datatables. net-bs/css/dataTables. bootstrap. min. css'        }      },      bootstrap_fonts: {        files: {          'dist/web/fonts': 'bootstrap/dist/fonts/*. *'        }      },      font_awesome_fonts: {        files: {          'dist/web/fonts': 'components-font-awesome/fonts/*. *'        }      }    },    copy: {      main: {        files: [          {expand: true, cwd: 'src/php/web/', src: ['**'], dest: 'dist/web/'},          {expand: true, cwd: 'src/php/lib/', src: ['**'], dest: 'dist/lib/'},          {expand: true, cwd: 'src/static/', src: ['**'], dest: 'dist/web/'},          {expand: true, cwd: 'src/ico/', src: ['**'], dest: 'dist/web/ico/'},          {expand: true, cwd: 'src/img/', src: ['**'], dest: 'dist/web/img/'},          {expand: true, cwd: 'src/css/', src: ['**'], dest: 'dist/web/css/'},          {expand: true, cwd: 'src/config/', src: ['**'], dest: 'dist/config/'},          {expand: true, cwd: 'src/templates/', src: ['**'], dest: 'dist/templates/'},          {src: ['composer. json'], dest: 'dist/'}        ]      }    },    composer: {      dist: {        options: {          cwd: 'dist'        }      }    },    'string-replace': {      version: {        files: {          'dist/config/settings. ini': 'dist/config/settings. ini'        },        options: {          replacements: [{            pattern: '%APPLICATION_VERSION%',            replacement: '&lt;%= pkg. version %&gt;-&lt;%= grunt. template. today( yyyymmdd ) %&gt;'          }]        }      }    }  });  grunt. loadNpmTasks('grunt-contrib-clean');  grunt. loadNpmTasks('grunt-contrib-concat');  grunt. loadNpmTasks('grunt-contrib-sass');  grunt. loadNpmTasks('grunt-contrib-jshint');  grunt. loadNpmTasks('grunt-contrib-uglify');  grunt. loadNpmTasks('grunt-contrib-cssmin');  grunt. loadNpmTasks('grunt-composer');  grunt. loadNpmTasks('grunt-bowercopy');  grunt. loadNpmTasks('grunt-contrib-copy');  grunt. loadNpmTasks('grunt-string-replace');  grunt. registerTask('build-all', ['clean:all', 'concat', 'uglify', 'sass', 'cssmin', 'bowercopy', 'copy', 'string-replace:version', 'composer:dist:install:optimize-autoloader', 'composer:dist:update:optimize-autoloader']);  grunt. registerTask('build-web', ['clean:web', 'concat', 'uglify', 'sass', 'cssmin', 'bowercopy', 'copy', 'string-replace:version']);  grunt. registerTask('run-composer', ['composer:dist:install:optimize-autoloader', 'composer:dist:update:optimize-autoloader']);};At this stage we have a buildenv containing a directory /app/dist/ with all our final build output ready to be run. Building the runtime image: So far everything we have done is in our ‘buildenv’. But we don’t want all this bloatware in our final image. So we start again with a slim operating system, and I chose Alpine. On top of that, we need PHP but this time we only need a small subset of the PHP packages to run this application (your mileage will vary). And for this particular runtime, we still have Nginx. As you will see later, I am also using supervisor to manage the processes to ensure php-fpm and nginx are kept running. # Create final imageFROM alpine:3. 11. 0 # Install packages RUN apk upgrade &amp;&amp; apk --no-cache add php7 php7-fpm \  php7-json php7-openssl \  nginx supervisor curlConfiguration: Our three processes each need some configuration, so we copy those into place in the image. These trivial configuration files have been shared in the example repository on GitHub. In summary, the configuration is simply to allow Nginx to listen on port 8080 and redirect requests to php-fpm to handle. # Configure nginx COPY config/nginx. conf /etc/nginx/nginx. conf # Configure php-fpm COPY config/fpm-pool. conf /etc/php7/php-fpm. d/www. conf COPY config/php. ini /etc/php7/conf. d/zzz_custom. ini  # Configure supervisord COPY config/supervisord. conf /etc/supervisor/conf. d/supervisord. confUse nobody user to own directories: We’re using the ‘nobody’ user, so we need to ensure directory permissions are aligned. RUN chown -R nobody. nobody /run &amp;&amp; \  chown -R nobody. nobody /var/lib/nginx &amp;&amp; \  chown -R nobody. nobody /var/log/nginxWe now have our runtime image, with PHP and Nginx installed and configured. But we still need our application. Copy the final distribution from the build environment: Let’s create the directory /var/www/html and switch to the nobody user, then copy all the distribution contents over from the ‘buildenv’. # Setup document root RUN mkdir -p /var/www/html  # Switch to use a non-root user from here on USER nobody # Add application WORKDIR /var/www/html COPY --chown=nobody --from=buildenv /app/dist/ /var/www/html/Expose the port: Nearly at the end now — we need to expose the port that Nginx will be reachable on. # Expose the port nginx is reachable on EXPOSE 8080Ensure services are running: The final stage now is to use supervisord to ensure that Nginx and php-fpm are running and able to serve requests. # Let supervisord start nginx &amp; php-fpm CMD [ /usr/bin/supervisord ,  -c ,  /etc/supervisor/conf. d/supervisord. conf ]Conclusion: And that’s it — we can build the image and run it locally. Our runtime has a low footprint because we used a slim base image and only installed the absolute bare-minimum packages we required to run our application. Your situation will obviously be different, the libraries you use may be more complex — but I hope the overview gives a sense of what can be accomplished. For build automation, I then combined **GitHub **with **DockerHub **so that future commits automatically kick off a build and the creation of a new versioned image. And I deployed that into Google Cloud Run, where the exact same image I ran on my laptop I could also happily run in Production. I killed off my Amazon EC2 “classic” micro instance after almost 6 years, and reduced the cost of running to pennies a year. And by choosing Docker, I had a maintainable platform to keep on top of. The final Dockerfile: I ended up with a single Dockerfile, applying the Docker multi-stage build technique, and ultimately created a cut-down, slim runtime image. I could have consolidated a few layers, and break out, but once the initial set up was done, I could spin out new images in under a minute so I left it all as one. FROM composer:1. 9. 1 AS composerFROM php:7. 4. 1-alpine AS buildenvCOPY --from=composer /usr/bin/composer /usr/bin/composerRUN apk add --update nodejs npm &amp;&amp; \  npm update -g npm &amp;&amp; \  npm install -g grunt-cli &amp;&amp; \  npm install -g bowerRUN apk add --update ruby ruby-bundler ruby-dev ruby-rdoc build-base gcc &amp;&amp; \  gem install sassRUN mkdir /app WORKDIR /app# Copy npm dependenciesCOPY package. json . RUN npm install # Copy composer and download dependenciesCOPY composer. json . RUN composer install# Copy bower and download dependenciesRUN apk add --update gitRUN echo '{  allow_root : true }' &gt; /root/. bowerrcCOPY bower. json . RUN bower update # Copy all the remaining source codeCOPY src/ /app/srcCOPY Gruntfile. js . RUN grunt build-all# Create final imageFROM alpine:3. 11. 0# Install packages RUN apk upgrade &amp;&amp; apk --no-cache add php7 php7-fpm php7-json php7-openssl \  nginx supervisor curl# Configure nginx COPY config/nginx. conf /etc/nginx/nginx. conf# Configure PHP-FPM COPY config/fpm-pool. conf /etc/php7/php-fpm. d/www. conf COPY config/php. ini /etc/php7/conf. d/zzz_custom. ini # Configure supervisord COPY config/supervisord. conf /etc/supervisor/conf. d/supervisord. conf# Make sure files/folders needed by the processes are accessable when they run under the nobody user RUN chown -R nobody. nobody /run &amp;&amp; \    chown -R nobody. nobody /var/lib/nginx &amp;&amp; \  chown -R nobody. nobody /var/log/nginx# Setup document root RUN mkdir -p /var/www/html # Create cache directoriesRUN mkdir /var/www/html/compilation_cache &amp;&amp; chown nobody. nobody /var/www/html/compilation_cache &amp;&amp; \  mkdir /var/www/html/doctrine_cache &amp;&amp; chown nobody. nobody /var/www/html/doctrine_cache# Switch to use a non-root user from here on USER nobody# Add application WORKDIR /var/www/html COPY --chown=nobody --from=buildenv /app/dist/ /var/www/html/# Expose the port nginx is reachable on EXPOSE 8080 # Let supervisord start nginx &amp; php-fpm CMD [ /usr/bin/supervisord ,  -c ,  /etc/supervisor/conf. d/supervisord. conf ] # Configure a healthcheck to validate that everything is up &amp; running HEALTHCHECK --timeout=10s CMD curl --silent --fail http://127. 0. 0. 1:8080/fpm-pingA note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. Source code is available from this example repository on GitHub. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 12,
    "url": "https://goatsintrees.net/schedule-aws-lambda-to-download-a-file-from-the-internet-and-save-it-to-s3/",
    "title": "Schedule cron in Lambda to download a file and save it to S3",
    "body": "2020/04/06 - This post will explain how to use AWS Lambda to download a file each day and save the file into an S3 bucket. Why did I pick Lambda? The task runs for ~1 second every day and I don’t really need a virtual machine for that — or any infrastructure in fact— I sought the cheapest and simplest solution that I could natively trigger my code on a schedule (cron). AWS Lambda ticked all the right boxes for me, and the cost of the solution is less than a $1 a year. Here’s the work brief that we’ll go through in this short tutorial.    The process should run once every hour of the day.     The process should download a file from an internet location (https) and save it into an S3 bucket.     The object metadata should include the content type from the origin.     It should be designed/written in such a way as we can run the same code for different source files.  I’m going to demonstrate how to do this directly via the Console, and I’ll follow up with setting up a development environment and using the AWS Serverless Application Model and command line to achieve the same. All the source code for this article can be downloaded from GitHub. Approach 1 — AWS Console: The steps we’re going to follow are;    Create an S3 bucket to hold our file     Create a Lambda function     Update IAM to allow our Lambda function to write to S3     Write our code and set some dynamic properties (source file, target bucket, and the target filename).     Create a Test and verify everything is working.     Configure a Schedule so the Lambda function will run every day.  First step is to create our bucket in AWS S3 — I selected all the default options, and I’ll be using a bucket called “our-lambda-demo”.  Next step is to head over to AWS Lambda and “Create function” where we are going to select to “Author from scratch”.  I’ve named the function “downloadFileToS3” and left all the defaults in place.  Once your function has been created, go to the “Permissions” tab and follow the link to our Execution Role in the AWS IAM Management Console.  Once in IAM, you will see that the default setup only has basic Lambda execution permissions. This includes the ability to write out to CloudWatch logs, but little else. So we need to give write access to S3.  I’m going the “Add inline policy” route, but you could also go through “Attach policies” and add an existing managed policy such as “AmazonS3FullAccess”. You can use the dialogue to create a policy, or download the JSON if you want to just copy mine (remember to change the bucket name). {   Version :  2012-10-17 ,   Statement : [    {       Sid :  VisualEditor0 ,       Effect :  Allow ,       Action :  s3:PutObject ,       Resource :  arn:aws:s3:::our-lambda-demo/*     }  ]}Give your Policy a name and save. Back in Lambda, navigate to the code on the Configuration tab and we’re going to upload some code and its dependencies. Lambda provides us with access to the ‘aws-sdk’ automatically but anything else you will need to upload — the AWS SAM instructions show a more complete solution that allows you how to develop the code further, package everything together automatically, and deploy. For now, select “Upload a . zip file” and upload the provided index. zip file (shared on GitHub).  Once uploaded, you should see the index. js file with the function code and its dependencies under node_modules.  You’ll see that this piece of code requires three environment variables to be set which you can add from the Configuration tab also.    SOURCE_URI — the full path to the internet source we are downloading     S3_BUCKET — the bucket we are writing to     S3_KEY — the name of the file we are going to write into S3  Scroll down to “Manage environment variables” and add the variables. Once done, you should have something that looks like this.  We should now be ready to test out our function. You can find “Test” at the upper right of the screen, use that to create a dummy test execution. Leave all the defaults in place and call it “MyTestEvent” and hit Create.  Ensure your new test event is selected in the drop down and click “Test” again to run your test.  All going well, and you should see output like this showing the completion of your test along with metadata about the job execution and a link to the CloudWatch logs.  Heading back to AWS S3 and we can see that our file was saved.  And opening the file displays its contents.  Awesome! Next and final stage is to configure a schedule so that our job will continue to run and update the file in our S3 bucket. Back in the Configuration tab, select “+ Add trigger”.  From here we’re going to choose “CloudWatch Events/EventBridge” and create a new rule.  Give your rule a name and create the schedule using Cron or rate expressions.  Save and you should find your Trigger is now generated.  I updated mine to every minute to demo the success of running on a schedule.  So that’s it. Congratulations. We’ve written a Lambda function that runs on a schedule, will download a file and save that file to an S3 bucket. Approach 2 — AWS Serverless Application Model: This section will repeat the same process — from scratch — using AWS Serverless Application Model, which is an “open-source framework that you can use to build serverless applications on AWS”. AWS SAM can be used for local running and testing of our NodeJS Lambda function, and helps us to build and deploy the application. It’s a really cool tool, and behind the scenes it creates a declarative CloudFormation template that defines the stack and all the associated services required. Setting up your development environment:    Install AWS SAM     Install Node. js (because our example was written in Node. js).  Although we don’t need the AWS CLI installed to use AWS SAM, I have version 2 installed. If you don’t have the CLI installed, you’ll need to create a credentials file or set your AWS credentials as environment variables. I’ve just reinstalled everything today, so here’s what I have. $ aws --versionaws-cli/2. 0. 6 Python/3. 7. 5 Windows/10 botocore/2. 0. 0dev10$ sam --versionSAM CLI, version 0. 47. 0$ node -vv12. 16. 1$ npm -v6. 13. 4Just a reminder that all the source code for this article can be downloaded from GitHub. I’m going to assume no prior knowledge of Node. js development, but feel free to skip through the project initialization stage. We start off in an empty directory and we initialize using npm. $ npm initGive your application a name and you can leave all the other defaults as-is. On completion, you will get a default “package. json” file. We need to add a few dependencies for our function to work (‘request-promise’ and ‘request’), so follow these two commands to get going. $ npm install request-promise --save$ npm install request --saveYou’ll notice that your “package. json” has been updated and you should have a file structure like this.  With our project initialized and dependencies in place, we are now going to create the Lambda function code, so create a file called “index. js” and copy the contents from here: index. js on GitHub Function source code: The code is trivial to meet our objectives. const request = require('request-promise')const aws = require('aws-sdk');const s3 = new aws. S3();exports. handler = async (event, context, callback) =&gt; { const options = {  uri: process. env. SOURCE_URI,  encoding: null,  resolveWithFullResponse: true }; const response = await request(options) const s3Response = await s3. upload({  Bucket: process. env. S3_BUCKET,  Key: process. env. S3_KEY,  ContentType: response. headers['content-type'],  Body: response. body }). promise() return callback(null, s3Response);};We take in three parameters (as environment variables) and use these to download a file and save to S3. If you skipped the Console demonstration, a reminder of those three parameters that you can see in the code;    SOURCE_URI — the full path to the internet source we are downloading     S3_BUCKET — the bucket we are writing to     S3_KEY — the name of the file we are going to write into S3  So far we’ve got our code and dependencies in place. Now to begin looking at AWS Serverless Application Model or SAM. All AWS SAM operations require a template file (“template. yaml” by default) that defines all the resources we require. This is an extension to CloudFormation so you’ll recognise there are plenty of overlaps. To get started we need create our SAM template in our root directory. Call the file template. yaml. AWSTemplateFormatVersion : '2010-09-09'Transform: AWS::Serverless-2016-10-31Resources: downloadFileToS3:  Type: AWS::Serverless::Function  Properties:   Handler: index. handler   Runtime: nodejs12. x   Policies: AmazonS3FullAccess   Timeout: 10   Environment:    Variables:     SOURCE_URI: https://raw. githubusercontent. com/davelms/medium-articles/master/lamda-download-example/test. txt     S3_BUCKET: our-lambda-demo     S3_KEY: our-example-file   MemorySize: 128This initial template file was very basic and is just to get started. You’ll see we define our handler and runtime, the policy we’re using (the managed AmazonS3FullAccess), and the environment variables. Your project folder should now look like this.  Create target bucket in S3: I’m using the command line throughout for this part of the tutorial, but you can use the Console of course. Remember earlier that I installed the AWS CLI. $ aws s3 mb s3://our-lambda-demomake_bucket: our-lambda-demo$ aws s3 ls2020-04-10 16:15:38 our-lambda-demoOptional Stage — local testing with AWS SAM — requires Docker: Local testing requires Docker to be installed on your workstation. AWS SAM provides a local Docker execution environment that allows for the testing of your Lambda function code without needing to upload to AWS — this is invoked using the sam local invoke command. sam local invoke --no-eventAll being well, you should see output like this showing the Lambda function has been successfully executed on your local workstation and the file was created successfully. The code shared previously returns the output from the s3. upload() command, and we can see that in the JSON in the screenshot.  Let’s view the contents of the bucket to confirm our file was saved. $ aws s3 ls s3://our-lambda-demo2020-04-10 16:17:03     85 our-example-fileBuild, package, and deploy to AWS: Now that we have successfully created our function, and optionally tested it, we are ready to deploy it to AWS. AWS SAM provides a couple of guided stages here:  sam build followed by sam deployFirst of all, lets use AWS SAM to build our application. $ sam buildBuilding resource 'downloadFileToS3'Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:NpmInstallRunning NodejsNpmBuilder:CleanUpNpmrcBuild SucceededBuilt Artifacts : . aws-sam\buildBuilt Template  : . aws-sam\build\template. yamlCommands you can use next=========================[*] Invoke Function: sam local invoke[*] Deploy: sam deploy --guidedYou’ll notice a new directory has been created called . aws-sam/ Now we’re going to deploy to AWS and use all the default options. $ sam deploy --guidedYou’ll see a lot of output as resources are generated (too much to paste here). Also notice how it’s backed off to CloudFormation. We have a stack, function, and role created for us automatically. And if you head over to S3, you’ll find a new bucket was created automatically to contain your versioned, packaged code.  So lets go to the Console to see what was created. And we can run a simple test there too. Starting off in CloudFormation we can see the sam-app stack.  And across in AWS Lambda we can see the function was created successfully. Let’s create a Test and run it to verify… all good… Adding a schedule trigger: So far, we are invoking our new function manually, let’s update the template to include our Schedule under Events. We’re going to add this code.  Events:  downloadFileToS3ScheduledEvent:   Type: Schedule   Properties:    Schedule: rate(1 minute)Your code should look like this with the additional schedule included.  Next we follow the same process as before, click through all the defaults. $ sam build $ sam deploy --guidedYou’ll see that CloudFormation will only update for the changes it recognises need to be applied. Heading back to the Console and we can see that the “CloudWatch Events/EventBridge” Trigger has been created for us.  And a quick check of the job run history, and we can see that it’s run a few times so that’s all worked as expected.  Conclusion: In this article, I demonstrated;    a Lambda function that will download a file from the internet and save it to an S3 bucket, and we passed in parameters so we can re-use the code.     how to schedule Lambda functions to run on a standard cron schedule.     how to achieve all the requirements within the AWS Management Console and by using the AWS Serverless Application Module.  A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. A reminder that all the source code for this article can be downloaded from GitHub. If you want to learn more about AWS SAM, check out the AWS Serverless Application Module documentation. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 13,
    "url": "https://goatsintrees.net/7-youtube-channels-every-cloud-engineer-should-subscribe-to/",
    "title": "7 YouTube channels every Cloud Engineer should subscribe to",
    "body": "2020/03/29 - Let’s be honest at the start; these are the YouTube channels that I subscribe to for training, resources, and to help me keep abreast of all the latest AWS, GCP, and Cloud service news. So jumping straight in and in no particular order… #1 — Google Cloud Platform: First up is the Google Cloud Platform. There’s quite a few playlists in here and videos are uploaded almost daily. Selecting three of my favourites playlists;    “Get Cooking In Cloud” — love these short 5-minute videos, a whole series of “How to…” scenarios and topics, and there are regular sub-series such as Pub/Sub. These videos take the form of practical scenario based problems, and talk through the solutions within the Google Cloud Platform; they teach us the recipe in keeping with the cooking analogy.     “This Week in Cloud” — keeping abreast of the latest updates across the various Cloud platforms is never easy. They move so fast. That’s why I find weekly 2-minute catch-ups like “This Week In Cloud” very useful. Here we get to hear about all the features releases, enhancements, and changes within the Google Cloud Platform.     “Stack Doctor” — not yet a playlist, but regular themed content around Service Level Indicators (SLIs), Service Level Objectives (SLOs), and the Cloud Monitoring suite (formerly Stackdriver). These resonate for me as Site Reliability Engineering Lead in the UK in our organisation.  Finally, “Cloud Minute” is a series of 73 videos demonstrating — all in under one minute each — how to accomplish tasks on the Google Cloud Platform. An awesome reference to bookmark, although no longer being added to. #2 — A Cloud Guru: For those that don’t know it, A Cloud Guru is an awesome cloud training platform and the A Cloud Guru YouTube channel contains some of their free-to-all weekly updates. I’m focussed myself right now on AWS and GCP, but there’s plenty of Azure content as well.    “AWS This Week” — as the name suggests, this is a weekly breakdown of all the latest updates to the AWS platform. It is complemented by “GCP This Month”, a similar playlist but for the Google Cloud Platform — although “This Week In Cloud” on the Google Cloud Platform channel has likely beaten A Cloud Guru to it for many new announcements.     “Kubernetes This Month” — a monthly roundup by Nigel Poulton, author of Docker Deep Dive and The Kubernetes Book, with a catch up of all the recent announcements in all things Kubernetes, Docker, and related domain. The videos always include a deep dive into one or two announcements per month to focus on the important new releases.  #3 — Amazon Web Services: Now a jump across to AWS and the “Amazon Web Services” channel. Every few days I tend to sift through all the new videos and select those that appeal — it’s a very busy channel — looking back at my viewing history, those I select often fall into one of these two playlists.  “AWS Demos” and “AWS Knowledge Center Videos” — I can’t quite work out the difference between these two playlists as the content always seems to overlap in context. Similar to Google’s “Cloud Minute” and “Get Cooking In Cloud”, these playlists feature videos that are short and cover solving specific tasks on Amazon Web Services platform. There is a broad mix of troubleshooting guides for common issues and solutions that focus towards the SysOps Administrator audience, and some that are aimed to Developers and Architects. At times, it’s well worth just working through all the AWS videos to find something that is interesting — some of the discussions with companies around their solutions (“This is My Architecture”) yields some interesting content. #4 — Google Developers: Over 2 million people subscribe to the “Google Developers” channel. Wow. Not specifically “Cloud”, but plenty of overlaps and tool chains — we develop, host, and run applications at the end of the day — so worth adding a developer channel to the list. There are videos and playlists here covering a wide variety of topics, including Android and Flutter development.  “The Google Developer Show” — this is the main playlist I like to check out each week on the Google Developers channel— as the name suggests, it’s a weekly update on the latest developer news from across Google. #5 — Hashicorp: As Cloud Engineers, we’re probably all familiar of Terraform that many of us will have used as our Infrastructure as Code provisioning tool. Likewise, we may also have used Vault for Secrets Management. The “Hashicorp” channel is updated regularly and has a lot of Cloud-related content and some great videos describing the fundamentals of building out secure infrastructures. There are plenty of in-depth “how to” guides and patterns, spanning virtual machines and Containers, and some of my favourites include explaining how to do secure introduction when building out the Cloud platform. #6 — KodeKloud: The “KodeKloud” channel is updated monthly and covers a variety of hands-on training series. I subscribe mostly for the “Docker” and “Kubernetes” series, which have videos pitched toward Absolute Beginners and work themselves through to the more expert topics. They are well-presented and I have many of them saved for reference. There are other topics such as Ansible, Puppet, and OpenShift. #7 — Serverless: I first came across “Serverless” thinking it was a channel dedicated to the serverless topic . In fact they are the creators of the Serverless Framework which “gives you everything you need to develop, deploy, monitor and secure serverless applications on any cloud. ” Despite not being what I was looking for, I stuck around; not because I use the Serverless Framework, but because their video content is great. I reference this channel for general guidance, serverless patterns, and developer how to references in topics such as auth0 and AWS services such as Lambda or DynamoDB (among many others). It’s easy to abstract the core topic away from the Serverless Framework itself. So that’s my current top 7 — but what else?: With “Linux Academy” recent sync-up with A Cloud Guru, it’s not clear what is happening to their YouTube channel. It’s not had any new videos since the announcement in 2019. All the same — some really useful archives of content here that are useful to any Cloud Engineer to keep as a reference. “AWS Online Tech Talks” are much longer episodes and discuss various topics in more details. These need more time commitment as they can often be over an hour in length. The “Docker” channel is also worth keeping in your subscription lists. A note from the author: Thank you for reading this article — I hope you found it useful. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 14,
    "url": "https://goatsintrees.net/3-aws-platform-questions-and-answers/",
    "title": "3 AWS Solutions Architect certification questions",
    "body": "2020/03/28 - In my role as the DevOps Practice Lead at my workplace in the UK, I’ve been trying to think of challenges for the Cloud engineering team that are a little different from those we see in our normal day-to-day work situation. An aim to keep sharp with our skills and technical thinking, and keeping abreast of the ever changing Cloud services platforms. The team already did a great job at pulling together a bank of Google Cloud Platform questions to help them with the GCP Associate Cloud Engineer exam. And that got me thinking about the same for Amazon Web Services. Couple that with being on lock-down — and so we have this article; I’m going to find sample questions and attempt to answer them. Every week, I’ll post an article with another set of questions. And I’ll use these back at work to discuss with the team. I’ve chosen a sample of 3 x recent real-world requests for help and sample questions posted on the Facebook groups “AWS Cloud” and “Amazon Web Services (AWS)”. Disclaimer; I am not advocating that my answers are the correct ones — indeed, I’ve chosen two examples where the online responses were mixed. I will explain my thinking and how I came to the answer I did. I welcome all feedback in the comments.   Scenario 1. An application uses an Amazon RDS MySQL cluster for the database layer. Database growth requires periodic resizing of the instance. Currently, administrators check the available disk space manually once a week. How can this process be improved?A. Use the largest instance type for the database. B. Use AWS CloudTrail to monitor storage capacity. C. Use Amazon CloudWatch to monitor storage capacity. D. Use Auto Scaling to increase storage size. — posted to “AWS Cloud” on 27th March. This question looks like the style of those found in Associate certification exams. A technique that’s useful when taking Amazon AWS Certifications is to first eliminate any obvious wrong answers. This question is no exception and we immediately eliminate the reference in (B) to AWS CloudTrail. Often confused with CloudWatch, however AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. As such, it does not monitor storage capacity and is irrelevant to this question. The next option to eliminate is (A) for using the largest instance type for the database.  A. Use the largest instance type for the database. Why? The question has two statements; firstly, around database growth requiring periodic resizing of the instance. And secondly, around checking for available disk space. Three answers focus in on the disk space. It’s true that with AWS RDS you manually scale up the underlying instance — and that is already happening periodically per the problem statement. How can this be further improved? Well, to some extent picking the largest instance size as (A) suggests would be a hammer to crack that particular nut; a rather expensive hammer though. But it would not help at all with the disk space challenge — maximum storage for MySQL is constant at 64 TiB for each of the** **Latest Generation Standard (m5) or Memory Optimized (r5) Instance Classes — scaling to the largest type in its Class will not offer us an increase in disk storage. Additionally, if the problem statement was around read performance, the introduction of “read replicas” or Amazon ElastiCache would help in many solutions before opting for vertical scaling. So we eliminate (A). As it is, our focus in this question is on the statement around disk usage.  Currently, administrators check the available disk space manually once a week. We have two options remaining, (C) and (D), both of which are improvements on manual checking. But one is better than the other, as I will explain next.  C. Use Amazon CloudWatch to monitor storage capacity.  D. Use Auto Scaling to increase storage size. Let’s think through the process those administrators are taking today — first, they manually check disk usage, and secondly (we have to assume that) if disk usage is above threshold they are increasing the disk allocation manually. Option (C) states to use CloudWatch to monitor storage capacity. There are a couple of techniques for this, including;    Monitor the FreeStorageSpace metric by creating a CloudWatch Alarm, with an SNS topic, and a Subscription to alert the team automatically.     Checking for predefined RDS Events for low storage, again in combination with an SNS topic and Subscription.  How is (C) an improvement? Well, it means the administrators are no longer having a weekly task in their diaries to sign on to manually check the storage usage — now, they will get notified automatically — that’s better detection — but, with (C) it still requires them to take manual action to fix the situation. That leaves us with option (D) which is to use RDS Auto Scaling option to automatically increase the storage allocation. Released in June 2019, RDS Storage Auto Scaling automatically scales storage capacity in response to growing database workloads, with zero downtime. With storage autoscaling enabled, Amazon RDS will trigger a scaling event when these factors apply:    Free available space is less than 10% of the allocated storage.     The low-storage condition lasts at least five minutes.     At least six hours have passed since the last storage modification.  When the event triggers, Amazon RDS will add additional storage in increments of whichever of the following is greater:    5 GiB     10% of currently allocated storage     Storage growth prediction based on the FreeStorageSpace metric change in the past hour.  In this question, our RDS database engine is MySQL — so the auto-scaling will keep going up to a maximum allocation of 64 TiB for all instance types in the latest generation class (m5 or r5). So that’s our answer; we can detect and fix the problem automatically.  D. Use Auto Scaling to increase storage size. So how does my answer (D) compare with those on the “AWS Cloud” group — the group was roughly equal 50–50 split between options (C) and (D). Leave a comment if you would have chosen (C) in this example.   Scenario 2. A company has a popular multi-player mobile game hosted in its on-premises datacenter. The current infrastructure can no longer keep up with demand and the company is considering a move to the cloud. Which solution should a Solutions Architect recommend as the MOST scalable and cost-effective solution to meet these needs?A. Amazon EC2 and an Application Load BalancerB. Amazon S3 and Amazon CloudFrontC. Amazon EC2 and Amazon Elastic TranscoderD. AWS Lambda and Amazon API Gateway— posted to “AWS Cloud” on 26th March. This is both a great and terrible question to pick next! It lacks information about the current solution and what else it relies upon; what databases, user identification and access management, session handling, and so on. What we can do is state our assumptions — and progress from there. What is good and clear about this question is the emphasis on choosing the option that is “MOST scalable and cost-effective”. Let’s get it out of the way to start with — Amazon GameLift, the dedicated game server hosting platform, is not listed as an option. First of all, as before, let’s eliminate the obviously wrong answers. We shall eliminate options (B) and (C).  B. Amazon S3 and Amazon CloudFront  C. Amazon EC2 and Amazon Elastic Transcoder We eliminate option (C) as Amazon Elastic Transcoder is for media transcoding in the cloud — not particularly relevant to a multi-player mobile game. Option (B) is a more interesting idea. The original problem lacks information about where the scaling issues are occurring in the current on-premise solution — it would be true that a hybrid solution could be explored whereby all static content is hosted on S3 and served through a Content Delivery Network using Amazon CloudFront, and we leave all the dynamic application and database where it is on the existing servers on-premise. For static content, using S3 and a CDN is definitely highly scalable and cost effective. It’s also useful in Single Page Applications like ReactJS or Angular. And yes, it could also be part of a phased approach to shift static content first, and even in a fully Cloud-native replacement solution hosted entirely in AWS, we are likely to end up with some use of S3 and Amazon CloudFront. However, given the crux of this question, and the fact that it’s a “mobile game” (can we infer an iOS or Android native app?) let’s assume that the problem with scaling is with the backend dynamic logic and database and we’ll eliminate option (B). We’re now left with two viable options.  A. Amazon EC2 and an Application Load Balancer  D. AWS Lambda and Amazon API Gateway Remember our core requirement; we’re looking for the “MOST scalable and cost-effective”. Both of these options allow us to create scalable solutions — so we have some more thinking to pick (A) or (D). I’ve made an assumption before I start; the current solution uses an RDBMS database and we’ll migrate that over to Amazon RDS — and I’ve assumed that the reason this isn’t mentioned in the problem statement is that we’ll end up picking the same solution for the database regardless of option chosen. So all we have to consider is the application logic and its compute requirements. With EC2, we can create auto-scaling groups and use Spot instances to achieve the “cost-effective”. When considering option (A), we have to pay attention to the original problem statement — the demand has out stripped the infrastructure they can use in their on-premise data center. So that statement tells us that there’s a lot of compute currently in-use and demand is increasing. I’ve inferred, a lot. Therefore, migrating as-is to EC2 will also require a lot of compute resources — like for like — and while costs can be managed through a combination of Reserved and Spot instances, that much compute is still going to have a price tag with it. We add the cost of the Application Load Balancer on top. In terms of effort required to migrate the application, moving across to EC2 is likely to require fewer application code changes — it could even “lift and shift”. Next to consider is option (D), which would likely require us to rewrite our code as AWS Lambda functions (although naturally stateless, we can handle state and combine API Gateway with web sockets). AWS Lambda lets us run our code without provisioning or managing servers, and we only pay for the compute time we consume. It naturally scales up to meet the peak demands, and we don’t pay for idle instances. In terms of costs, as we scale out to the volumes implied in the problem statement, running AWS Lambda is likely to be cheaper than running the equivalent load through a fleet of EC2 instances. The downside is that we would likely need to re-develop our solution (but not doing so wasn’t stated as a constraint — so assume a greenfield). In conclusion, for the “MOST scalable and cost-effective” solution I’d pick (D) — AWS Lambda and API Gateway.  D. AWS Lambda and Amazon API Gateway So how does my answer (D) compare with those on the “AWS Cloud” group — the group was pretty much all going for (A), nearly everyone liked the EC2 option. So my answer definitely leaves me in the minority. Leave a comment if you would also have chosen (A) in this example and let me know why.   Scenario 3. When will you incur costs with an Elastic IP address (EIP)?A. When an EIP is allocated. B. When it is allocated and associated with a running instance. C. When it is allocated and associated with a stopped instance. D. Costs are incurred regardless of whether the EIP is associated with a running instance. A quick one to close off for this week. This question was in a bank of AWS sample questions, and resonated with me earlier this week because I found I was paying for an Elastic IP Address in my own AWS account. So I’m sharing this answer out of personal experience. An Elastic IP is free, but only as long as it is being used by a running instance. So of these options the answer is (C); we would pay for an Elastic IP address when it is allocated and associated with a stopped instance. A note from the author: So there we have it. Three example AWS questions and answers. Thanks for reading and I hope you enjoyed the article and found it useful. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 15,
    "url": "https://goatsintrees.net/using-a-custom-domain-in-cloudfront-with-an-ssl-certificate-and-route-53/",
    "title": "Configure a custom domain in AWS CloudFront",
    "body": "2020/02/09 - CloudFront is Amazon’s low-latency Content Delivery Network (CDN). Using a CDN speeds up the distribution of content to visitors by serving content from edge locations that are closest to the user. Delivering content from Amazon S3 using CloudFront edge locations At the time of writing, Amazon has 216 Points of Presence (205 Edge Locations and 11 Regional Edge Caches) in 84 cities across 42 countries. When you set up your CloudFront distribution, straight out of the box with the default settings you will have your own cloudfront. net domain.  (that is assuming you have already configured CloudFront in front of an S3 bucket that holds your static web content, but if not check out this guide on serving static content from S3 using CloudFront and come back) But what if you want to serve your content from my-custom-domain. com. To use a custom domain requires a combination of Route 53 — Amazon’s highly available and scalable cloud DNS web service — and some additional configuration CloudFront. It doesn’t take too long to set up. Before we start, I assume that you have your domain managed in Route 53; it doesn’t matter if you don’t, but this guide assumes you do. There is an initial step to obtain an SSL Certificate within Certificate Manager. This allows you to serve your content over https and is a service provided by Amazon for free, and they’ll also take care of its renewal. Within the Certificate Manager service, make sure you change your region to North Virginia; I cannot emphasize this one enough as it’s caught me out many a time. Then Request a Certificate. The form is pretty self explanatory and you’ll need to provide a means to prove you own the domain — if you’re using Route 53, and we assume you are, then selecting the option that Amazon automatically manages the validation is the simplest approach. The process usually takes a few minutes. Now head over to CloudFront and set up your custom domain. This can be done at the time of creating the distribution, but don’t worry if you forgot — you can go back and edit all these settings later. However, you do have to complete the setting in CloudFront before you finish off the setup in Route 53. The first setting is to list all your Alternative Domain Names in the CloudFront distribution settings. Add all your domain names to CloudFront distribution settings The second setting is to reference the SSL Certificate you created. Check the Custom SSL Certificate (example. com) option and pick your SSL Certificate from the list. Warning; your Alternate Domain Names must match those you specified in the SSL Certificate provisioning request — so if you don’t see your certificate in the list, that is probably the reason. With these settings done, the final step is to configure the DNS in Route 53. In your domain hosted zone in Route 53, select to Create Record Set. We will be creating as an A record for IPv4 and we’ll select the Alias option. In the Alias Target, you will find your CloudFront distribution — select and save. Warning; your Alternate Domain Names you configured in CloudFront must match the record set name — so if you don’t see your CloudFront distribution in the target drop down list, that is probably the reason.  Repeat to create an AAAA record for IPv6. And that’s it. Success. You will find that you are now able to view your website using my-custom-domain. com, with all the added benefits of CloudFront providing edge locations around the world to reduce latency for your visitors. You will also have an SSL Certificate that is managed by Amazon and will be automatically renewed for you (at the time of writing, it’s free).  Variations. If you don’t use Route 53, the final step will be to add a CNAME entry in your DNS settings and set the value to your CloudFront domain. A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your feedback. You can follow me on Twitter and connect on LinkedIn. "
    }, {
    "id": 16,
    "url": "https://goatsintrees.net/serving-static-content-from-s3-using-cloudfront-and-origin-access-identity/",
    "title": "Serve private static content from S3 with CloudFront and Origin Access Identity",
    "body": "2020/02/08 - Using Amazon Simple Storage Service (Amazon S3) is a cheap and effective way to host static websites and other web content. You can do so directly from the S3 console by enabling the Static website hosting feature and you’ll get a website of the form http://my-bucket-name. s3-website-eu-west-1. amazonaws. com. You can also create an A-record ALIAS in Route 53 to use your own custom domain. Basic set up — hosting a static website in S3 Some solutions may stop here. For some, this is *good enough. *(but this is not the purpose of our article; it would be incredibly short if it was). More likely the solution will evolve toward serving content from edge cache locations using CloudFront — Amazon’s low-latency Content Delivery Network (CDN). Using a CDN both speeds up the distribution of content to visitors and will also reduce the overall cost for a busy site. Introducing CloudFront as our Content Delivery Network Even with the CDN our visitors can still access the S3 bucket directly, and the Solution Architect will now be asked “how do we restrict access to the S3 bucket so that our html, css, and images, are only accessible through CloudFront?” (this question is the purpose of this article).  The answer is to use Origin Access Identity (OAI). We can restrict public access to objects in the S3 bucket (as of today, this is the default setting) and we grant permission to the OAI to distribute the content through CloudFront to our visitors from around the world. The steps we follow to achieve this solution are;    Create the S3 bucket with default settings and upload an index. html file (index. html will not be accessible directly from S3).     Create a CloudFront distribution with the S3 bucket as its origin (index. html still cannot be accessed).     Set up the OAI, and configure a policy that permits CloudFront to serve the index. html file (now it all works).  Steps 2 and 3 would normally be applied at the same time, but I’ll demonstrate separately to show the individual steps and how the OAI is the bit of magic sugar in the solution. Step 1. Create the bucket in S3 that will hold the static content and use all the default settings. The bucket and its objects are not accessible to the public. S3 Bucket holding our static website content When we attempt to reach the index. html file in a browser, we get an Access Denied error as expected. Hitting index. html on S3 endpoint receives an Access Denied error Step 2. In CloudFront, create a Web distribution and select the S3 bucket as the origin. At this stage, leave everything else as the default settings, scroll to the bottom and create the distribution. You’ll have to wait until it’s deployed and as this can take 10 minutes, go grab a coffee… Our CloudFront distribution for our S3 origin Once its status has changed to “Deployed”, it’s ready. At this stage, the index. html page is not accessible on the CloudFront domain. Hitting index. html on CloudFront endpoint receives an Access Denied error Step 3. Create the Origin Access Identity and configure the policy in S3 that grants the OAI permission to access objects. Since we’re doing this in two stages, we have to edit our existing Origin to access the OAI option; however, usually you would do this at the same time as creating your Web distribution and it’s on the initial list of options. Select our origin and click Edit On the next page, select Restrict Bucket Access, allow Amazon to Create a New Identity, and choose Yes, Update Bucket Policy. Select the options and save Wait for the status to change to “Deployed” again, and then refresh the page and the index. html page will now be displayed. Our static content is now served correctly via CloudFront All set. Success. We are now using CloudFront edge locations to serve our static content uploaded to S3. And no one can hit the content in S3 directly. Check the direct S3 endpoint again just to be sure that remains blocked, and view the Bucket Policy in S3 that was added automatically to learn more. What we didn’t cover today. Most solutions are likely to require a custom domain that is configured in Route 53 and CloudFront; an SSL certificate from Amazon Certificate Manager so that content can be served over https; and have cache expiry limits set on the objects. In addition, if you have built a Single Page Application (SPA), like Angular or ReactJS, then you may need to configure CORS. Finally, we didn’t cover off creating signed URLs, which is useful if you are distributing paid-for content and want to limit access to your edge location caches. A note from the author: Thank you for reading this article — I hope you found this article useful and I look forward to your comments and feedback. You can follow me on Twitter and connect on LinkedIn. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});